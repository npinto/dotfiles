#!/usr/bin/env python3
"""
DocSend Downloader - Enhanced version with verbose OCR and title extraction
"""

import argparse
import hashlib
import logging
import re
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

from curl_cffi import requests as curl_requests
from playwright.sync_api import sync_playwright
import browser_cookie3
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.table import Table

console = Console()

# Set up logging
logger = logging.getLogger(__name__)

# Default number of pages to analyze for title extraction (OCR + Claude vision)
DEFAULT_TITLE_PAGES = 10

# Check if Claude CLI is available
def _check_claude_available():
    """Check if claude CLI is available on the system."""
    import shutil
    return shutil.which("claude") is not None

CLAUDE_AVAILABLE = _check_claude_available()


def setup_logging(level):
    """Set up logging configuration."""
    logging_level = getattr(logging, level.upper(), logging.INFO)

    # Create formatter
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Create console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging_level)
    console_handler.setFormatter(formatter)

    # Set up root logger
    logging.basicConfig(level=logging_level, handlers=[console_handler])

    # Also configure requests library logging
    requests_logger = logging.getLogger("requests.packages.urllib3")
    requests_logger.setLevel(
        logging.WARNING if logging_level > logging.DEBUG else logging.DEBUG
    )

    logger.info(f"Logging configured at {level.upper()} level")


def get_physical_cpu_count():
    """Get the number of physical (performance) CPU cores.

    On Apple Silicon Macs, returns performance cores only (not efficiency cores).
    On Intel Macs/Linux, returns physical cores (not hyperthreaded).
    Falls back to os.cpu_count() if detection fails.
    """
    import os
    import platform

    try:
        if platform.system() == "Darwin":
            # macOS: try to get performance cores first (Apple Silicon)
            result = subprocess.run(
                ["sysctl", "-n", "hw.perflevel0.physicalcpu"],
                capture_output=True, text=True
            )
            if result.returncode == 0 and result.stdout.strip():
                return int(result.stdout.strip())

            # Fallback: physical CPU count (Intel Mac)
            result = subprocess.run(
                ["sysctl", "-n", "hw.physicalcpu"],
                capture_output=True, text=True
            )
            if result.returncode == 0 and result.stdout.strip():
                return int(result.stdout.strip())

        elif platform.system() == "Linux":
            # Linux: count physical cores from /proc/cpuinfo
            with open("/proc/cpuinfo") as f:
                cores = set()
                physical_id = None
                core_id = None
                for line in f:
                    if line.startswith("physical id"):
                        physical_id = line.split(":")[1].strip()
                    elif line.startswith("core id"):
                        core_id = line.split(":")[1].strip()
                    elif line.strip() == "" and physical_id and core_id:
                        cores.add((physical_id, core_id))
                        physical_id = core_id = None
                if cores:
                    return len(cores)
    except Exception:
        pass

    # Fallback to logical CPU count
    return os.cpu_count() or 4


def get_image_hash(image_data):
    """Get hash of image data."""
    return hashlib.md5(image_data).hexdigest()


def extract_title_from_html(html_content):
    """Extract document title from DocSend page HTML.

    Parses the HTML response to extract the document title from:
    1. The <title> tag (usually "Document Name - DocSend")
    2. Open Graph meta tags (og:title)

    Returns:
        Document title string or None if not found
    """
    if not html_content:
        return None

    try:
        # Try to extract from <title> tag
        # DocSend titles are typically: "Document Name - DocSend" or "Document Name | DocSend"
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
        if title_match:
            title = title_match.group(1).strip()
            # Remove common suffixes like " - DocSend", " | DocSend"
            title = re.sub(r'\s*[-|]\s*(DocSend|Docsend).*$', '', title, flags=re.IGNORECASE)
            title = title.strip()
            if title and len(title) > 3 and len(title) < 200:
                logger.debug(f"Extracted title from <title> tag: {title}")
                return title

        # Try og:title meta tag
        og_title_match = re.search(
            r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\']([^"\']+)["\']',
            html_content,
            re.IGNORECASE
        )
        if not og_title_match:
            # Try alternate attribute order
            og_title_match = re.search(
                r'<meta[^>]+content=["\']([^"\']+)["\'][^>]+property=["\']og:title["\']',
                html_content,
                re.IGNORECASE
            )
        if og_title_match:
            title = og_title_match.group(1).strip()
            title = re.sub(r'\s*[-|]\s*(DocSend|Docsend).*$', '', title, flags=re.IGNORECASE)
            title = title.strip()
            if title and len(title) > 3 and len(title) < 200:
                logger.debug(f"Extracted title from og:title: {title}")
                return title

    except Exception as e:
        logger.debug(f"Could not extract title from HTML: {e}")

    return None


def extract_title_from_image(image_path):
    """Try to extract title from the first slide using OCR."""
    console.print(
        "\n[yellow]Attempting to extract document title from first slide...[/yellow]"
    )

    try:
        # Use tesseract to extract text from first slide
        result = subprocess.run(
            ["tesseract", str(image_path), "stdout", "--psm", "3"],
            capture_output=True,
            text=True,
        )

        if result.returncode == 0:
            text = result.stdout
            # Look for title-like text (first few lines, longer phrases)
            lines = [line.strip() for line in text.split("\n") if line.strip()]

            if lines:
                # Try to find a good title candidate
                # Usually the title is in the first few lines and is substantial
                for i, line in enumerate(lines[:5]):
                    # Skip very short lines or lines that look like dates/numbers
                    if len(line) > 10 and not re.match(r"^[\d\s\-/]+$", line):
                        # Clean up the title
                        title = re.sub(r"[^\w\s\-]", " ", line)
                        title = " ".join(title.split())[:50]  # Limit length
                        if title:
                            console.print(
                                f"[green]‚úì Found potential title: '{title}'[/green]"
                            )
                            return title

                # If no good title found, use first substantial line
                first_line = lines[0] if lines else ""
                if len(first_line) > 5:
                    title = re.sub(r"[^\w\s\-]", " ", first_line)
                    title = " ".join(title.split())[:50]
                    console.print(
                        f"[yellow]Using first line as title: '{title}'[/yellow]"
                    )
                    return title

    except FileNotFoundError:
        console.print(
            "[yellow]Tesseract not installed - cannot extract title from image[/yellow]"
        )
        console.print("Install with: brew install tesseract")
    except Exception as e:
        console.print(f"[yellow]Could not extract title: {e}[/yellow]")

    return None


def suggest_title_with_claude(output_dir, slide_count, max_pages=DEFAULT_TITLE_PAGES):
    """Use claude -p with slide images to suggest a document title and extract date.

    Args:
        output_dir: Directory containing slide images
        slide_count: Total number of slides
        max_pages: Maximum number of pages to send to Claude (default: 10)

    Returns:
        Tuple of (title, date_str) where:
        - title: Suggested title (max 30 chars) or None if failed
        - date_str: Date string like "2024-01-15", "2024-01", "2024", or None
    """
    if not CLAUDE_AVAILABLE:
        console.print("\n[yellow]‚ö† Claude CLI not available - skipping AI analysis[/yellow]")
        return None, None

    console.print("\n[cyan]ü§ñ Using Claude to analyze document...[/cyan]")

    # Determine how many slides to use
    pages_to_use = min(slide_count, max_pages)
    console.print(f"   Analyzing {pages_to_use} slide(s)...")

    # Collect slide filenames that exist
    slide_files = []
    for i in range(1, pages_to_use + 1):
        slide_path = output_dir / f"slide_{i:03d}.png"
        if slide_path.exists():
            slide_files.append(f"slide_{i:03d}.png")

    if not slide_files:
        console.print("[yellow]   No slides found[/yellow]")
        return None, None

    # Extract OCR text from slides to help with accurate name/title extraction
    # OCR the same slides we send to Claude for cross-referencing
    ocr_texts = []
    for slide_file in slide_files:  # OCR all slides we're sending to Claude
        slide_path = output_dir / slide_file
        try:
            ocr_result = subprocess.run(
                ["tesseract", str(slide_path), "stdout", "--psm", "3"],
                capture_output=True,
                text=True,
                timeout=30,
            )
            if ocr_result.returncode == 0 and ocr_result.stdout.strip():
                # Take first 400 chars per slide
                ocr_texts.append(f"[{slide_file}]\n{ocr_result.stdout.strip()[:400]}")
        except Exception as e:
            logger.debug(f"OCR extraction failed for {slide_file}: {e}")

    ocr_text = "\n\n".join(ocr_texts)
    if ocr_text:
        console.print(f"   OCR extracted {len(ocr_text)} chars from {len(ocr_texts)} slide(s)")

    # Build file list for prompt
    files_str = ", ".join(slide_files)

    # Build a concise prompt - long/complex prompts cause Claude CLI to hang
    # Use "through" syntax for file ranges to keep prompt short
    if len(slide_files) > 1:
        first_file = slide_files[0]
        last_file = slide_files[-1]
        files_ref = f"{first_file} through {last_file}"
    else:
        files_ref = slide_files[0]

    # Include OCR text for name verification (keep it brief to avoid hang)
    ocr_hint = ""
    if ocr_text:
        # Extract just key identifiers from OCR (names, companies) - first 500 chars
        ocr_hint = f" OCR hints: {ocr_text[:500]}"

    # Step 1: Extract title and date from slides
    prompt1 = (
        f"Read {files_ref}. Extract the document title (~30 chars, "
        "capture company/topic, no generic words like Presentation/Deck/Slides, no dates) "
        f"and any document date found (YYYY-MM-DD, YYYY-MM, or YYYY format).{ocr_hint} "
        "Format as:\nTITLE: xxx\nDATE: xxx or NONE"
    )

    cmd = ["claude", "-p", prompt1]

    try:
        logger.debug(f"Running claude step 1 in {output_dir} with {len(slide_files)} files")
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30,
            cwd=str(output_dir),
        )

        if result.returncode == 0 and result.stdout.strip():
            output = result.stdout.strip()
            logger.debug(f"Claude step 1 output: {output}")

            # Parse the output
            title = None
            date_str = None

            for line in output.split("\n"):
                line = line.strip()
                if line.upper().startswith("TITLE:"):
                    raw_title = line[6:].strip()
                    # Clean up the title for use as filename
                    raw_title = raw_title.strip('"\'')
                    # Remove invalid filename characters
                    clean_title = re.sub(r'[<>:"/\\|?*]', "", raw_title)
                    # Collapse whitespace and underscores to spaces
                    clean_title = clean_title.replace("_", " ")
                    clean_title = " ".join(clean_title.split())
                    clean_title = clean_title.strip()
                    if clean_title:
                        title = clean_title

                elif line.upper().startswith("DATE:"):
                    raw_date = line[5:].strip()
                    if raw_date.upper() != "NONE" and raw_date:
                        # Validate date format (YYYY, YYYY-MM, or YYYY-MM-DD)
                        if re.match(r"^\d{4}(-\d{2}(-\d{2})?)?$", raw_date):
                            date_str = raw_date
                        else:
                            logger.debug(f"Invalid date format: {raw_date}")

            # Step 2: If title is too long, ask Claude to shorten it
            if title and len(title) > 40:
                console.print(f"   Title too long ({len(title)} chars), refining...")
                prompt2 = (
                    f"Shorten this title to ~30 chars while keeping key info: '{title}'. "
                    "Format as:\nTITLE: xxx"
                )
                cmd2 = ["claude", "-p", prompt2]
                try:
                    result2 = subprocess.run(
                        cmd2,
                        capture_output=True,
                        text=True,
                        timeout=15,
                    )
                    if result2.returncode == 0 and result2.stdout.strip():
                        for line in result2.stdout.strip().split("\n"):
                            line = line.strip()
                            if line.upper().startswith("TITLE:"):
                                short_title = line[6:].strip().strip('"\'')
                                short_title = re.sub(r'[<>:"/\\|?*]', "", short_title)
                                short_title = " ".join(short_title.split())
                                if short_title:
                                    logger.debug(f"Shortened: '{title}' -> '{short_title}'")
                                    title = short_title
                                break
                except Exception as e:
                    logger.debug(f"Step 2 refinement failed: {e}")

            if title:
                console.print(f"   ‚úì Title: '[green]{title}[/green]'")
            if date_str:
                console.print(f"   ‚úì Document date: '[green]{date_str}[/green]'")
            elif title:
                console.print("   ‚úì No document date found")

            return title, date_str

        console.print(
            f"[yellow]   Claude returned no output (exit code: {result.returncode})[/yellow]"
        )
        if result.stderr:
            logger.debug(f"Claude stderr: {result.stderr}")

    except FileNotFoundError:
        console.print("[yellow]   'claude' command not found - install Claude CLI[/yellow]")
    except subprocess.TimeoutExpired:
        console.print("[yellow]   Claude timed out[/yellow]")
    except Exception as e:
        console.print(f"[yellow]   Error calling Claude: {e}[/yellow]")

    return None, None


def discover_dataroom_documents(url, browser_name):
    """Use Playwright to discover all documents in a DocSend data room.

    This intercepts GraphQL API responses to extract document URLs directly,
    which is more reliable than trying to click on documents to open them.

    Args:
        url: Data room URL (e.g., /view/s/xxx or /view/s/xxx/f/yyy)
        browser_name: Browser to load cookies from

    Returns:
        List of (name, url, folder_name) tuples for each document found
    """
    console.print(f"\n[cyan]üìÇ Discovering documents in data room...[/cyan]")

    try:
        # Load cookies from browser
        import time

        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "firefox": browser_cookie3.firefox,
            "safari": browser_cookie3.safari,
            "edge": browser_cookie3.edge,
            "chromium": browser_cookie3.chromium,
        }

        if browser_name.lower() not in browser_funcs:
            console.print(f"[red]Unknown browser: {browser_name}[/red]")
            return []

        cj = browser_funcs[browser_name.lower()](domain_name="docsend.com")
        cookies = []
        for c in cj:
            cookie = {
                "name": c.name,
                "value": c.value,
                "domain": c.domain if c.domain else ".docsend.com",
                "path": c.path if c.path else "/",
            }
            if c.expires:
                cookie["expires"] = int(c.expires)
            if c.secure:
                cookie["secure"] = bool(c.secure)
            cookies.append(cookie)

        # Extract base space URL (without /f/ folder filter)
        # /view/s/SPACE_ID/f/FOLDER_ID -> /view/s/SPACE_ID
        parsed = urlparse(url)
        path_parts = parsed.path.strip("/").split("/")
        if "/f/" in url:
            # Remove folder part to get base URL
            f_index = path_parts.index("f")
            base_path = "/".join(path_parts[:f_index])
            base_url = f"{parsed.scheme}://{parsed.netloc}/{base_path}"
        else:
            base_url = url

        # Collect all GraphQL responses
        all_graphql_responses = []

        with sync_playwright() as p:
            # Use headless mode with anti-detection
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled', '--no-sandbox', '--disable-dev-shm-usage']
            )
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1200},
            )

            # Remove webdriver detection
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            """)

            # Add cookies
            try:
                context.add_cookies(cookies)
            except Exception as e:
                logger.warning(f"Could not add all cookies: {e}")

            page = context.new_page()
            page.set_default_timeout(60000)

            # Intercept GraphQL responses to capture document data
            def handle_graphql_route(route):
                response = route.fetch()
                if response.status == 200:
                    try:
                        body = response.json()
                        all_graphql_responses.append(body)
                    except Exception:
                        pass
                route.fulfill(response=response)

            page.route('**/graphql', handle_graphql_route)

            console.print(f"   Loading data room... [dim](this may take 30-60s)[/dim]")
            page.goto(base_url, wait_until="networkidle", timeout=60000)
            console.print(f"   Page loaded, waiting for content...")
            time.sleep(3)

            # Check if we're on an error page or auth page
            page_title = page.title().lower()
            if "error" in page_title or "could not be satisfied" in page_title:
                console.print("[yellow]   Data room access failed - cookies may have expired[/yellow]")
                console.print("[yellow]   Try refreshing the page in your browser first[/yellow]")
                browser.close()
                return []

            # Check if auth form is present (email verification required)
            content = page.content().lower()
            if page.query_selector('input[type="email"]') and "verify" in content:
                console.print("[yellow]   Data room requires email verification[/yellow]")
                console.print("[yellow]   Please authenticate in your browser first[/yellow]")
                browser.close()
                return []

            # Get folder list from sidebar
            folder_items = page.query_selector_all('[role="listitem"]')
            folder_names = []
            for item in folder_items:
                try:
                    text = item.inner_text().strip()
                    if not text or not item.is_visible():
                        continue
                    folder_name = text.split('\n')[0].strip()
                    if folder_name and folder_name != "Home" and folder_name not in folder_names:
                        folder_names.append(folder_name)
                except Exception:
                    continue

            console.print(f"   Found {len(folder_names)} folders")

            # Click each folder to trigger GraphQL requests for their contents
            for idx, folder_name in enumerate(folder_names, 1):
                console.print(f"   Scanning folder {idx}/{len(folder_names)}: {folder_name}...")
                items = page.query_selector_all('[role="listitem"]')
                for item in items:
                    try:
                        text = item.inner_text().strip().split('\n')[0].strip()
                        if text == folder_name and item.is_visible():
                            item.click()
                            page.wait_for_load_state('networkidle', timeout=10000)
                            time.sleep(2)
                            break
                    except Exception:
                        continue

            # Wait for all async responses to complete
            time.sleep(3)

            browser.close()

        # Parse all GraphQL responses to extract documents
        console.print(f"   Processing {len(all_graphql_responses)} API responses...")

        def extract_docs_from_response(obj, current_folder='Home'):
            """Recursively extract documents from GraphQL response."""
            results = []
            if isinstance(obj, dict):
                folder = current_folder
                if obj.get('__typename') == 'SpaceFolder' and 'name' in obj:
                    folder = obj['name']

                if obj.get('__typename') == 'SpaceDocument':
                    href = obj.get('href', '')
                    name = obj.get('name', '')
                    if href and '/d/' in href:
                        results.append((name, href, folder))

                if 'contents' in obj and isinstance(obj.get('contents'), dict):
                    nodes = obj['contents'].get('nodes', [])
                    for node in nodes:
                        results.extend(extract_docs_from_response(node, folder))

                for k, v in obj.items():
                    if k not in ('contents', 'nodes'):
                        results.extend(extract_docs_from_response(v, folder))
            elif isinstance(obj, list):
                for item in obj:
                    results.extend(extract_docs_from_response(item, current_folder))
            return results

        all_docs = {}  # href -> (name, folder)
        for resp in all_graphql_responses:
            for name, href, folder in extract_docs_from_response(resp):
                if not href.startswith('http'):
                    href = f'https://docsend.com{href}'
                if href not in all_docs:
                    all_docs[href] = (name, folder)

        # Group by folder for display
        by_folder = {}
        for doc_url, (name, folder) in all_docs.items():
            if folder not in by_folder:
                by_folder[folder] = []
            by_folder[folder].append((name, doc_url))

        # Display results
        for folder in ['Home'] + folder_names:
            docs = by_folder.get(folder, [])
            if docs:
                console.print(f"   üìÅ {folder}: {len(docs)} docs")
                for name, doc_url in docs:
                    console.print(f"      ‚Ä¢ {name[:45]}")

        # Convert to list format
        documents_with_urls = []
        for doc_url, (name, folder) in all_docs.items():
            documents_with_urls.append((name, doc_url, folder))

        console.print(f"\n   ‚úì Discovered {len(documents_with_urls)} document(s) total")
        return documents_with_urls

    except Exception as e:
        console.print(f"[red]   Error discovering documents: {e}[/red]")
        import traceback
        logger.debug(traceback.format_exc())
        return []


def load_cookies_from_browser(browser_name, domain="docsend.com"):
    """Load cookies from a browser for a specific domain.

    Args:
        browser_name: 'brave', 'chrome', 'firefox', 'safari', 'edge'
        domain: Domain to filter cookies for

    Returns:
        dict of cookies or None if failed
    """
    console.print(f"\n[cyan]üç™ Loading cookies from {browser_name}...[/cyan]")

    try:
        # Get the appropriate cookie jar
        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "firefox": browser_cookie3.firefox,
            "safari": browser_cookie3.safari,
            "edge": browser_cookie3.edge,
            "chromium": browser_cookie3.chromium,
        }

        if browser_name.lower() not in browser_funcs:
            console.print(f"[red]Unknown browser: {browser_name}[/red]")
            console.print(f"[yellow]Supported: {', '.join(browser_funcs.keys())}[/yellow]")
            return None

        cj = browser_funcs[browser_name.lower()](domain_name=domain)

        # Convert to dict
        cookies = {}
        for cookie in cj:
            cookies[cookie.name] = cookie.value

        if cookies:
            console.print(f"   ‚úì Loaded {len(cookies)} cookies from {browser_name}")
            return cookies
        else:
            console.print(f"[yellow]   No cookies found for {domain}[/yellow]")
            return None

    except Exception as e:
        console.print(f"[red]   Error loading cookies: {e}[/red]")
        return None


def authenticate_spa_with_playwright(url, email, passcode):
    """Use Playwright to authenticate on JavaScript SPA DocSend pages.

    Args:
        url: DocSend URL
        email: Email address for authentication
        passcode: Passcode for authentication

    Returns:
        tuple of (cookie_dict, final_url) if successful, (None, None) if failed
        final_url may be different from input url (e.g., space -> document redirect)
    """
    console.print("\n[cyan]üé≠ Using Playwright for browser-based authentication...[/cyan]")

    try:
        with sync_playwright() as p:
            # Launch headless browser
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()

            console.print(f"   Loading page...")
            page.goto(url, wait_until="networkidle")

            # Wait for auth form to appear
            console.print(f"   Waiting for auth form...")
            try:
                # Look for email input
                email_input = page.wait_for_selector(
                    'input[type="email"], input[name*="email"], input[placeholder*="email" i]',
                    timeout=10000
                )
                if email_input:
                    console.print(f"   Filling email: [cyan]{email}[/cyan]")
                    email_input.fill(email)
            except Exception:
                console.print("[yellow]   No email field found[/yellow]")

            # Look for passcode input
            try:
                passcode_input = page.wait_for_selector(
                    'input[type="password"], input[name*="passcode"], input[name*="password"]',
                    timeout=5000
                )
                if passcode_input:
                    console.print(f"   Filling passcode...")
                    passcode_input.fill(passcode)
            except Exception:
                console.print("[yellow]   No passcode field found[/yellow]")

            # Click submit button
            try:
                submit_btn = page.wait_for_selector(
                    'button[type="submit"], button:has-text("Confirm"), button:has-text("Continue")',
                    timeout=5000
                )
                if submit_btn:
                    console.print(f"   Submitting form...")
                    submit_btn.click()
                    # Wait for navigation or content to load
                    page.wait_for_load_state("networkidle", timeout=15000)
            except Exception as e:
                console.print(f"[yellow]   Could not find/click submit button: {e}[/yellow]")

            # Check if authentication was successful
            import time
            time.sleep(3)  # Give page time to settle

            # Get the final URL (may have redirected to a document)
            final_url = page.url
            if final_url != url:
                console.print(f"   Redirected to: [cyan]{final_url}[/cyan]")

            # If this is a space/folder, look for document links
            # DocSend spaces show a list of documents after auth
            doc_links = page.query_selector_all('a[href*="/view/"][href*="/d/"]')
            if doc_links:
                console.print(f"   Found {len(doc_links)} document(s) in space")
                # Get the first document link
                first_doc = doc_links[0]
                doc_href = first_doc.get_attribute("href")
                if doc_href:
                    if not doc_href.startswith("http"):
                        doc_href = f"https://docsend.com{doc_href}"
                    console.print(f"   Using first document: [cyan]{doc_href}[/cyan]")
                    final_url = doc_href

            # Extract cookies
            cookies = context.cookies()
            cookie_dict = {c["name"]: c["value"] for c in cookies}

            # Verify auth worked by checking for document viewer elements
            try:
                # Look for signs of successful auth
                viewer = page.query_selector('[class*="viewer"], [class*="document"], [class*="page"]')
                if viewer or len(cookies) > 2:
                    console.print("[green]   ‚úì Authentication appears successful[/green]")
                    browser.close()
                    return cookie_dict, final_url
            except Exception:
                pass

            # If we got cookies, assume it worked
            if cookie_dict:
                console.print("[green]   ‚úì Got session cookies[/green]")
                browser.close()
                return cookie_dict, final_url

            console.print("[yellow]   ‚úó Authentication may have failed[/yellow]")
            browser.close()
            return None, None

    except Exception as e:
        console.print(f"[red]   Playwright error: {e}[/red]")
        return None, None


def authenticate_with_playwright_and_cookies(url, browser_name, passcode, email=None):
    """Use Playwright with browser cookies to authenticate on DocSend pages.

    This handles the case where CloudFront blocks direct requests but we can
    use Playwright with anti-detection measures to complete authentication.

    Args:
        url: DocSend URL
        browser_name: Browser to load cookies from ('brave', 'chrome', etc.)
        passcode: Passcode for authentication
        email: Optional email (if not provided, uses email from cookie session)

    Returns:
        tuple of (cookie_dict, final_url) if successful, (None, None) if failed
    """
    console.print("\n[cyan]üé≠ Using Playwright with browser cookies...[/cyan]")

    try:
        # Load cookies from browser
        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "firefox": browser_cookie3.firefox,
            "safari": browser_cookie3.safari,
            "edge": browser_cookie3.edge,
            "chromium": browser_cookie3.chromium,
        }

        if browser_name.lower() not in browser_funcs:
            console.print(f"[red]Unknown browser: {browser_name}[/red]")
            return None, None

        cj = browser_funcs[browser_name.lower()](domain_name="docsend.com")
        cookies = []
        for c in cj:
            cookie = {
                "name": c.name,
                "value": c.value,
                "domain": c.domain if c.domain else ".docsend.com",
                "path": c.path if c.path else "/",
            }
            if c.expires:
                cookie["expires"] = int(c.expires)
            if c.secure:
                cookie["secure"] = bool(c.secure)
            cookies.append(cookie)

        console.print(f"   Loaded {len(cookies)} cookies from {browser_name}")

        with sync_playwright() as p:
            # Launch headless with anti-detection settings
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                ]
            )

            context = browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="en-US",
            )

            # Remove webdriver detection
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)

            # Add cookies
            try:
                context.add_cookies(cookies)
            except Exception as e:
                logger.warning(f"Could not add all cookies: {e}")

            page = context.new_page()

            console.print(f"   Loading page...")
            page.goto(url, wait_until="domcontentloaded", timeout=30000)

            import time
            time.sleep(3)  # Let page render

            # Check for auth form - use specific DocSend selectors
            email_input = page.query_selector('input[name="link_auth_form[email]"]')
            passcode_input = page.query_selector('input[name="link_auth_form[passcode]"]')

            if passcode_input and passcode:
                console.print(f"   Filling passcode...")
                passcode_input.fill(passcode)

                # If email field exists and is empty, fill it
                if email_input:
                    current_email = email_input.input_value()
                    if not current_email and email:
                        console.print(f"   Filling email: [cyan]{email}[/cyan]")
                        email_input.fill(email)
                    elif current_email:
                        console.print(f"   Email pre-filled: [cyan]{current_email}[/cyan]")

                # Click submit button - find visible one
                submit_btns = page.query_selector_all('button[type="submit"]')
                submit_btn = None
                for btn in submit_btns:
                    if btn.is_visible():
                        text = btn.inner_text().strip()
                        if text in ["Continue", "Submit", "Confirm", ""]:
                            submit_btn = btn
                            break

                if submit_btn:
                    console.print(f"   Clicking submit button...")
                    submit_btn.click()
                    page.wait_for_load_state("networkidle", timeout=15000)
                else:
                    console.print("[yellow]   Could not find visible submit button[/yellow]")

            time.sleep(3)  # Let page settle

            # Check if email verification is required
            content = page.content().lower()
            if "emailed a link" in content or "verify that you own" in content:
                console.print("\n[yellow]‚ö†Ô∏è  Email verification required[/yellow]")
                console.print("   DocSend sent a verification email to your address.")
                console.print("\n[bold]To complete authentication:[/bold]")
                console.print("   1. Check your email for a link from DocSend")
                console.print("   2. Click the link to verify in your browser")
                console.print("   3. Run this script again with --cookies-from-browser")
                console.print("\n   The verification link is usually valid for 15 minutes.")
                browser.close()
                return None, None

            # Get final URL and cookies
            final_url = page.url
            if final_url != url:
                console.print(f"   Redirected to: [cyan]{final_url}[/cyan]")

            # Extract cookies
            all_cookies = context.cookies()
            cookie_dict = {c["name"]: c["value"] for c in all_cookies}

            console.print(f"   ‚úì Got {len(cookie_dict)} cookies after auth")

            browser.close()
            return cookie_dict, final_url

    except Exception as e:
        console.print(f"[red]   Playwright error: {e}[/red]")
        return None, None


def detect_auth_requirements(html):
    """Detect what authentication fields are required from the page HTML.

    Returns:
        dict with keys: email_required, passcode_required
    """
    requirements = {
        "email_required": False,
        "passcode_required": False,
    }

    # Check for email field
    if "link_auth_form[email]" in html or 'name="link_auth_form[email]"' in html:
        requirements["email_required"] = True

    # Check for passcode/password field
    if "link_auth_form[passcode]" in html or 'name="link_auth_form[passcode]"' in html:
        requirements["passcode_required"] = True

    # Also check for common password field indicators in the visible text
    if re.search(
        r"<label[^>]*>.*?Password.*?</label>", html, re.IGNORECASE | re.DOTALL
    ):
        requirements["passcode_required"] = True

    return requirements


def authenticate_with_email(session, url, email, passcode, headers):
    """Try to authenticate with a specific email."""
    console.print(f"Trying email: [bold cyan]{email}[/bold cyan]")
    logger.info(f"Attempting authentication with email: {email}")

    # Get the page first to extract CSRF token
    response = session.get(url)
    logger.debug(f"Initial auth page response: {response.status_code}")

    # Extract CSRF token
    csrf_match = re.search(
        r'name="authenticity_token"\s+value="([^"]+)"', response.text
    )
    csrf_token = csrf_match.group(1) if csrf_match else None

    auth_data = {
        "utf8": "‚úì",
        "_method": "patch",
        "link_auth_form[email]": email,
        "link_auth_form[passcode]": passcode,
        "commit": "Continue",
    }

    # Add CSRF token if found
    if csrf_token:
        auth_data["authenticity_token"] = csrf_token

    auth_headers = headers.copy()
    auth_headers.update(
        {
            "Content-Type": "application/x-www-form-urlencoded",
            "Origin": "https://docsend.com",
            "Referer": url,
        }
    )

    logger.debug(f"Posting auth data to {url}")
    auth_response = session.post(
        url, data=auth_data, headers=auth_headers, allow_redirects=True
    )
    logger.debug(
        f"Auth response status: {auth_response.status_code}, History: {[r.status_code for r in auth_response.history]}"
    )

    # Check if authentication was successful
    if auth_response.status_code in [200, 302] or (
        auth_response.history
        and any(r.status_code == 302 for r in auth_response.history)
    ):
        # Additional check: try to access page_data/1
        # For folder URLs, we need to use the full path
        if "/d/" in url:
            # This is a folder URL, use the full path
            test_url = f"{url}/page_data/1"
        else:
            # Simple URL, use the document code
            document_code = url.strip("/").split("/")[-1]
            test_url = f"https://docsend.com/view/{document_code}/page_data/1"
        logger.debug(f"Testing authentication by accessing: {test_url}")
        test_response = session.get(test_url, headers={"Accept": "application/json"})

        if test_response.status_code == 200:
            try:
                data = test_response.json()
                if "imageUrl" in data:
                    console.print(
                        f"[bold green]‚úì Authentication successful with {email}[/bold green]"
                    )
                    return True
            except (ValueError, KeyError):
                pass

    console.print(f"[yellow]‚úó Authentication failed with {email}[/yellow]")
    return False


def download_image_batch(
    session, document_code, start_idx, batch_size=8, full_path=None
):
    """Download a batch of images in parallel."""
    console.print(
        f"[cyan]Downloading batch: slides {start_idx} to {start_idx + batch_size - 1}[/cyan]"
    )
    logger.debug(
        f"Starting batch download for slides {start_idx} to {start_idx + batch_size - 1}"
    )

    def download_single(idx):
        # Use full path if provided (for folder-based URLs)
        if full_path:
            page_url = f"{full_path}/page_data/{idx}"
        else:
            page_url = f"https://docsend.com/view/{document_code}/page_data/{idx}"

        try:
            logger.debug(f"Requesting page data for slide {idx}: {page_url}")
            resp = session.get(
                page_url, headers={"Accept": "application/json"}, timeout=10
            )
            logger.debug(f"Slide {idx} response: {resp.status_code}")

            if resp.status_code == 200:
                data = resp.json()
                image_url = data.get("imageUrl")

                if image_url:
                    logger.debug(f"Downloading image for slide {idx}: {image_url}")
                    img_resp = session.get(image_url, timeout=30)

                    if img_resp.status_code == 200:
                        img_hash = get_image_hash(img_resp.content)
                        console.print(
                            f"  ‚úì Slide {idx}: Downloaded (hash: {img_hash[:8]}...)"
                        )
                        return idx, img_hash, len(img_resp.content), img_resp.content
                    else:
                        console.print(f"  ‚úó Slide {idx}: Image download failed")
        except Exception as e:
            logger.error(f"Error downloading slide {idx}: {str(e)}")
            console.print(f"  ‚úó Slide {idx}: Error - {str(e)}")

        return idx, None, None, None

    # Download batch in parallel
    results = {}
    with ThreadPoolExecutor(max_workers=min(batch_size, 8)) as executor:
        futures = {
            executor.submit(download_single, i): i
            for i in range(start_idx, start_idx + batch_size)
        }

        for future in as_completed(futures):
            idx, img_hash, size, content = future.result()
            if img_hash:
                results[idx] = (img_hash, size, content)

    return results


def probe_slide_exists(session, document_code, slide_num, full_path=None):
    """Quietly check if a slide exists by verifying image size (no console output).

    DocSend returns imageUrl even for non-existent slides, but those URLs return
    tiny placeholder images (~111 bytes). Real slides are typically > 10KB.
    """
    if full_path:
        page_url = f"{full_path}/page_data/{slide_num}"
    else:
        page_url = f"https://docsend.com/view/{document_code}/page_data/{slide_num}"

    try:
        resp = session.get(page_url, headers={"Accept": "application/json"}, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            image_url = data.get("imageUrl")

            if not image_url:
                logger.debug(f"Probe slide {slide_num}: no imageUrl")
                return False

            # Download the image and check its size
            # Placeholder images are ~111 bytes, real slides are > 10KB
            img_resp = session.get(image_url, timeout=15)
            img_size = len(img_resp.content)

            # Real slide images are typically > 10KB
            is_real_slide = img_size > 1000
            logger.debug(f"Probe slide {slide_num}: imageSize={img_size}, isReal={is_real_slide}")
            return is_real_slide

        elif resp.status_code == 404:
            logger.debug(f"Probe slide {slide_num}: status=404")
            return False
        else:
            logger.debug(f"Probe slide {slide_num}: status={resp.status_code}")
    except Exception as e:
        logger.debug(f"Probe slide {slide_num}: error={e}")
    return False


def extract_slide_count_from_html(session, url):
    """Try to extract slide count from the document HTML page.

    DocSend pages contain "X PAGES" text that we can parse.
    Returns slide count or None if not found.
    """
    try:
        resp = session.get(url, timeout=15)
        if resp.status_code == 200:
            # Look for "X PAGES" pattern in HTML
            match = re.search(r'(\d+)\s*PAGES?', resp.text, re.IGNORECASE)
            if match:
                count = int(match.group(1))
                logger.debug(f"Extracted slide count from HTML: {count}")
                return count
    except Exception as e:
        logger.debug(f"Failed to extract slide count from HTML: {e}")
    return None


def detect_slide_count_auto(session, document_code, full_path=None):
    """Automatically detect slide count (tries HTML first, falls back to binary search)."""
    console.print("\n[bold yellow]Auto-detecting slide count...[/bold yellow]\n")
    logger.info(
        f"Starting automatic slide count detection for document: {document_code}"
    )

    # Step 1: Try to get slide count from HTML (fast path)
    url = full_path if full_path else f"https://docsend.com/view/{document_code}"
    html_count = extract_slide_count_from_html(session, url)
    if html_count:
        console.print(f"[green]‚úì Found {html_count} slides from page metadata[/green]\n")
        return html_count

    # Step 2: Fall back to probing - check slide 1 exists (authentication test)
    console.print("[cyan]Checking document access...[/cyan]")
    if not probe_slide_exists(session, document_code, 1, full_path):
        console.print("[red]Cannot access slide 1 - authentication may have failed[/red]")
        return None
    console.print("  ‚úì Document accessible")

    # Step 3: Find upper bound using exponential probing
    # Probe at 8, 16, 32, 64, 128, 256... until we find a slide that doesn't exist
    console.print("[cyan]Finding document length...[/cyan]")

    lower_bound = 1  # We know slide 1 exists
    upper_bound = None

    probe_points = [8, 16, 32, 64, 128, 256, 512]
    for probe in probe_points:
        if probe_slide_exists(session, document_code, probe, full_path):
            lower_bound = probe
            console.print(f"  ‚úì Slide {probe} exists")
        else:
            upper_bound = probe
            console.print(f"  ‚úó Slide {probe} doesn't exist")
            break

    if upper_bound is None:
        # Document has more than 512 slides, keep probing
        upper_bound = 1024
        while probe_slide_exists(session, document_code, upper_bound, full_path):
            lower_bound = upper_bound
            upper_bound *= 2
            if upper_bound > 10000:  # Safety limit
                break

    # Step 3: Binary search between lower_bound and upper_bound
    console.print(f"[cyan]Binary search between {lower_bound} and {upper_bound}...[/cyan]")

    while upper_bound - lower_bound > 1:
        mid = (lower_bound + upper_bound) // 2
        if probe_slide_exists(session, document_code, mid, full_path):
            lower_bound = mid
        else:
            upper_bound = mid

    slide_count = lower_bound
    console.print(f"\n[bold green]‚úì Document has {slide_count} slides[/bold green]\n")

    return slide_count


def download_all_slides_parallel(
    session, document_code, slide_count, output_dir, max_workers=8, full_path=None
):
    """Download all slides in parallel with progress tracking."""
    console.print(
        f"\n[bold]Downloading all {slide_count} slides in parallel (workers: {max_workers})...[/bold]\n"
    )

    def download_slide(slide_num, retries=3):
        """Download a single slide with retry logic."""
        import time as time_module
        last_error = None

        for attempt in range(retries):
            try:
                # Use full path if provided (for folder-based URLs)
                if full_path:
                    page_url = f"{full_path}/page_data/{slide_num}"
                else:
                    page_url = (
                        f"https://docsend.com/view/{document_code}/page_data/{slide_num}"
                    )
                resp = session.get(
                    page_url, headers={"Accept": "application/json"}, timeout=15
                )

                if resp.status_code == 200:
                    data = resp.json()
                    image_url = data.get("imageUrl")

                    if image_url:
                        img_resp = session.get(image_url, timeout=30)

                        if img_resp.status_code == 200:
                            filename = f"slide_{slide_num:03d}.png"
                            filepath = output_dir / filename

                            with open(filepath, "wb") as f:
                                f.write(img_resp.content)

                            return slide_num, True, None
                        else:
                            last_error = f"Image HTTP {img_resp.status_code}"
                    else:
                        last_error = "No imageUrl in response"
                elif resp.status_code == 404:
                    # Slide doesn't exist - don't retry
                    return slide_num, False, "Slide not found (404)"
                else:
                    last_error = f"API HTTP {resp.status_code}"

            except Exception as e:
                last_error = str(e)

            # Wait before retry (exponential backoff)
            if attempt < retries - 1:
                time_module.sleep(0.5 * (attempt + 1))

        return slide_num, False, last_error or "Failed after retries"

    successful = 0
    failed = []

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("({task.completed}/{task.total})"),
        console=console,
    ) as progress:
        download_task = progress.add_task(
            "[cyan]Downloading slides...", total=slide_count
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all download tasks
            futures = {
                executor.submit(download_slide, i): i for i in range(1, slide_count + 1)
            }

            # Process completed downloads
            for future in as_completed(futures):
                slide_num, success, error = future.result()

                if success:
                    successful += 1
                else:
                    failed.append((slide_num, error))

                progress.update(download_task, advance=1)

    # Validate downloaded files on disk
    actual_files = list(output_dir.glob("slide_*.png"))

    console.print(f"\n[bold]Download summary:[/bold]")
    console.print(f"   Expected: {slide_count} slides")
    console.print(f"   Downloaded: {successful} slides")
    console.print(f"   Files on disk: {len(actual_files)} PNG files")

    if failed:
        # Separate 404s (slide doesn't exist) from real failures
        not_found = [(s, e) for s, e in failed if "404" in str(e) or "not found" in str(e).lower()]
        real_failures = [(s, e) for s, e in failed if (s, e) not in not_found]

        if not_found:
            console.print(f"   [yellow]Not found (beyond doc end): {len(not_found)} slides[/yellow]")
        if real_failures:
            console.print(f"   [red]Failed downloads: {len(real_failures)} slides[/red]")
            for slide, error in real_failures[:5]:
                console.print(f"      - Slide {slide}: {error}")
            if len(real_failures) > 5:
                console.print(f"      ... and {len(real_failures) - 5} more")

    # Return actual files on disk (most accurate count)
    return len(actual_files)


def download_dropbox_embedded_document(url, output_dir, browser_name="brave"):
    """Download a Dropbox-embedded document (e.g., Excel spreadsheet) via screenshots.

    Some DocSend documents are embedded Excel/spreadsheet files rendered via Dropbox's
    preview service. These don't have traditional slides accessible via the API.
    Instead, we capture screenshots of each spreadsheet tab.

    Args:
        url: The DocSend document URL
        output_dir: Directory to save slide screenshots
        browser_name: Browser to load cookies from

    Returns:
        Number of slides captured, or None if not a Dropbox-embedded document
    """
    console.print("\n[cyan]Checking if document is Dropbox-embedded (spreadsheet)...[/cyan]")

    import time

    # Load cookies
    browser_funcs = {
        "brave": browser_cookie3.brave,
        "chrome": browser_cookie3.chrome,
        "firefox": browser_cookie3.firefox,
        "safari": browser_cookie3.safari,
        "edge": browser_cookie3.edge,
        "chromium": browser_cookie3.chromium,
    }

    if browser_name.lower() not in browser_funcs:
        return None

    cj = browser_funcs[browser_name.lower()](domain_name="docsend.com")
    cookies = []
    for c in cj:
        cookie = {
            "name": c.name,
            "value": c.value,
            "domain": c.domain if c.domain else ".docsend.com",
            "path": c.path if c.path else "/",
        }
        if c.expires:
            cookie["expires"] = int(c.expires)
        if c.secure:
            cookie["secure"] = bool(c.secure)
        cookies.append(cookie)

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            viewport={"width": 1920, "height": 1200},
        )
        context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")
        context.add_cookies(cookies)

        page = context.new_page()
        page.set_default_timeout(60000)

        page.goto(url, wait_until="networkidle", timeout=60000)
        time.sleep(3)

        # Check for Dropbox iframe
        iframes = page.query_selector_all('iframe')
        dropbox_iframe = None
        for iframe in iframes:
            src = iframe.get_attribute('src') or ''
            if 'dropboxusercontent.com' in src:
                dropbox_iframe = iframe
                break

        if not dropbox_iframe:
            console.print("[dim]Not a Dropbox-embedded document[/dim]")
            browser.close()
            return None

        console.print("[green]‚úì Detected Dropbox-embedded spreadsheet[/green]")

        frame = dropbox_iframe.content_frame()
        if not frame:
            console.print("[red]Could not access Dropbox iframe content[/red]")
            browser.close()
            return None

        # Wait for content to render
        time.sleep(2)

        # Find spreadsheet tabs (links with # href inside the iframe)
        # Only include visible tabs with meaningful names
        links = frame.query_selector_all('a')
        tab_links = []
        seen_names = set()  # Avoid duplicate tab names like [1], [2]
        for link in links:
            href = link.get_attribute('href') or ''
            text = link.inner_text().strip()
            # Tabs typically have # as href and meaningful text
            # Skip invisible elements and duplicates
            if href == '#' and text and len(text) < 100:
                # Check if element is visible
                if not link.is_visible():
                    continue
                # Skip duplicate names (like [1], [2] markers)
                if text in seen_names:
                    continue
                # Skip generic markers that are likely navigation arrows
                if text in ['[1]', '[2]', '[3]', '>', '<', '...']:
                    continue
                seen_names.add(text)
                tab_links.append((link, text))

        if tab_links:
            console.print(f"[cyan]Found {len(tab_links)} spreadsheet tabs[/cyan]")
            for _, text in tab_links:
                console.print(f"  ‚Ä¢ {text}")
        else:
            # Single-tab spreadsheet
            console.print("[cyan]Single-tab spreadsheet[/cyan]")

        # Capture slides
        slides_captured = 0

        if tab_links:
            # Multi-tab: capture each tab
            for i, (link, text) in enumerate(tab_links, start=1):
                try:
                    if i > 1:
                        # Click to switch tabs (first tab is already visible)
                        # Use short timeout to avoid hanging on invisible elements
                        link.click(timeout=5000)
                        time.sleep(1.5)  # Wait for tab content to load

                    slide_path = output_path / f"slide_{i:03d}.png"
                    dropbox_iframe.screenshot(path=str(slide_path))
                    console.print(f"  ‚úì Captured tab {i}: {text}")
                    slides_captured += 1
                except Exception as e:
                    # Log but continue with other tabs
                    console.print(f"  ‚úó Skipped tab {i} ({text}): not clickable")
        else:
            # Single view - capture once
            slide_path = output_path / "slide_001.png"
            dropbox_iframe.screenshot(path=str(slide_path))
            console.print("  ‚úì Captured single view")
            slides_captured = 1

        browser.close()

    console.print(f"\n[green]‚úì Captured {slides_captured} slide(s) from Dropbox-embedded document[/green]")
    return slides_captured


def create_pdfs_verbose(
    sandbox_dir, document_code, document_title=None, document_date=None, ocr_jobs=None, final_output_dir=None
):
    """Create PDFs with OCR and compression - with verbose progress.

    Args:
        sandbox_dir: Directory containing slides and for intermediate files
        document_code: Document code for naming
        document_title: Optional title for the document
        document_date: Optional date string
        ocr_jobs: Number of OCR jobs to run in parallel
        final_output_dir: Where to place the final PDF (default: sandbox_dir parent)
    """
    console.print("\n[bold]Creating PDFs with OCR and compression...[/bold]\n")

    # Find slides in sandbox
    slides = sorted(sandbox_dir.glob("slide_*.png"))
    if not slides:
        console.print("[red]No slides found to create PDF[/red]")
        return

    # Where the final PDF will go
    if final_output_dir is None:
        final_output_dir = sandbox_dir.parent

    console.print(f"üìÅ Found {len(slides)} slides in sandbox")

    # Get today's date in YYYY-MM-DD format
    today = datetime.now().strftime("%Y-%m-%d")

    # Use document title if available, otherwise use code
    title_part = document_title if document_title else document_code

    # Build base name: "YYYY-MM-DD Title (doc_date)" or "YYYY-MM-DD Title"
    if document_date:
        base_name = f"{today} {title_part} ({document_date})"
    else:
        base_name = f"{today} {title_part}"

    # 1. Create raw PDF (in sandbox)
    try:
        import img2pdf
        from PIL import Image

        pdf_path = sandbox_dir / f"{base_name}.pdf"
        console.print("üìÑ Creating raw PDF...")
        console.print(f"   Input: {len(slides)} PNG files")
        console.print(f"   Output: {pdf_path.name}")

        # Convert slides to RGB (remove alpha channel) to avoid img2pdf warnings
        import io
        pdf_bytes = []
        for slide in slides:
            img = Image.open(slide)
            if img.mode == 'RGBA':
                # Create white background and composite
                background = Image.new('RGB', img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[3])  # Use alpha as mask
                img = background
            elif img.mode != 'RGB':
                img = img.convert('RGB')
            # Save to bytes
            buf = io.BytesIO()
            img.save(buf, format='PNG')
            pdf_bytes.append(buf.getvalue())

        with open(pdf_path, "wb") as f:
            f.write(img2pdf.convert(pdf_bytes))

        size_mb = pdf_path.stat().st_size / (1024 * 1024)
        console.print(f"   ‚úì Created: {pdf_path.name} ({size_mb:.1f} MB)")

    except ImportError:
        console.print("[yellow]img2pdf not installed, trying ImageMagick...[/yellow]")
        try:
            pdf_path = sandbox_dir / f"{base_name}.pdf"
            subprocess.run(
                ["convert"] + [str(s) for s in slides] + [str(pdf_path)], check=True
            )
            console.print(f"  ‚úì Created: {pdf_path}")
        except (subprocess.CalledProcessError, FileNotFoundError):
            console.print(
                "[red]Could not create PDF - install img2pdf or ImageMagick[/red]"
            )
            return

    # 2. Create OCR PDF with progress bar (in sandbox)
    if pdf_path.exists():
        try:
            ocr_path = sandbox_dir / f"{base_name} [OCR uncompressed].pdf"
            console.print("\nüîç Creating searchable PDF with OCR...")
            console.print(f"   Input: {pdf_path.name}")
            console.print(f"   Output: {ocr_path.name}")
            # Show CPU usage info
            import os

            cpu_count = ocr_jobs or get_physical_cpu_count()
            console.print(
                f"   Settings: rotate pages, deskew, optimize, {cpu_count} CPU core{'s' if cpu_count != 1 else ''}"
            )

            # Count total pages for progress bar
            total_pages = len(slides)

            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total} pages)"),
                console=console,
            ) as progress:
                ocr_task = progress.add_task("   Processing", total=total_pages)

                # Run OCR in a separate thread to update progress
                import threading
                import time

                def run_ocr():
                    # Use specified number of CPU cores for OCR processing
                    import os

                    cpu_count = ocr_jobs or get_physical_cpu_count()
                    return subprocess.run(
                        [
                            "ocrmypdf",
                            "--rotate-pages",
                            "--deskew",
                            "--optimize",
                            "3",
                            "--jobs",
                            str(cpu_count),  # Use multiple CPU cores
                            str(pdf_path),
                            str(ocr_path),
                        ],
                        capture_output=True,
                        text=True,
                    )

                # Start OCR in thread
                result_container = []
                ocr_thread = threading.Thread(
                    target=lambda: result_container.append(run_ocr())
                )
                ocr_thread.start()

                # Update progress bar while OCR runs
                pages_per_second = 0.5  # Estimate based on typical OCR speed
                elapsed_time = 0
                while ocr_thread.is_alive():
                    time.sleep(0.1)
                    elapsed_time += 0.1
                    estimated_pages = min(
                        int(elapsed_time * pages_per_second), total_pages - 1
                    )
                    progress.update(ocr_task, completed=estimated_pages)

                ocr_thread.join()
                result = result_container[0]

                # Complete the progress bar
                progress.update(ocr_task, completed=total_pages)

            if result.returncode == 0:
                size_mb = ocr_path.stat().st_size / (1024 * 1024)
                console.print(f"   ‚úì Created: {ocr_path.name} ({size_mb:.1f} MB)")
            else:
                console.print(
                    f"   ‚úó OCR failed: {result.stderr if result.stderr else 'Unknown error'}"
                )
                return  # Don't proceed to compression if OCR failed

        except FileNotFoundError:
            console.print("[yellow]  ocrmypdf not installed - skipping OCR[/yellow]")
            console.print("  Install with: brew install ocrmypdf")

    # 3. Try ghostscript compression, keep smaller of the two
    if (sandbox_dir / f"{base_name} [OCR uncompressed].pdf").exists():
        final_path = final_output_dir / f"{base_name} [OCR].pdf"
        ocr_size = ocr_path.stat().st_size

        try:
            compressed_path = sandbox_dir / f"{base_name} [OCR compressed].pdf"
            console.print("\nüóúÔ∏è  Trying ghostscript compression...")
            console.print(f"   Input: {ocr_path.name}")
            console.print("   Settings: PDF 1.4, ebook preset (150 dpi)")

            result = subprocess.run(
                [
                    "gs",
                    "-sDEVICE=pdfwrite",
                    "-dCompatibilityLevel=1.4",
                    "-dPDFSETTINGS=/ebook",
                    "-dNOPAUSE",
                    "-dBATCH",
                    f"-sOutputFile={compressed_path}",
                    str(ocr_path),
                ],
                capture_output=True,
                text=True,
            )

            if result.returncode == 0:
                compressed_size = compressed_path.stat().st_size
                ocr_size_mb = ocr_size / (1024 * 1024)
                compressed_size_mb = compressed_size / (1024 * 1024)

                if compressed_size < ocr_size:
                    # Ghostscript version is smaller, use it
                    reduction = ((ocr_size - compressed_size) / ocr_size) * 100
                    console.print(
                        f"   ‚úì Compressed: {compressed_size_mb:.1f} MB (was {ocr_size_mb:.1f} MB, -{reduction:.1f}%)"
                    )
                    import shutil
                    shutil.copy2(compressed_path, final_path)
                    console.print(f"   ‚úì Using compressed version")
                else:
                    # OCR version is smaller or same, use it
                    console.print(
                        f"   ‚úó Compressed larger: {compressed_size_mb:.1f} MB vs {ocr_size_mb:.1f} MB"
                    )
                    console.print(f"   ‚úì Using OCR version (already optimal)")
                    import shutil
                    shutil.copy2(ocr_path, final_path)
            else:
                console.print(f"   ‚úó Compression failed, using OCR version")
                import shutil
                shutil.copy2(ocr_path, final_path)

        except FileNotFoundError:
            console.print(
                "[yellow]  Ghostscript not installed - using OCR version[/yellow]"
            )
            console.print("  Install with: brew install ghostscript")
            import shutil
            shutil.copy2(ocr_path, final_path)

        # Verify page count in final PDF
        try:
            from pypdf import PdfReader
            reader = PdfReader(str(final_path))
            pdf_pages = len(reader.pages)
        except ImportError:
            try:
                # Fallback to pdfinfo
                result = subprocess.run(
                    ["pdfinfo", str(final_path)],
                    capture_output=True,
                    text=True,
                )
                for line in result.stdout.split("\n"):
                    if line.startswith("Pages:"):
                        pdf_pages = int(line.split(":")[1].strip())
                        break
                else:
                    pdf_pages = None
            except Exception:
                pdf_pages = None
        except Exception:
            pdf_pages = None

        # Show final file sizes comparison
        final_size_mb = final_path.stat().st_size / (1024 * 1024)
        console.print("\n[bold]Final summary:[/bold]")
        console.print(f"   Input slides: {len(slides)} PNG files")
        if pdf_pages:
            if pdf_pages == len(slides):
                console.print(f"   Output pages: [green]{pdf_pages} pages ‚úì[/green]")
            else:
                console.print(f"   Output pages: [red]{pdf_pages} pages (mismatch!)[/red]")
        console.print(f"\n[bold]File sizes:[/bold]")
        console.print(f"   [dim]sandbox/[/dim]{pdf_path.name}: {pdf_path.stat().st_size / (1024 * 1024):.1f} MB (raw)")
        console.print(f"   [dim]sandbox/[/dim]{ocr_path.name}: {ocr_path.stat().st_size / (1024 * 1024):.1f} MB")
        console.print(f"   [green]{final_path.name}: {final_size_mb:.1f} MB[/green] ‚Üê final")


def download_docsend(
    url,
    email=None,
    passcode="",
    slide_count=None,
    max_workers=8,
    ocr_jobs=None,
    basic_title=False,
    llm_title_pages=10,
    cookies_from_browser=None,
    output_dir=None,
    _folder_name=None,  # Internal: folder name for data room organization
    _dataroom_title=None,  # Internal: document title from data room (skip Claude title inference)
    dry_run=False,  # If True, only show what would be downloaded
):
    """Download DocSend presentation with enhanced features.

    Args:
        output_dir: Base output directory (default: current directory)
        _folder_name: Internal param for organizing data room downloads by folder
        _dataroom_title: Internal param for document title from data room listing
        dry_run: If True, only discover and show slide counts without downloading
    """

    logger.info(f"Starting download for URL: {url}")

    # Use curl_cffi with Chrome impersonation to avoid TLS fingerprint detection
    session = curl_requests.Session(impersonate="chrome")
    logger.info("Using curl_cffi with Chrome impersonation (stealth mode)")

    # Headers for specific requests (curl_cffi's impersonate handles most headers)
    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
    }

    # Default browser for cookie loading (can be overridden with --cookies-from-browser)
    if not cookies_from_browser:
        cookies_from_browser = "brave"

    # Check if this is a data room URL (/view/s/xxx) - needs special handling
    parsed_check = urlparse(url)
    path_check = parsed_check.path.strip("/").split("/")
    is_dataroom_listing = "view" in path_check and "s" in path_check and "/d/" not in url
    is_dataroom_document = "view" in path_check and "s" in path_check and "/d/" in url

    if is_dataroom_listing:
        # Data rooms always need browser cookies
        console.print("\n[bold cyan]üìÇ Data room detected[/bold cyan]")
        console.print(f"   Using browser: [cyan]{cookies_from_browser}[/cyan]")

        cookies = load_cookies_from_browser(cookies_from_browser)
        if not cookies:
            console.print(f"   [red]‚úó No cookies found for docsend.com in {cookies_from_browser}[/red]")
            console.print(f"   [yellow]Please authenticate in {cookies_from_browser} first, then retry[/yellow]")
            return None

        for name, value in cookies.items():
            session.cookies.set(name, value, domain=".docsend.com")
        console.print(f"   [green]‚úì Cookies loaded - proceeding with download[/green]")

        # Discover all documents in the data room
        documents = discover_dataroom_documents(url, cookies_from_browser)

        if not documents:
            console.print("[yellow]No documents found in data room[/yellow]")
            return None

        # Download each document
        console.print(f"\n[bold cyan]Downloading {len(documents)} document(s) from data room...[/bold cyan]")

        if dry_run:
            # Dry run mode: show documents and probe slide counts without downloading
            console.print(f"\n[bold yellow]DRY RUN: Probing {len(documents)} document(s)[/bold yellow]\n")

            current_folder = None
            for i, (doc_name, doc_url, folder_name) in enumerate(documents, 1):
                # Print folder header when it changes
                if folder_name != current_folder:
                    current_folder = folder_name
                    console.print(f"\n[bold cyan]üìÅ {folder_name}[/bold cyan]")

                # Try to get slide count from HTML
                try:
                    temp_session = curl_requests.Session(impersonate="chrome")
                    for name, value in cookies.items():
                        temp_session.cookies.set(name, value, domain=".docsend.com")
                    resp = temp_session.get(doc_url, timeout=15)
                    match = re.search(r'(\d+)\s*PAGES?', resp.text, re.IGNORECASE)
                    pages = match.group(1) if match else "?"
                except:
                    pages = "?"

                console.print(f"   {i:2d}. [green]{doc_name}[/green] ({pages} pages)")
                console.print(f"       [dim]{doc_url}[/dim]")

            console.print(f"\n[bold yellow]DRY RUN complete - no files downloaded[/bold yellow]")
            return None

        # Use provided output_dir or current directory
        base_output_dir = Path(output_dir) if output_dir else Path(".")
        base_output_dir.mkdir(parents=True, exist_ok=True)

        for i, (doc_name, doc_url, folder_name) in enumerate(documents, 1):
            console.print(f"\n{'='*60}")
            console.print(f"[bold]Document {i}/{len(documents)}: {doc_name}[/bold]")
            console.print(f"[dim]Folder: {folder_name}[/dim]")
            console.print(f"{'='*60}")

            # Recursively download this document, passing the title from the data room
            download_docsend(
                doc_url,
                email=email,
                passcode=passcode,
                slide_count=slide_count,
                max_workers=max_workers,
                ocr_jobs=ocr_jobs,
                basic_title=basic_title,
                llm_title_pages=llm_title_pages,
                cookies_from_browser=cookies_from_browser,
                output_dir=str(base_output_dir),
                _folder_name=folder_name,
                _dataroom_title=doc_name,  # Use title from data room listing
            )

        console.print(f"\n[bold green]‚úì Completed downloading {len(documents)} document(s) from data room[/bold green]")
        console.print(f"[bold]Output directory: {base_output_dir.absolute()}[/bold]")
        return None

    # Handle single document from a data room - needs browser cookies loaded first
    if is_dataroom_document:
        console.print("\n[bold cyan]üìÑ Data room document detected[/bold cyan]")
        console.print(f"   Using browser: [cyan]{cookies_from_browser}[/cyan]")

        cookies = load_cookies_from_browser(cookies_from_browser)
        if not cookies:
            console.print(f"   [red]‚úó No cookies found for docsend.com in {cookies_from_browser}[/red]")
            console.print(f"   [yellow]Please authenticate in {cookies_from_browser} first, then retry[/yellow]")
            return None

        for name, value in cookies.items():
            session.cookies.set(name, value, domain=".docsend.com")
        console.print(f"   [green]‚úì Cookies loaded[/green]")

    # First, follow any redirects to get the actual document URL
    logger.info("Following redirects to resolve actual document URL...")
    try:
        initial_response = session.get(url, allow_redirects=True)
        resolved_url = initial_response.url
        logger.info(f"Resolved URL: {resolved_url}")

        # If we got redirected, use the final URL
        if resolved_url != url:
            console.print(f"[yellow]Redirected to: {resolved_url}[/yellow]")
            url = resolved_url
    except Exception as e:
        logger.error(f"Error resolving URL: {e}")
        console.print(f"[red]Error resolving URL: {e}[/red]")
        return None

    # Try to extract document title from HTML <title> tag
    html_page_title = extract_title_from_html(initial_response.text)
    if html_page_title:
        logger.debug(f"Extracted title from page HTML: {html_page_title}")

    # Extract document code from the resolved URL
    parsed_url = urlparse(url)
    # Clean path by removing query parameters and fragments
    clean_path = parsed_url.path.strip("/")
    path_parts = clean_path.split("/")

    # Handle various URL patterns:
    # - /view/xxx (simple)
    # - /view/s/xxx (space/shared)
    # - /view/xxx/d/yyy (folder with subdocument)
    full_doc_path = None  # Will be used for API calls on folder URLs

    if "view" in path_parts:
        view_index = path_parts.index("view")
        if view_index + 1 < len(path_parts):
            next_part = path_parts[view_index + 1]

            # Check for /view/s/xxx pattern (space/shared URL)
            if next_part == "s" and len(path_parts) > view_index + 2:
                # Check for /view/s/DATAROOM/d/DOCUMENT pattern
                if len(path_parts) > view_index + 4 and path_parts[view_index + 3] == "d":
                    # Data room with specific document: /view/s/DATAROOM/d/DOCUMENT
                    dataroom_code = path_parts[view_index + 2]
                    document_code = path_parts[view_index + 4]
                    full_doc_path = (
                        f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                    )
                    logger.debug(
                        f"Data room document URL. Dataroom: {dataroom_code}, Document: {document_code}, Full path: {full_doc_path}"
                    )
                else:
                    # Simple space URL: /view/s/DOCUMENT
                    document_code = path_parts[view_index + 2]
                    # Store the full path for API calls
                    full_doc_path = (
                        f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                    )
                    logger.debug(
                        f"Space URL detected. Document: {document_code}, Full path: {full_doc_path}"
                    )
            # Check if this is a folder URL with /d/ subdocument
            elif len(path_parts) > view_index + 2 and path_parts[view_index + 2] == "d":
                # Folder URL: /view/FOLDER/d/DOCUMENT
                folder_code = path_parts[view_index + 1]
                document_code = (
                    path_parts[view_index + 3]
                    if len(path_parts) > view_index + 3
                    else folder_code
                )
                # Store the full path for API calls
                full_doc_path = (
                    f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                )
                logger.debug(
                    f"Folder URL detected. Using document: {document_code}, Full path: {full_doc_path}"
                )
            else:
                # Simple URL: /view/DOCUMENT
                document_code = path_parts[view_index + 1]
        else:
            document_code = path_parts[-1]
    else:
        # For other patterns, use the last part
        document_code = path_parts[-1]

    # Ensure document code doesn't contain query parameters
    if "?" in document_code:
        document_code = document_code.split("?")[0]
    if "#" in document_code:
        document_code = document_code.split("#")[0]

    logger.debug(f"Extracted document code: {document_code}")

    # Rebuild clean URL without query parameters
    clean_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
    url = clean_url  # Use clean URL for all subsequent requests

    console.print("\n[bold cyan]DocSend Downloader (Enhanced)[/bold cyan]")
    console.print(f"Document URL: {url}")
    console.print(f"Document Code: {document_code}")
    console.print(f"Parallel workers: {max_workers}")

    # Progressive authentication for regular documents:
    # 1. Try public access (no auth)
    # 2. Try platypus email (no passcode)
    # 3. Try browser cookies
    # 4. Ask user for passcode or to login via browser

    console.print("\n[bold]Checking access...[/bold]")
    authenticated_email = None

    # Helper to test if we can access the document
    def can_access_document():
        test_url = f"{full_doc_path}/page_data/1" if full_doc_path else f"https://docsend.com/view/{document_code}/page_data/1"
        try:
            test_resp = session.get(test_url, headers={"Accept": "application/json"}, timeout=10)
            if test_resp.status_code == 200:
                data = test_resp.json()
                return "imageUrl" in data
        except Exception:
            pass
        return False

    # For data room documents, cookies are already loaded - just check access
    if is_dataroom_document:
        if can_access_document():
            authenticated_email = f"({cookies_from_browser} cookies)"
            console.print(f"   [green]‚úì Authenticated via {cookies_from_browser} cookies[/green]")
        else:
            console.print(f"   [red]‚úó Browser cookies didn't work[/red]")
            console.print(f"   [yellow]Please visit the data room in {cookies_from_browser} to refresh your session[/yellow]")
            return None
    # Step 1: Try direct access (public document)
    elif can_access_document():
        console.print("   [green]‚úì Document is public - no authentication needed[/green]")
    else:
        # Step 2: Try platypus email (with passcode if provided)
        console.print("   [yellow]‚úó Not public[/yellow]")
        console.print("   Trying platypus email...")

        privacy_email = "platypus@gmail.com"
        # Use provided passcode if available, otherwise try without
        auth_passcode = passcode if passcode else ""
        if authenticate_with_email(session, url, privacy_email, auth_passcode, headers):
            if can_access_document():
                authenticated_email = privacy_email
                console.print(f"   [green]‚úì Authenticated with {privacy_email}[/green]")

        if not authenticated_email:
            # Step 3: Try browser cookies
            console.print(f"   [yellow]‚úó Email auth failed[/yellow]")
            console.print(f"   Trying {cookies_from_browser} cookies...")

            browser_cookies = load_cookies_from_browser(cookies_from_browser)
            if browser_cookies:
                for name, value in browser_cookies.items():
                    session.cookies.set(name, value, domain=".docsend.com")

                if can_access_document():
                    authenticated_email = f"({cookies_from_browser} cookies)"
                    console.print(f"   [green]‚úì Authenticated via {cookies_from_browser} cookies[/green]")

        if not authenticated_email:
            # Step 4: Try Playwright with browser cookies + passcode if provided
            console.print(f"   [yellow]‚úó Browser cookies insufficient[/yellow]")

            if passcode:
                console.print(f"   Trying Playwright with passcode...")
                pw_cookies, final_url = authenticate_with_playwright_and_cookies(
                    url, cookies_from_browser, passcode, email
                )
                if pw_cookies is None and final_url is None:
                    # Email verification required - message already shown
                    return None
                if pw_cookies:
                    for name, value in pw_cookies.items():
                        session.cookies.set(name, value, domain=".docsend.com")

                    if can_access_document():
                        authenticated_email = email if email else f"({cookies_from_browser} + passcode)"
                        console.print(f"   [green]‚úì Authenticated via Playwright[/green]")

                        # Update URL if redirected
                        if final_url and final_url != url:
                            url = final_url
                            parsed_url = urlparse(url)
                            clean_path = parsed_url.path.strip("/")
                            path_parts = clean_path.split("/")
                            if "view" in path_parts:
                                view_index = path_parts.index("view")
                                if view_index + 1 < len(path_parts):
                                    next_part = path_parts[view_index + 1]
                                    if next_part == "s" and len(path_parts) > view_index + 2:
                                        document_code = path_parts[view_index + 2]
                                        full_doc_path = f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                                    elif len(path_parts) > view_index + 2 and path_parts[view_index + 2] == "d":
                                        document_code = path_parts[view_index + 3] if len(path_parts) > view_index + 3 else path_parts[view_index + 1]
                                        full_doc_path = f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                                    else:
                                        document_code = path_parts[view_index + 1]

        if not authenticated_email and not can_access_document():
            # All fallbacks failed
            console.print("\n[red]‚úó Could not access document[/red]")
            console.print("[yellow]Try one of:[/yellow]")
            console.print(f"   1. Login to docsend.com in {cookies_from_browser} and retry")
            console.print("   2. Run with --passcode YOUR_PASSCODE")
            console.print("   3. Run with --cookies-from-browser <browser>")
            return None

    if authenticated_email:
        console.print(f"\n[bold green]üìß Authenticated as: {authenticated_email}[/bold green]")

    # Create output directory structure first (needed for both normal and fallback paths)
    # Final PDF goes to: output_dir/[folder_name]/ or output_dir/ for root
    # Sandbox files go to: output_dir/0__sandbox/{document_code}/
    base_output_dir = Path(output_dir) if output_dir else Path(".")
    base_output_dir.mkdir(parents=True, exist_ok=True)

    # Determine where final PDF will go
    if _folder_name and _folder_name != "Home":
        safe_folder = "".join(c if c.isalnum() or c in " -_&" else "_" for c in _folder_name)
        pdf_output_dir = base_output_dir / safe_folder
    else:
        pdf_output_dir = base_output_dir
    pdf_output_dir.mkdir(parents=True, exist_ok=True)

    # Sandbox for intermediate files (slides, raw PDF, etc.)
    sandbox_base = base_output_dir / "0__sandbox" / document_code
    sandbox_base.mkdir(parents=True, exist_ok=True)

    # Auto-detect slide count
    is_dropbox_embedded = False
    if not slide_count:
        detected = detect_slide_count_auto(session, document_code, full_doc_path)

        if not detected:
            # Normal slide detection failed - try Dropbox-embedded document fallback
            console.print("[yellow]Standard slide detection failed, trying embedded document fallback...[/yellow]")

            if cookies_from_browser:
                dropbox_slides = download_dropbox_embedded_document(
                    url, str(sandbox_base), cookies_from_browser
                )
                if dropbox_slides:
                    slide_count = dropbox_slides
                    is_dropbox_embedded = True
                    console.print(f"[green]‚úì Successfully captured {slide_count} slide(s) from embedded document[/green]")
                else:
                    console.print("[red]Failed to detect slide count (not a standard or embedded document)[/red]")
                    return None
            else:
                console.print("[red]Failed to detect slide count[/red]")
                console.print("[yellow]Try using --cookies-from-browser for embedded documents[/yellow]")
                return None
        else:
            slide_count = detected

    # Download all slides in parallel (to sandbox) - skip if already captured via Dropbox fallback
    if is_dropbox_embedded:
        successful = slide_count  # Already downloaded via Dropbox fallback
        console.print(f"[dim]Slides already captured via embedded document handler[/dim]")
    else:
        successful = download_all_slides_parallel(
            session, document_code, slide_count, sandbox_base, max_workers, full_doc_path
        )

        # If all downloads failed (e.g., HTTP 403), try Dropbox-embedded fallback
        if successful == 0 and cookies_from_browser:
            console.print("\n[yellow]All image downloads failed - trying Dropbox-embedded fallback...[/yellow]")
            dropbox_slides = download_dropbox_embedded_document(
                url, str(sandbox_base), cookies_from_browser
            )
            if dropbox_slides:
                successful = dropbox_slides
                is_dropbox_embedded = True
                console.print(f"[green]‚úì Successfully captured {successful} slide(s) via Dropbox fallback[/green]")

    # Extract document title and date (from sandbox)
    document_title = None
    document_date = None

    # Priority order for title extraction:
    # 1. Data room title (most accurate, passed via _dataroom_title)
    # 2. HTML page title (from <title> tag - simple and reliable)
    # 3. Claude vision analysis (can extract formatted title + date from slides)
    # 4. OCR from first slide (basic fallback)

    if _dataroom_title:
        # Title from data room listing (most accurate)
        document_title = _dataroom_title
        console.print(f"\n[cyan]üìÇ Using title from data room: [green]{document_title}[/green][/cyan]")
        # Call Claude just for date extraction (if available)
        if CLAUDE_AVAILABLE:
            _, document_date = suggest_title_with_claude(
                sandbox_base, slide_count, llm_title_pages
            )
    elif html_page_title:
        # Title from HTML <title> tag (reliable for single documents)
        document_title = html_page_title
        console.print(f"\n[cyan]üìÑ Using title from page: [green]{document_title}[/green][/cyan]")
        # Call Claude just for date extraction (if available)
        if CLAUDE_AVAILABLE:
            _, document_date = suggest_title_with_claude(
                sandbox_base, slide_count, llm_title_pages
            )
    elif basic_title:
        # Use simple OCR-based extraction from first slide
        first_slide = sandbox_base / "slide_001.png"
        if first_slide.exists():
            document_title = extract_title_from_image(first_slide)
    elif CLAUDE_AVAILABLE:
        # Use Claude to suggest title and extract date from slide images
        document_title, document_date = suggest_title_with_claude(
            sandbox_base, slide_count, llm_title_pages
        )
        if not document_title:
            # Fallback to basic extraction if Claude fails
            console.print("[yellow]Falling back to basic title extraction...[/yellow]")
            first_slide = sandbox_base / "slide_001.png"
            if first_slide.exists():
                document_title = extract_title_from_image(first_slide)
    else:
        # Claude not available and no HTML/dataroom title - use OCR fallback
        console.print("\n[yellow]‚ö† Claude not available - using OCR for title extraction[/yellow]")
        first_slide = sandbox_base / "slide_001.png"
        if first_slide.exists():
            document_title = extract_title_from_image(first_slide)

    # Create PDFs with verbose output (sandbox for intermediate, pdf_output_dir for final)
    if successful > 0:
        create_pdfs_verbose(sandbox_base, document_code, document_title, document_date, ocr_jobs, pdf_output_dir)

    # Final summary
    console.print("\n[bold green]‚úì Complete![/bold green]")

    # Show what was created - check pdf_output_dir for final PDFs
    table = Table(title="Files Created")
    table.add_column("Location", style="dim")
    table.add_column("File", style="cyan")
    table.add_column("Size", style="green")

    # Show final PDFs from pdf_output_dir
    for file in sorted(pdf_output_dir.iterdir()):
        if file.is_file() and file.suffix == ".pdf":
            size_kb = file.stat().st_size / 1024
            if size_kb > 1024:
                size_str = f"{size_kb / 1024:.1f} MB"
            else:
                size_str = f"{size_kb:.1f} KB"
            # Show relative path from base_output_dir
            rel_path = file.parent.relative_to(base_output_dir) if file.parent != base_output_dir else Path(".")
            table.add_row(str(rel_path), file.name, size_str)

    console.print(table)

    # Show sandbox location
    console.print(f"\n[dim]Intermediate files: {sandbox_base.relative_to(base_output_dir)}/[/dim]")

    if authenticated_email:
        console.print(f"[dim]Downloaded using: {authenticated_email}[/dim]")

    if document_title:
        console.print(f"[dim]Document title: {document_title}[/dim]")

    return base_output_dir


def main():
    parser = argparse.ArgumentParser(
        description="DocSend downloader with enhanced OCR progress and title extraction"
    )

    parser.add_argument("url", help="DocSend URL")
    parser.add_argument("--email", help="Fallback email if privacy email fails")
    parser.add_argument("--passcode", default="", help="Passcode if required")
    parser.add_argument("--slides", type=int, help="Override automatic detection")
    parser.add_argument(
        "--workers",
        type=int,
        default=8,
        help="Number of parallel download workers (default: 8)",
    )
    parser.add_argument(
        "--ocr-jobs",
        type=int,
        help="Number of CPU cores for OCR processing (default: all available)",
    )
    parser.add_argument(
        "--basic-title",
        action="store_true",
        help="Use simple OCR-based title extraction instead of Claude",
    )
    parser.add_argument(
        "--llm-title-pages",
        type=int,
        default=DEFAULT_TITLE_PAGES,
        help=f"Number of pages for OCR + Claude title extraction (default: {DEFAULT_TITLE_PAGES})",
    )
    parser.add_argument(
        "--cookies-from-browser",
        choices=["brave", "chrome", "firefox", "safari", "edge", "chromium"],
        help="Load cookies from browser (for authenticated sessions)",
    )
    parser.add_argument(
        "--log",
        choices=["debug", "info", "warning", "error"],
        default="warning",
        help="Set logging level (default: warning)",
    )
    parser.add_argument(
        "--output-dir", "-o",
        default=".",
        help="Output directory (default: current dir). Final PDFs go to folder structure, intermediate files to 0__sandbox/",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be downloaded without actually downloading (discovery + slide count only)",
    )

    args = parser.parse_args()

    # Set up logging
    setup_logging(args.log)

    download_docsend(
        args.url,
        args.email,
        args.passcode,
        args.slides,
        args.workers,
        args.ocr_jobs,
        args.basic_title,
        args.llm_title_pages,
        args.cookies_from_browser,
        args.output_dir,
        dry_run=args.dry_run,
    )


if __name__ == "__main__":
    main()
