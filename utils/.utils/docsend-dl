#!/usr/bin/env python3
"""
DocSend Downloader - Enhanced version with verbose OCR and title extraction
"""

import argparse
import hashlib
import logging
import re
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

import requests

try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

try:
    import browser_cookie3
    BROWSER_COOKIES_AVAILABLE = True
except ImportError:
    BROWSER_COOKIES_AVAILABLE = False
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.table import Table

console = Console()

# Set up logging
logger = logging.getLogger(__name__)


def setup_logging(level):
    """Set up logging configuration."""
    logging_level = getattr(logging, level.upper(), logging.INFO)

    # Create formatter
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Create console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging_level)
    console_handler.setFormatter(formatter)

    # Set up root logger
    logging.basicConfig(level=logging_level, handlers=[console_handler])

    # Also configure requests library logging
    requests_logger = logging.getLogger("requests.packages.urllib3")
    requests_logger.setLevel(
        logging.WARNING if logging_level > logging.DEBUG else logging.DEBUG
    )

    logger.info(f"Logging configured at {level.upper()} level")


def get_image_hash(image_data):
    """Get hash of image data."""
    return hashlib.md5(image_data).hexdigest()


def extract_title_from_image(image_path):
    """Try to extract title from the first slide using OCR."""
    console.print(
        "\n[yellow]Attempting to extract document title from first slide...[/yellow]"
    )

    try:
        # Use tesseract to extract text from first slide
        result = subprocess.run(
            ["tesseract", str(image_path), "stdout", "--psm", "3"],
            capture_output=True,
            text=True,
        )

        if result.returncode == 0:
            text = result.stdout
            # Look for title-like text (first few lines, longer phrases)
            lines = [line.strip() for line in text.split("\n") if line.strip()]

            if lines:
                # Try to find a good title candidate
                # Usually the title is in the first few lines and is substantial
                for i, line in enumerate(lines[:5]):
                    # Skip very short lines or lines that look like dates/numbers
                    if len(line) > 10 and not re.match(r"^[\d\s\-/]+$", line):
                        # Clean up the title
                        title = re.sub(r"[^\w\s\-]", " ", line)
                        title = " ".join(title.split())[:50]  # Limit length
                        if title:
                            console.print(
                                f"[green]‚úì Found potential title: '{title}'[/green]"
                            )
                            return title

                # If no good title found, use first substantial line
                first_line = lines[0] if lines else ""
                if len(first_line) > 5:
                    title = re.sub(r"[^\w\s\-]", " ", first_line)
                    title = " ".join(title.split())[:50]
                    console.print(
                        f"[yellow]Using first line as title: '{title}'[/yellow]"
                    )
                    return title

    except FileNotFoundError:
        console.print(
            "[yellow]Tesseract not installed - cannot extract title from image[/yellow]"
        )
        console.print("Install with: brew install tesseract")
    except Exception as e:
        console.print(f"[yellow]Could not extract title: {e}[/yellow]")

    return None


def suggest_title_with_claude(output_dir, slide_count, max_pages=10):
    """Use claude -p with slide images to suggest a document title and extract date.

    Args:
        output_dir: Directory containing slide images
        slide_count: Total number of slides
        max_pages: Maximum number of pages to send to Claude (default: 10)

    Returns:
        Tuple of (title, date_str) where:
        - title: Suggested title (max 30 chars) or None if failed
        - date_str: Date string like "2024-01-15", "2024-01", "2024", or None
    """
    console.print("\n[cyan]ü§ñ Using Claude to analyze document...[/cyan]")

    # Determine how many slides to use
    pages_to_use = min(slide_count, max_pages)
    console.print(f"   Analyzing {pages_to_use} slide(s)...")

    # Collect slide filenames that exist
    slide_files = []
    for i in range(1, pages_to_use + 1):
        slide_path = output_dir / f"slide_{i:03d}.png"
        if slide_path.exists():
            slide_files.append(f"slide_{i:03d}.png")

    if not slide_files:
        console.print("[yellow]   No slides found[/yellow]")
        return None, None

    # Build file list for prompt
    files_str = ", ".join(slide_files)

    # Build claude command - claude reads files when instructed in the prompt
    prompt = (
        f"Read {files_str} in this directory and extract two things:\n\n"
        "1. TITLE: A concise, descriptive filename title (max 30 characters). "
        "Should capture what the document is about (company name, topic, purpose). "
        "Do not include generic words like 'Presentation', 'Slides', 'Deck'. "
        "Do not include dates in the title.\n\n"
        "2. DATE: Any document date you can find (creation date, last updated, "
        "quarter/year mentioned, etc). Format as YYYY-MM-DD if full date found, "
        "YYYY-MM if only month/year, or YYYY if only year. Use NONE if no date found.\n\n"
        "Output EXACTLY in this format, nothing else:\n"
        "TITLE: <title here>\n"
        "DATE: <date or NONE>"
    )

    cmd = ["claude", "-p", prompt]

    try:
        logger.debug(f"Running claude in {output_dir} with {len(slide_files)} files")
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(output_dir),
        )

        if result.returncode == 0 and result.stdout.strip():
            output = result.stdout.strip()
            logger.debug(f"Claude output: {output}")

            # Parse the output
            title = None
            date_str = None

            for line in output.split("\n"):
                line = line.strip()
                if line.upper().startswith("TITLE:"):
                    raw_title = line[6:].strip()
                    # Clean up the title for use as filename
                    raw_title = raw_title.strip('"\'')
                    # Remove invalid filename characters
                    clean_title = re.sub(r'[<>:"/\\|?*]', "", raw_title)
                    # Collapse whitespace and underscores to spaces
                    clean_title = clean_title.replace("_", " ")
                    clean_title = " ".join(clean_title.split())
                    # Truncate to 30 chars
                    clean_title = clean_title[:30].strip()
                    if clean_title:
                        title = clean_title

                elif line.upper().startswith("DATE:"):
                    raw_date = line[5:].strip()
                    if raw_date.upper() != "NONE" and raw_date:
                        # Validate date format (YYYY, YYYY-MM, or YYYY-MM-DD)
                        if re.match(r"^\d{4}(-\d{2}(-\d{2})?)?$", raw_date):
                            date_str = raw_date
                        else:
                            logger.debug(f"Invalid date format: {raw_date}")

            if title:
                console.print(f"   ‚úì Title: '[green]{title}[/green]'")
            if date_str:
                console.print(f"   ‚úì Document date: '[green]{date_str}[/green]'")
            elif title:
                console.print("   ‚úì No document date found")

            return title, date_str

        console.print(
            f"[yellow]   Claude returned no output (exit code: {result.returncode})[/yellow]"
        )
        if result.stderr:
            logger.debug(f"Claude stderr: {result.stderr}")

    except FileNotFoundError:
        console.print("[yellow]   'claude' command not found - install Claude CLI[/yellow]")
    except subprocess.TimeoutExpired:
        console.print("[yellow]   Claude timed out[/yellow]")
    except Exception as e:
        console.print(f"[yellow]   Error calling Claude: {e}[/yellow]")

    return None, None


def discover_dataroom_documents(url, browser_name):
    """Use Playwright to discover all documents in a DocSend data room.

    This uses a keyboard-based approach: select document row + Enter opens the
    document in a new tab, which reveals the actual document URL.

    Args:
        url: Data room URL (e.g., /view/s/xxx or /view/s/xxx/f/yyy)
        browser_name: Browser to load cookies from

    Returns:
        List of (name, url) tuples for each document found
    """
    if not PLAYWRIGHT_AVAILABLE:
        console.print("[red]Playwright not installed - needed for data room discovery.[/red]")
        console.print("[yellow]Install with: pip install playwright && playwright install chromium[/yellow]")
        return []

    if not BROWSER_COOKIES_AVAILABLE:
        console.print("[red]browser-cookie3 not installed - needed for authenticated data rooms.[/red]")
        return []

    console.print(f"\n[cyan]üìÇ Discovering documents in data room...[/cyan]")

    try:
        # Load cookies from browser
        import browser_cookie3
        import time

        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "firefox": browser_cookie3.firefox,
            "safari": browser_cookie3.safari,
            "edge": browser_cookie3.edge,
            "chromium": browser_cookie3.chromium,
        }

        if browser_name.lower() not in browser_funcs:
            console.print(f"[red]Unknown browser: {browser_name}[/red]")
            return []

        cj = browser_funcs[browser_name.lower()](domain_name="docsend.com")
        cookies = []
        for c in cj:
            cookie = {
                "name": c.name,
                "value": c.value,
                "domain": c.domain if c.domain else ".docsend.com",
                "path": c.path if c.path else "/",
            }
            if c.expires:
                cookie["expires"] = int(c.expires)
            if c.secure:
                cookie["secure"] = bool(c.secure)
            cookies.append(cookie)

        with sync_playwright() as p:
            # Use headless mode with anti-detection
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled', '--no-sandbox', '--disable-dev-shm-usage']
            )
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
            )

            # Remove webdriver detection
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            """)

            # Add cookies
            try:
                context.add_cookies(cookies)
            except Exception as e:
                logger.warning(f"Could not add all cookies: {e}")

            page = context.new_page()
            console.print(f"   Loading data room...")
            page.goto(url, wait_until="networkidle")
            time.sleep(3)

            # Check if we're on an error page or auth page
            page_title = page.title().lower()
            if "error" in page_title or "could not be satisfied" in page_title:
                console.print("[yellow]   Data room access failed - cookies may have expired[/yellow]")
                console.print("[yellow]   Try refreshing the page in your browser first[/yellow]")
                browser.close()
                return []

            # Check if auth form is present (email verification required)
            content = page.content().lower()
            if page.query_selector('input[type="email"]') and "verify" in content:
                console.print("[yellow]   Data room requires email verification[/yellow]")
                console.print("[yellow]   Please authenticate in your browser first[/yellow]")
                browser.close()
                return []

            # Get all folders from sidebar (look for items with "X items" count)
            folder_items = page.query_selector_all('[role="listitem"]')
            folders = []
            for item in folder_items:
                text = item.inner_text().strip()
                # Folders have "X items" or "X item" in their text
                if text and ("items" in text.lower() or "item" in text.lower()) and item.is_visible():
                    # Extract folder name (first line)
                    folder_name = text.split('\n')[0].strip()
                    if folder_name:
                        folders.append((item, folder_name))

            # Remove duplicates
            seen_names = set()
            unique_folders = []
            for item, name in folders:
                if name not in seen_names:
                    seen_names.add(name)
                    unique_folders.append((item, name))

            console.print(f"   Found {len(unique_folders)} folders")

            # Collect all documents
            all_documents = []
            seen_urls = set()

            # Helper function to get documents from current view
            def get_docs_from_view(folder_name):
                doc_rows = page.query_selector_all('[role="row"]')
                docs_found = []
                for row in doc_rows:
                    text = row.inner_text().strip().split('\n')[0]
                    # Skip headers, folders (have "items"), and empty rows
                    if text and "Sort" not in text and "items" not in text.lower() and "item" not in text.lower() and row.is_visible():
                        docs_found.append((row, text))
                return docs_found

            # First, get documents at root level (Home)
            # Click on "Home" in sidebar to ensure we're at root
            home_item = page.query_selector('[role="listitem"]:has-text("Home")')
            if home_item:
                try:
                    home_item.click()
                    time.sleep(1.5)

                    console.print(f"   üìÅ Home (root)")
                    root_docs = get_docs_from_view("Home")

                    for row, doc_name in root_docs:
                        pages_before = len(context.pages)
                        try:
                            row.click()
                            time.sleep(0.2)
                            page.keyboard.press("Enter")
                            time.sleep(1.5)

                            if len(context.pages) > pages_before:
                                new_page = context.pages[-1]
                                doc_url = new_page.url

                                if doc_url not in seen_urls and "/d/" in doc_url:
                                    seen_urls.add(doc_url)
                                    all_documents.append((doc_name, doc_url, "Home"))
                                    console.print(f"      ‚Ä¢ {doc_name[:45]}")

                                new_page.close()
                                time.sleep(0.3)
                        except Exception as e:
                            logger.debug(f"Error getting document URL: {e}")
                            continue
                except Exception:
                    pass

            # Then process each folder
            for folder_elem, folder_name in unique_folders:
                console.print(f"   üìÅ {folder_name}")

                # Click folder to show its documents
                try:
                    folder_elem.click()
                    time.sleep(1.5)
                except Exception:
                    continue

                # Get document rows in this folder
                docs_in_folder = get_docs_from_view(folder_name)

                # Get URL for each document by selecting + Enter
                for row, doc_name in docs_in_folder:
                    pages_before = len(context.pages)

                    try:
                        row.click()
                        time.sleep(0.2)
                        page.keyboard.press("Enter")
                        time.sleep(1.5)

                        # Check for new page
                        if len(context.pages) > pages_before:
                            new_page = context.pages[-1]
                            doc_url = new_page.url

                            if doc_url not in seen_urls and "/d/" in doc_url:
                                seen_urls.add(doc_url)
                                all_documents.append((doc_name, doc_url, folder_name))
                                console.print(f"      ‚Ä¢ {doc_name[:45]}")

                            new_page.close()
                            time.sleep(0.3)
                    except Exception as e:
                        logger.debug(f"Error getting document URL: {e}")
                        continue

            console.print(f"\n   ‚úì Found {len(all_documents)} document(s) total")

            browser.close()
            return all_documents

    except Exception as e:
        console.print(f"[red]   Error discovering documents: {e}[/red]")
        import traceback
        logger.debug(traceback.format_exc())
        return []


def load_cookies_from_browser(browser_name, domain="docsend.com"):
    """Load cookies from a browser for a specific domain.

    Args:
        browser_name: 'brave', 'chrome', 'firefox', 'safari', 'edge'
        domain: Domain to filter cookies for

    Returns:
        dict of cookies or None if failed
    """
    if not BROWSER_COOKIES_AVAILABLE:
        console.print("[red]browser-cookie3 not installed.[/red]")
        console.print("[yellow]Install with: pip install browser-cookie3[/yellow]")
        return None

    console.print(f"\n[cyan]üç™ Loading cookies from {browser_name}...[/cyan]")

    try:
        # Get the appropriate cookie jar
        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "firefox": browser_cookie3.firefox,
            "safari": browser_cookie3.safari,
            "edge": browser_cookie3.edge,
            "chromium": browser_cookie3.chromium,
        }

        if browser_name.lower() not in browser_funcs:
            console.print(f"[red]Unknown browser: {browser_name}[/red]")
            console.print(f"[yellow]Supported: {', '.join(browser_funcs.keys())}[/yellow]")
            return None

        cj = browser_funcs[browser_name.lower()](domain_name=domain)

        # Convert to dict
        cookies = {}
        for cookie in cj:
            cookies[cookie.name] = cookie.value

        if cookies:
            console.print(f"   ‚úì Loaded {len(cookies)} cookies from {browser_name}")
            return cookies
        else:
            console.print(f"[yellow]   No cookies found for {domain}[/yellow]")
            return None

    except Exception as e:
        console.print(f"[red]   Error loading cookies: {e}[/red]")
        return None


def authenticate_spa_with_playwright(url, email, passcode):
    """Use Playwright to authenticate on JavaScript SPA DocSend pages.

    Args:
        url: DocSend URL
        email: Email address for authentication
        passcode: Passcode for authentication

    Returns:
        tuple of (cookie_dict, final_url) if successful, (None, None) if failed
        final_url may be different from input url (e.g., space -> document redirect)
    """
    if not PLAYWRIGHT_AVAILABLE:
        console.print("[red]Playwright not installed.[/red]")
        console.print("[yellow]Install with: pip install playwright && playwright install chromium[/yellow]")
        return None, None

    console.print("\n[cyan]üé≠ Using Playwright for browser-based authentication...[/cyan]")

    try:
        with sync_playwright() as p:
            # Launch headless browser
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()

            console.print(f"   Loading page...")
            page.goto(url, wait_until="networkidle")

            # Wait for auth form to appear
            console.print(f"   Waiting for auth form...")
            try:
                # Look for email input
                email_input = page.wait_for_selector(
                    'input[type="email"], input[name*="email"], input[placeholder*="email" i]',
                    timeout=10000
                )
                if email_input:
                    console.print(f"   Filling email: [cyan]{email}[/cyan]")
                    email_input.fill(email)
            except Exception:
                console.print("[yellow]   No email field found[/yellow]")

            # Look for passcode input
            try:
                passcode_input = page.wait_for_selector(
                    'input[type="password"], input[name*="passcode"], input[name*="password"]',
                    timeout=5000
                )
                if passcode_input:
                    console.print(f"   Filling passcode...")
                    passcode_input.fill(passcode)
            except Exception:
                console.print("[yellow]   No passcode field found[/yellow]")

            # Click submit button
            try:
                submit_btn = page.wait_for_selector(
                    'button[type="submit"], button:has-text("Confirm"), button:has-text("Continue")',
                    timeout=5000
                )
                if submit_btn:
                    console.print(f"   Submitting form...")
                    submit_btn.click()
                    # Wait for navigation or content to load
                    page.wait_for_load_state("networkidle", timeout=15000)
            except Exception as e:
                console.print(f"[yellow]   Could not find/click submit button: {e}[/yellow]")

            # Check if authentication was successful
            import time
            time.sleep(3)  # Give page time to settle

            # Get the final URL (may have redirected to a document)
            final_url = page.url
            if final_url != url:
                console.print(f"   Redirected to: [cyan]{final_url}[/cyan]")

            # If this is a space/folder, look for document links
            # DocSend spaces show a list of documents after auth
            doc_links = page.query_selector_all('a[href*="/view/"][href*="/d/"]')
            if doc_links:
                console.print(f"   Found {len(doc_links)} document(s) in space")
                # Get the first document link
                first_doc = doc_links[0]
                doc_href = first_doc.get_attribute("href")
                if doc_href:
                    if not doc_href.startswith("http"):
                        doc_href = f"https://docsend.com{doc_href}"
                    console.print(f"   Using first document: [cyan]{doc_href}[/cyan]")
                    final_url = doc_href

            # Extract cookies
            cookies = context.cookies()
            cookie_dict = {c["name"]: c["value"] for c in cookies}

            # Verify auth worked by checking for document viewer elements
            try:
                # Look for signs of successful auth
                viewer = page.query_selector('[class*="viewer"], [class*="document"], [class*="page"]')
                if viewer or len(cookies) > 2:
                    console.print("[green]   ‚úì Authentication appears successful[/green]")
                    browser.close()
                    return cookie_dict, final_url
            except Exception:
                pass

            # If we got cookies, assume it worked
            if cookie_dict:
                console.print("[green]   ‚úì Got session cookies[/green]")
                browser.close()
                return cookie_dict, final_url

            console.print("[yellow]   ‚úó Authentication may have failed[/yellow]")
            browser.close()
            return None, None

    except Exception as e:
        console.print(f"[red]   Playwright error: {e}[/red]")
        return None, None


def authenticate_with_playwright_and_cookies(url, browser_name, passcode, email=None):
    """Use Playwright with browser cookies to authenticate on DocSend pages.

    This handles the case where CloudFront blocks direct requests but we can
    use Playwright with anti-detection measures to complete authentication.

    Args:
        url: DocSend URL
        browser_name: Browser to load cookies from ('brave', 'chrome', etc.)
        passcode: Passcode for authentication
        email: Optional email (if not provided, uses email from cookie session)

    Returns:
        tuple of (cookie_dict, final_url) if successful, (None, None) if failed
    """
    if not PLAYWRIGHT_AVAILABLE:
        console.print("[red]Playwright not installed.[/red]")
        return None, None

    if not BROWSER_COOKIES_AVAILABLE:
        console.print("[red]browser-cookie3 not installed.[/red]")
        return None, None

    console.print("\n[cyan]üé≠ Using Playwright with browser cookies...[/cyan]")

    try:
        # Load cookies from browser
        import browser_cookie3
        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "firefox": browser_cookie3.firefox,
            "safari": browser_cookie3.safari,
            "edge": browser_cookie3.edge,
            "chromium": browser_cookie3.chromium,
        }

        if browser_name.lower() not in browser_funcs:
            console.print(f"[red]Unknown browser: {browser_name}[/red]")
            return None, None

        cj = browser_funcs[browser_name.lower()](domain_name="docsend.com")
        cookies = []
        for c in cj:
            cookie = {
                "name": c.name,
                "value": c.value,
                "domain": c.domain if c.domain else ".docsend.com",
                "path": c.path if c.path else "/",
            }
            if c.expires:
                cookie["expires"] = int(c.expires)
            if c.secure:
                cookie["secure"] = bool(c.secure)
            cookies.append(cookie)

        console.print(f"   Loaded {len(cookies)} cookies from {browser_name}")

        with sync_playwright() as p:
            # Launch headless with anti-detection settings
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                ]
            )

            context = browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="en-US",
            )

            # Remove webdriver detection
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)

            # Add cookies
            try:
                context.add_cookies(cookies)
            except Exception as e:
                logger.warning(f"Could not add all cookies: {e}")

            page = context.new_page()

            console.print(f"   Loading page...")
            page.goto(url, wait_until="domcontentloaded", timeout=30000)

            import time
            time.sleep(3)  # Let page render

            # Check for auth form - use specific DocSend selectors
            email_input = page.query_selector('input[name="link_auth_form[email]"]')
            passcode_input = page.query_selector('input[name="link_auth_form[passcode]"]')

            if passcode_input and passcode:
                console.print(f"   Filling passcode...")
                passcode_input.fill(passcode)

                # If email field exists and is empty, fill it
                if email_input:
                    current_email = email_input.input_value()
                    if not current_email and email:
                        console.print(f"   Filling email: [cyan]{email}[/cyan]")
                        email_input.fill(email)
                    elif current_email:
                        console.print(f"   Email pre-filled: [cyan]{current_email}[/cyan]")

                # Click submit button - find visible one
                submit_btns = page.query_selector_all('button[type="submit"]')
                submit_btn = None
                for btn in submit_btns:
                    if btn.is_visible():
                        text = btn.inner_text().strip()
                        if text in ["Continue", "Submit", "Confirm", ""]:
                            submit_btn = btn
                            break

                if submit_btn:
                    console.print(f"   Clicking submit button...")
                    submit_btn.click()
                    page.wait_for_load_state("networkidle", timeout=15000)
                else:
                    console.print("[yellow]   Could not find visible submit button[/yellow]")

            time.sleep(3)  # Let page settle

            # Check if email verification is required
            content = page.content().lower()
            if "emailed a link" in content or "verify that you own" in content:
                console.print("\n[yellow]‚ö†Ô∏è  Email verification required[/yellow]")
                console.print("   DocSend sent a verification email to your address.")
                console.print("\n[bold]To complete authentication:[/bold]")
                console.print("   1. Check your email for a link from DocSend")
                console.print("   2. Click the link to verify in your browser")
                console.print("   3. Run this script again with --cookies-from-browser")
                console.print("\n   The verification link is usually valid for 15 minutes.")
                browser.close()
                return None, None

            # Get final URL and cookies
            final_url = page.url
            if final_url != url:
                console.print(f"   Redirected to: [cyan]{final_url}[/cyan]")

            # Extract cookies
            all_cookies = context.cookies()
            cookie_dict = {c["name"]: c["value"] for c in all_cookies}

            console.print(f"   ‚úì Got {len(cookie_dict)} cookies after auth")

            browser.close()
            return cookie_dict, final_url

    except Exception as e:
        console.print(f"[red]   Playwright error: {e}[/red]")
        return None, None


def detect_auth_requirements(html):
    """Detect what authentication fields are required from the page HTML.

    Returns:
        dict with keys: email_required, passcode_required
    """
    requirements = {
        "email_required": False,
        "passcode_required": False,
    }

    # Check for email field
    if "link_auth_form[email]" in html or 'name="link_auth_form[email]"' in html:
        requirements["email_required"] = True

    # Check for passcode/password field
    if "link_auth_form[passcode]" in html or 'name="link_auth_form[passcode]"' in html:
        requirements["passcode_required"] = True

    # Also check for common password field indicators in the visible text
    if re.search(
        r"<label[^>]*>.*?Password.*?</label>", html, re.IGNORECASE | re.DOTALL
    ):
        requirements["passcode_required"] = True

    return requirements


def authenticate_with_email(session, url, email, passcode, headers):
    """Try to authenticate with a specific email."""
    console.print(f"Trying email: [bold cyan]{email}[/bold cyan]")
    logger.info(f"Attempting authentication with email: {email}")

    # Get the page first to extract CSRF token
    response = session.get(url)
    logger.debug(f"Initial auth page response: {response.status_code}")

    # Extract CSRF token
    csrf_match = re.search(
        r'name="authenticity_token"\s+value="([^"]+)"', response.text
    )
    csrf_token = csrf_match.group(1) if csrf_match else None

    auth_data = {
        "utf8": "‚úì",
        "_method": "patch",
        "link_auth_form[email]": email,
        "link_auth_form[passcode]": passcode,
        "commit": "Continue",
    }

    # Add CSRF token if found
    if csrf_token:
        auth_data["authenticity_token"] = csrf_token

    auth_headers = headers.copy()
    auth_headers.update(
        {
            "Content-Type": "application/x-www-form-urlencoded",
            "Origin": "https://docsend.com",
            "Referer": url,
        }
    )

    logger.debug(f"Posting auth data to {url}")
    auth_response = session.post(
        url, data=auth_data, headers=auth_headers, allow_redirects=True
    )
    logger.debug(
        f"Auth response status: {auth_response.status_code}, History: {[r.status_code for r in auth_response.history]}"
    )

    # Check if authentication was successful
    if auth_response.status_code in [200, 302] or (
        auth_response.history
        and any(r.status_code == 302 for r in auth_response.history)
    ):
        # Additional check: try to access page_data/1
        # For folder URLs, we need to use the full path
        if "/d/" in url:
            # This is a folder URL, use the full path
            test_url = f"{url}/page_data/1"
        else:
            # Simple URL, use the document code
            document_code = url.strip("/").split("/")[-1]
            test_url = f"https://docsend.com/view/{document_code}/page_data/1"
        logger.debug(f"Testing authentication by accessing: {test_url}")
        test_response = session.get(test_url, headers={"Accept": "application/json"})

        if test_response.status_code == 200:
            try:
                data = test_response.json()
                if "imageUrl" in data:
                    console.print(
                        f"[bold green]‚úì Authentication successful with {email}[/bold green]"
                    )
                    return True
            except (ValueError, KeyError):
                pass

    console.print(f"[yellow]‚úó Authentication failed with {email}[/yellow]")
    return False


def download_image_batch(
    session, document_code, start_idx, batch_size=8, full_path=None
):
    """Download a batch of images in parallel."""
    console.print(
        f"[cyan]Downloading batch: slides {start_idx} to {start_idx + batch_size - 1}[/cyan]"
    )
    logger.debug(
        f"Starting batch download for slides {start_idx} to {start_idx + batch_size - 1}"
    )

    def download_single(idx):
        # Use full path if provided (for folder-based URLs)
        if full_path:
            page_url = f"{full_path}/page_data/{idx}"
        else:
            page_url = f"https://docsend.com/view/{document_code}/page_data/{idx}"

        try:
            logger.debug(f"Requesting page data for slide {idx}: {page_url}")
            resp = session.get(
                page_url, headers={"Accept": "application/json"}, timeout=10
            )
            logger.debug(f"Slide {idx} response: {resp.status_code}")

            if resp.status_code == 200:
                data = resp.json()
                image_url = data.get("imageUrl")

                if image_url:
                    logger.debug(f"Downloading image for slide {idx}: {image_url}")
                    img_resp = session.get(image_url, timeout=30)

                    if img_resp.status_code == 200:
                        img_hash = get_image_hash(img_resp.content)
                        console.print(
                            f"  ‚úì Slide {idx}: Downloaded (hash: {img_hash[:8]}...)"
                        )
                        return idx, img_hash, len(img_resp.content), img_resp.content
                    else:
                        console.print(f"  ‚úó Slide {idx}: Image download failed")
        except Exception as e:
            logger.error(f"Error downloading slide {idx}: {str(e)}")
            console.print(f"  ‚úó Slide {idx}: Error - {str(e)}")

        return idx, None, None, None

    # Download batch in parallel
    results = {}
    with ThreadPoolExecutor(max_workers=min(batch_size, 8)) as executor:
        futures = {
            executor.submit(download_single, i): i
            for i in range(start_idx, start_idx + batch_size)
        }

        for future in as_completed(futures):
            idx, img_hash, size, content = future.result()
            if img_hash:
                results[idx] = (img_hash, size, content)

    return results


def detect_slide_count_auto(session, document_code, full_path=None):
    """Automatically detect slide count without user interaction."""
    console.print("\n[bold yellow]Auto-detecting slide count...[/bold yellow]\n")
    logger.info(
        f"Starting automatic slide count detection for document: {document_code}"
    )

    all_hashes = {}
    batch_size = 8
    last_valid_slide = 0

    # First, check single slides to handle small documents
    console.print("[cyan]Checking first few slides individually...[/cyan]")
    for i in range(1, 6):
        results = download_image_batch(session, document_code, i, 1, full_path)
        if results:
            last_valid_slide = i
            for idx, (img_hash, size, content) in results.items():
                # Check for duplicates
                for prev_idx, (prev_hash, prev_size) in all_hashes.items():
                    if prev_hash == img_hash and idx > prev_idx:
                        logger.info(
                            f"Found duplicate: slide {idx} matches slide {prev_idx}"
                        )
                        console.print(
                            f"\n[bold green]‚úì Found duplicate: slide {idx} matches slide {prev_idx}[/bold green]"
                        )
                        console.print(
                            f"[bold green]Document has {prev_idx} slides[/bold green]\n"
                        )
                        return prev_idx
                all_hashes[idx] = (img_hash, size)
        else:
            # No valid slide at position i
            if i == 1:
                console.print(
                    "[red]Cannot access slide 1 - authentication may have failed[/red]"
                )
                return None
            else:
                logger.info(
                    f"No valid slide at position {i}, document has {i - 1} slides"
                )
                console.print(
                    f"\n[bold green]‚úì No valid slide at position {i}[/bold green]"
                )
                console.print(f"[bold green]Document has {i - 1} slides[/bold green]\n")
                return i - 1

    # For larger documents, use batch mode
    console.print(
        "\n[cyan]Document has more than 5 slides, switching to batch mode...[/cyan]"
    )

    for batch_num in range(1, 20):  # Check up to 160 slides
        start_idx = 5 + (batch_num - 1) * batch_size + 1
        console.print(f"\n[bold]Batch {batch_num}:[/bold]")
        batch_results = download_image_batch(
            session, document_code, start_idx, batch_size, full_path
        )

        if not batch_results:
            # Empty batch - we've reached the end
            console.print(
                f"\n[bold green]‚úì Empty batch - document ends at slide {last_valid_slide}[/bold green]\n"
            )
            return last_valid_slide

        # Update last valid slide
        if batch_results:
            last_valid_slide = max(batch_results.keys())

        # If batch is partially empty, we might be at the end
        if len(batch_results) < batch_size:
            console.print(
                f"\n[yellow]Batch returned only {len(batch_results)} slides (expected {batch_size})[/yellow]"
            )

            # Check if the missing slides are at the end
            missing_at_end = True
            for i in range(start_idx, start_idx + batch_size):
                if i <= last_valid_slide and i not in batch_results:
                    missing_at_end = False
                    break

            if missing_at_end:
                console.print(
                    f"[bold green]‚úì Reached end of document at slide {last_valid_slide}[/bold green]\n"
                )
                return last_valid_slide

        # Check for duplicates
        for idx, (img_hash, size, content) in batch_results.items():
            for prev_idx, (prev_hash, prev_size) in all_hashes.items():
                if prev_hash == img_hash and size == prev_size and idx > prev_idx + 2:
                    console.print(
                        f"\n[bold green]‚úì Found duplicate: slide {idx} matches slide {prev_idx}[/bold green]"
                    )
                    console.print(
                        f"[bold green]Document has {prev_idx} slides[/bold green]\n"
                    )
                    return prev_idx

            all_hashes[idx] = (img_hash, size)

    # If we've checked many slides without finding the end
    console.print(
        f"\n[yellow]Checked {last_valid_slide} slides without finding document end[/yellow]"
    )
    console.print(
        f"[bold green]Using last valid slide: {last_valid_slide}[/bold green]\n"
    )
    return last_valid_slide


def download_all_slides_parallel(
    session, document_code, slide_count, output_dir, max_workers=8, full_path=None
):
    """Download all slides in parallel with progress tracking."""
    console.print(
        f"\n[bold]Downloading all {slide_count} slides in parallel (workers: {max_workers})...[/bold]\n"
    )

    def download_slide(slide_num):
        """Download a single slide."""
        try:
            # Use full path if provided (for folder-based URLs)
            if full_path:
                page_url = f"{full_path}/page_data/{slide_num}"
            else:
                page_url = (
                    f"https://docsend.com/view/{document_code}/page_data/{slide_num}"
                )
            resp = session.get(
                page_url, headers={"Accept": "application/json"}, timeout=10
            )

            if resp.status_code == 200:
                data = resp.json()
                image_url = data.get("imageUrl")

                if image_url:
                    img_resp = session.get(image_url, timeout=30)

                    if img_resp.status_code == 200:
                        filename = f"slide_{slide_num:03d}.png"
                        filepath = output_dir / filename

                        with open(filepath, "wb") as f:
                            f.write(img_resp.content)

                        return slide_num, True, None

        except Exception as e:
            return slide_num, False, str(e)

        return slide_num, False, "Failed to download"

    successful = 0
    failed = []

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("({task.completed}/{task.total})"),
        console=console,
    ) as progress:
        download_task = progress.add_task(
            "[cyan]Downloading slides...", total=slide_count
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all download tasks
            futures = {
                executor.submit(download_slide, i): i for i in range(1, slide_count + 1)
            }

            # Process completed downloads
            for future in as_completed(futures):
                slide_num, success, error = future.result()

                if success:
                    successful += 1
                else:
                    failed.append((slide_num, error))

                progress.update(download_task, advance=1)

    console.print(f"\n‚úì Downloaded {successful}/{slide_count} slides")

    if failed:
        console.print(f"‚úó Failed: {len(failed)} slides")
        for slide, error in failed[:5]:
            console.print(f"  - Slide {slide}: {error}")
        if len(failed) > 5:
            console.print(f"  ... and {len(failed) - 5} more")

    return successful


def create_pdfs_verbose(
    sandbox_dir, document_code, document_title=None, document_date=None, ocr_jobs=None, final_output_dir=None
):
    """Create PDFs with OCR and compression - with verbose progress.

    Args:
        sandbox_dir: Directory containing slides and for intermediate files
        document_code: Document code for naming
        document_title: Optional title for the document
        document_date: Optional date string
        ocr_jobs: Number of OCR jobs to run in parallel
        final_output_dir: Where to place the final PDF (default: sandbox_dir parent)
    """
    console.print("\n[bold]Creating PDFs with OCR and compression...[/bold]\n")

    # Find slides in sandbox
    slides = sorted(sandbox_dir.glob("slide_*.png"))
    if not slides:
        console.print("[red]No slides found to create PDF[/red]")
        return

    # Where the final PDF will go
    if final_output_dir is None:
        final_output_dir = sandbox_dir.parent

    console.print(f"üìÅ Found {len(slides)} slides in sandbox")

    # Get today's date in YYYY-MM-DD format
    today = datetime.now().strftime("%Y-%m-%d")

    # Use document title if available, otherwise use code
    title_part = document_title if document_title else document_code

    # Build base name: "YYYY-MM-DD Title (doc_date)" or "YYYY-MM-DD Title"
    if document_date:
        base_name = f"{today} {title_part} ({document_date})"
    else:
        base_name = f"{today} {title_part}"

    # 1. Create raw PDF (in sandbox)
    try:
        import img2pdf
        from PIL import Image

        pdf_path = sandbox_dir / f"{base_name}.pdf"
        console.print("üìÑ Creating raw PDF...")
        console.print(f"   Input: {len(slides)} PNG files")
        console.print(f"   Output: {pdf_path.name}")

        # Convert slides to RGB (remove alpha channel) to avoid img2pdf warnings
        import io
        pdf_bytes = []
        for slide in slides:
            img = Image.open(slide)
            if img.mode == 'RGBA':
                # Create white background and composite
                background = Image.new('RGB', img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[3])  # Use alpha as mask
                img = background
            elif img.mode != 'RGB':
                img = img.convert('RGB')
            # Save to bytes
            buf = io.BytesIO()
            img.save(buf, format='PNG')
            pdf_bytes.append(buf.getvalue())

        with open(pdf_path, "wb") as f:
            f.write(img2pdf.convert(pdf_bytes))

        size_mb = pdf_path.stat().st_size / (1024 * 1024)
        console.print(f"   ‚úì Created: {pdf_path.name} ({size_mb:.1f} MB)")

    except ImportError:
        console.print("[yellow]img2pdf not installed, trying ImageMagick...[/yellow]")
        try:
            pdf_path = sandbox_dir / f"{base_name}.pdf"
            subprocess.run(
                ["convert"] + [str(s) for s in slides] + [str(pdf_path)], check=True
            )
            console.print(f"  ‚úì Created: {pdf_path}")
        except (subprocess.CalledProcessError, FileNotFoundError):
            console.print(
                "[red]Could not create PDF - install img2pdf or ImageMagick[/red]"
            )
            return

    # 2. Create OCR PDF with progress bar (in sandbox)
    if pdf_path.exists():
        try:
            ocr_path = sandbox_dir / f"{base_name} [OCR uncompressed].pdf"
            console.print("\nüîç Creating searchable PDF with OCR...")
            console.print(f"   Input: {pdf_path.name}")
            console.print(f"   Output: {ocr_path.name}")
            # Show CPU usage info
            import os

            cpu_count = ocr_jobs or os.cpu_count() or 4
            console.print(
                f"   Settings: rotate pages, deskew, optimize, {cpu_count} CPU core{'s' if cpu_count != 1 else ''}"
            )

            # Count total pages for progress bar
            total_pages = len(slides)

            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total} pages)"),
                console=console,
            ) as progress:
                ocr_task = progress.add_task("   Processing", total=total_pages)

                # Run OCR in a separate thread to update progress
                import threading
                import time

                def run_ocr():
                    # Use specified number of CPU cores for OCR processing
                    import os

                    cpu_count = ocr_jobs or os.cpu_count() or 4
                    return subprocess.run(
                        [
                            "ocrmypdf",
                            "--rotate-pages",
                            "--deskew",
                            "--optimize",
                            "3",
                            "--jobs",
                            str(cpu_count),  # Use multiple CPU cores
                            str(pdf_path),
                            str(ocr_path),
                        ],
                        capture_output=True,
                        text=True,
                    )

                # Start OCR in thread
                result_container = []
                ocr_thread = threading.Thread(
                    target=lambda: result_container.append(run_ocr())
                )
                ocr_thread.start()

                # Update progress bar while OCR runs
                pages_per_second = 0.5  # Estimate based on typical OCR speed
                elapsed_time = 0
                while ocr_thread.is_alive():
                    time.sleep(0.1)
                    elapsed_time += 0.1
                    estimated_pages = min(
                        int(elapsed_time * pages_per_second), total_pages - 1
                    )
                    progress.update(ocr_task, completed=estimated_pages)

                ocr_thread.join()
                result = result_container[0]

                # Complete the progress bar
                progress.update(ocr_task, completed=total_pages)

            if result.returncode == 0:
                size_mb = ocr_path.stat().st_size / (1024 * 1024)
                console.print(f"   ‚úì Created: {ocr_path.name} ({size_mb:.1f} MB)")
            else:
                console.print(
                    f"   ‚úó OCR failed: {result.stderr if result.stderr else 'Unknown error'}"
                )
                return  # Don't proceed to compression if OCR failed

        except FileNotFoundError:
            console.print("[yellow]  ocrmypdf not installed - skipping OCR[/yellow]")
            console.print("  Install with: brew install ocrmypdf")

    # 3. Try ghostscript compression, keep smaller of the two
    if (sandbox_dir / f"{base_name} [OCR uncompressed].pdf").exists():
        final_path = final_output_dir / f"{base_name} [OCR].pdf"
        ocr_size = ocr_path.stat().st_size

        try:
            compressed_path = sandbox_dir / f"{base_name} [OCR compressed].pdf"
            console.print("\nüóúÔ∏è  Trying ghostscript compression...")
            console.print(f"   Input: {ocr_path.name}")
            console.print("   Settings: PDF 1.4, ebook preset (150 dpi)")

            result = subprocess.run(
                [
                    "gs",
                    "-sDEVICE=pdfwrite",
                    "-dCompatibilityLevel=1.4",
                    "-dPDFSETTINGS=/ebook",
                    "-dNOPAUSE",
                    "-dBATCH",
                    f"-sOutputFile={compressed_path}",
                    str(ocr_path),
                ],
                capture_output=True,
                text=True,
            )

            if result.returncode == 0:
                compressed_size = compressed_path.stat().st_size
                ocr_size_mb = ocr_size / (1024 * 1024)
                compressed_size_mb = compressed_size / (1024 * 1024)

                if compressed_size < ocr_size:
                    # Ghostscript version is smaller, use it
                    reduction = ((ocr_size - compressed_size) / ocr_size) * 100
                    console.print(
                        f"   ‚úì Compressed: {compressed_size_mb:.1f} MB (was {ocr_size_mb:.1f} MB, -{reduction:.1f}%)"
                    )
                    import shutil
                    shutil.copy2(compressed_path, final_path)
                    console.print(f"   ‚úì Using compressed version")
                else:
                    # OCR version is smaller or same, use it
                    console.print(
                        f"   ‚úó Compressed larger: {compressed_size_mb:.1f} MB vs {ocr_size_mb:.1f} MB"
                    )
                    console.print(f"   ‚úì Using OCR version (already optimal)")
                    import shutil
                    shutil.copy2(ocr_path, final_path)
            else:
                console.print(f"   ‚úó Compression failed, using OCR version")
                import shutil
                shutil.copy2(ocr_path, final_path)

        except FileNotFoundError:
            console.print(
                "[yellow]  Ghostscript not installed - using OCR version[/yellow]"
            )
            console.print("  Install with: brew install ghostscript")
            import shutil
            shutil.copy2(ocr_path, final_path)

        # Show final file sizes comparison
        final_size_mb = final_path.stat().st_size / (1024 * 1024)
        console.print("\n[bold]Final file sizes:[/bold]")
        console.print(f"  [dim]sandbox/[/dim]{pdf_path.name}: {pdf_path.stat().st_size / (1024 * 1024):.1f} MB (raw)")
        console.print(f"  [dim]sandbox/[/dim]{ocr_path.name}: {ocr_path.stat().st_size / (1024 * 1024):.1f} MB")
        console.print(f"  [green]{final_path.name}: {final_size_mb:.1f} MB[/green] ‚Üê final")


def download_docsend(
    url,
    email=None,
    passcode="",
    slide_count=None,
    max_workers=8,
    ocr_jobs=None,
    basic_title=False,
    llm_title_pages=10,
    cookies_from_browser=None,
    output_dir=None,
    _folder_name=None,  # Internal: folder name for data room organization
):
    """Download DocSend presentation with enhanced features.

    Args:
        output_dir: Base output directory (default: current directory)
        _folder_name: Internal param for organizing data room downloads by folder
    """

    logger.info(f"Starting download for URL: {url}")

    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }
    session.headers.update(headers)

    # Load cookies from browser if specified
    browser_cookies_loaded = False
    if cookies_from_browser:
        cookies = load_cookies_from_browser(cookies_from_browser)
        if cookies:
            for name, value in cookies.items():
                session.cookies.set(name, value)
            browser_cookies_loaded = True

    # Check if this is a data room URL (/view/s/xxx) - needs special handling
    parsed_check = urlparse(url)
    path_check = parsed_check.path.strip("/").split("/")
    is_dataroom = "view" in path_check and "s" in path_check and "/d/" not in url

    if is_dataroom:
        if not cookies_from_browser:
            console.print("[red]Data room URLs require --cookies-from-browser[/red]")
            console.print("[yellow]First authenticate in your browser, then run:[/yellow]")
            console.print(f"[yellow]  docsend-dl.py '{url}' --cookies-from-browser brave[/yellow]")
            return None

        # Discover all documents in the data room
        documents = discover_dataroom_documents(url, cookies_from_browser)

        if not documents:
            console.print("[yellow]No documents found in data room[/yellow]")
            return None

        # Download each document
        console.print(f"\n[bold cyan]Downloading {len(documents)} document(s) from data room...[/bold cyan]")

        # Use provided output_dir or current directory
        base_output_dir = Path(output_dir) if output_dir else Path(".")
        base_output_dir.mkdir(parents=True, exist_ok=True)

        for i, (doc_name, doc_url, folder_name) in enumerate(documents, 1):
            console.print(f"\n{'='*60}")
            console.print(f"[bold]Document {i}/{len(documents)}: {doc_name}[/bold]")
            console.print(f"[dim]Folder: {folder_name}[/dim]")
            console.print(f"{'='*60}")

            # Recursively download this document
            download_docsend(
                doc_url,
                email=email,
                passcode=passcode,
                slide_count=slide_count,
                max_workers=max_workers,
                ocr_jobs=ocr_jobs,
                basic_title=basic_title,
                llm_title_pages=llm_title_pages,
                cookies_from_browser=cookies_from_browser,
                output_dir=str(base_output_dir),
                _folder_name=folder_name,
            )

        console.print(f"\n[bold green]‚úì Completed downloading {len(documents)} document(s) from data room[/bold green]")
        console.print(f"[bold]Output directory: {base_output_dir.absolute()}[/bold]")
        return None

    # First, follow any redirects to get the actual document URL
    logger.info("Following redirects to resolve actual document URL...")
    try:
        initial_response = session.get(url, allow_redirects=True)
        resolved_url = initial_response.url
        logger.info(f"Resolved URL: {resolved_url}")

        # If we got redirected, use the final URL
        if resolved_url != url:
            console.print(f"[yellow]Redirected to: {resolved_url}[/yellow]")
            url = resolved_url
    except Exception as e:
        logger.error(f"Error resolving URL: {e}")
        console.print(f"[red]Error resolving URL: {e}[/red]")
        return None

    # Extract document code from the resolved URL
    parsed_url = urlparse(url)
    # Clean path by removing query parameters and fragments
    clean_path = parsed_url.path.strip("/")
    path_parts = clean_path.split("/")

    # Handle various URL patterns:
    # - /view/xxx (simple)
    # - /view/s/xxx (space/shared)
    # - /view/xxx/d/yyy (folder with subdocument)
    full_doc_path = None  # Will be used for API calls on folder URLs

    if "view" in path_parts:
        view_index = path_parts.index("view")
        if view_index + 1 < len(path_parts):
            next_part = path_parts[view_index + 1]

            # Check for /view/s/xxx pattern (space/shared URL)
            if next_part == "s" and len(path_parts) > view_index + 2:
                # Space URL: /view/s/DOCUMENT
                document_code = path_parts[view_index + 2]
                # Store the full path for API calls
                full_doc_path = (
                    f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                )
                logger.debug(
                    f"Space URL detected. Document: {document_code}, Full path: {full_doc_path}"
                )
            # Check if this is a folder URL with /d/ subdocument
            elif len(path_parts) > view_index + 2 and path_parts[view_index + 2] == "d":
                # Folder URL: /view/FOLDER/d/DOCUMENT
                folder_code = path_parts[view_index + 1]
                document_code = (
                    path_parts[view_index + 3]
                    if len(path_parts) > view_index + 3
                    else folder_code
                )
                # Store the full path for API calls
                full_doc_path = (
                    f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                )
                logger.debug(
                    f"Folder URL detected. Using document: {document_code}, Full path: {full_doc_path}"
                )
            else:
                # Simple URL: /view/DOCUMENT
                document_code = path_parts[view_index + 1]
        else:
            document_code = path_parts[-1]
    else:
        # For other patterns, use the last part
        document_code = path_parts[-1]

    # Ensure document code doesn't contain query parameters
    if "?" in document_code:
        document_code = document_code.split("?")[0]
    if "#" in document_code:
        document_code = document_code.split("#")[0]

    logger.debug(f"Extracted document code: {document_code}")

    # Rebuild clean URL without query parameters
    clean_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
    url = clean_url  # Use clean URL for all subsequent requests

    console.print("\n[bold cyan]DocSend Downloader (Enhanced)[/bold cyan]")
    console.print(f"Document URL: {url}")
    console.print(f"Document Code: {document_code}")
    console.print(f"Parallel workers: {max_workers}")

    # Initial request and authentication
    console.print("\n[bold]Checking authentication requirements...[/bold]")
    logger.info("Checking if authentication is required...")
    response = session.get(url)
    logger.debug(f"Initial response status: {response.status_code}")

    authenticated_email = None

    # Check for bot detection
    if "appear to be a bot" in response.text.lower():
        console.print("[red]DocSend blocked request as bot[/red]")
        console.print("[yellow]This may be a newer DocSend page requiring browser automation[/yellow]")
        return None

    # Check for JavaScript SPA (newer DocSend) - no traditional form, but auth still needed
    # Skip this check if we already loaded browser cookies (assume authenticated)
    is_spa = "window.ENV" in response.text and "link_auth_form" not in response.text
    if is_spa and not browser_cookies_loaded:
        # SPA pages need special handling - try API directly to check if auth needed
        test_url = f"{full_doc_path}/page_data/1" if full_doc_path else f"https://docsend.com/view/{document_code}/page_data/1"
        test_resp = session.get(test_url, headers={"Accept": "application/json"})
        if test_resp.status_code == 404:
            console.print("[yellow]JavaScript SPA detected - requires browser authentication[/yellow]")

            if not passcode:
                console.print("\n[red]This document likely requires a passcode.[/red]")
                console.print("[yellow]Run again with: --passcode YOUR_PASSCODE[/yellow]")
                return None

            # Use Playwright for browser-based auth
            privacy_email = "platypus@gmail.com"
            auth_email = email if email else privacy_email

            cookies, final_url = authenticate_spa_with_playwright(url, auth_email, passcode)
            if cookies:
                # Transfer cookies to requests session
                for name, value in cookies.items():
                    session.cookies.set(name, value)
                authenticated_email = auth_email

                # If URL changed (e.g., space -> document), re-parse it
                if final_url and final_url != url:
                    url = final_url
                    parsed_url = urlparse(url)
                    clean_path = parsed_url.path.strip("/")
                    path_parts = clean_path.split("/")

                    # Re-extract document code from new URL
                    if "view" in path_parts:
                        view_index = path_parts.index("view")
                        if view_index + 1 < len(path_parts):
                            next_part = path_parts[view_index + 1]
                            if next_part == "s" and len(path_parts) > view_index + 2:
                                document_code = path_parts[view_index + 2]
                            elif len(path_parts) > view_index + 2 and path_parts[view_index + 2] == "d":
                                document_code = path_parts[view_index + 3] if len(path_parts) > view_index + 3 else path_parts[view_index + 1]
                                full_doc_path = f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                            else:
                                document_code = path_parts[view_index + 1]
                    console.print(f"   Updated document code: [cyan]{document_code}[/cyan]")

                # Build new test URL with updated path
                test_url = f"{full_doc_path}/page_data/1" if full_doc_path else f"https://docsend.com/view/{document_code}/page_data/1"

                # Verify auth worked
                test_resp = session.get(test_url, headers={"Accept": "application/json"})
                if test_resp.status_code == 200:
                    try:
                        data = test_resp.json()
                        if "imageUrl" in data:
                            console.print(f"\n[bold green]üìß Successfully authenticated with: {authenticated_email}[/bold green]")
                        else:
                            console.print("[red]Authentication failed - no image data[/red]")
                            return None
                    except Exception:
                        console.print("[red]Authentication failed - invalid response[/red]")
                        return None
                else:
                    console.print(f"[red]Authentication failed - API returned {test_resp.status_code}[/red]")
                    return None
            else:
                console.print("[red]Browser authentication failed[/red]")
                return None

    # If browser cookies loaded, verify they work before trying other auth
    if browser_cookies_loaded:
        test_api_url = f"{full_doc_path}/page_data/1" if full_doc_path else f"https://docsend.com/view/{document_code}/page_data/1"
        test_resp = session.get(test_api_url, headers={"Accept": "application/json"})
        if test_resp.status_code == 200:
            try:
                data = test_resp.json()
                if "imageUrl" in data:
                    console.print("[green]‚úì Authenticated via browser cookies[/green]")
                    authenticated_email = "(browser cookies)"
                    # Skip auth flow
                else:
                    console.print("[yellow]Cookie auth response incomplete, trying Playwright...[/yellow]")
                    browser_cookies_loaded = False
            except Exception:
                console.print("[yellow]Cookie auth failed, trying Playwright...[/yellow]")
                browser_cookies_loaded = False
        else:
            console.print(f"[yellow]Cookie API check failed (status: {test_resp.status_code}), trying Playwright...[/yellow]")
            # The requests library may be blocked by CloudFront WAF
            # Try Playwright with the browser cookies + passcode if provided
            if PLAYWRIGHT_AVAILABLE and passcode:
                console.print("[cyan]Using Playwright to complete authentication...[/cyan]")
                pw_cookies, final_url = authenticate_with_playwright_and_cookies(
                    url, cookies_from_browser, passcode, email
                )
                if pw_cookies is None and final_url is None:
                    # Email verification required - message already shown, exit
                    return None
                if pw_cookies:
                    # Transfer Playwright cookies to requests session
                    for name, value in pw_cookies.items():
                        session.cookies.set(name, value)
                    authenticated_email = email if email else "(browser session)"

                    # Update URL if redirected
                    if final_url and final_url != url:
                        url = final_url
                        # Re-parse URL components
                        parsed_url = urlparse(url)
                        clean_path = parsed_url.path.strip("/")
                        path_parts = clean_path.split("/")
                        if "view" in path_parts:
                            view_index = path_parts.index("view")
                            if view_index + 1 < len(path_parts):
                                next_part = path_parts[view_index + 1]
                                if next_part == "s" and len(path_parts) > view_index + 2:
                                    document_code = path_parts[view_index + 2]
                                    full_doc_path = f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                                elif len(path_parts) > view_index + 2 and path_parts[view_index + 2] == "d":
                                    document_code = path_parts[view_index + 3] if len(path_parts) > view_index + 3 else path_parts[view_index + 1]
                                    full_doc_path = f"{parsed_url.scheme}://{parsed_url.netloc}/{'/'.join(path_parts)}"
                                else:
                                    document_code = path_parts[view_index + 1]

                    # Verify auth worked
                    test_api_url = f"{full_doc_path}/page_data/1" if full_doc_path else f"https://docsend.com/view/{document_code}/page_data/1"
                    test_resp = session.get(test_api_url, headers={"Accept": "application/json"})
                    if test_resp.status_code == 200:
                        try:
                            data = test_resp.json()
                            if "imageUrl" in data:
                                console.print("[green]‚úì Authenticated via Playwright[/green]")
                                browser_cookies_loaded = True  # Mark as authenticated
                            else:
                                browser_cookies_loaded = False
                        except Exception:
                            browser_cookies_loaded = False
                    else:
                        browser_cookies_loaded = False
                else:
                    browser_cookies_loaded = False
            else:
                browser_cookies_loaded = False

    if "link_auth_form" in response.text and not browser_cookies_loaded:
        console.print("[yellow]Authentication required[/yellow]")

        # Detect what fields are required
        auth_reqs = detect_auth_requirements(response.text)
        logger.debug(f"Auth requirements detected: {auth_reqs}")

        if auth_reqs["email_required"]:
            console.print("  ‚Ä¢ Email: [cyan]required[/cyan]")
        if auth_reqs["passcode_required"]:
            console.print("  ‚Ä¢ Passcode: [cyan]required[/cyan]")

        # Check if passcode is required but not provided
        if auth_reqs["passcode_required"] and not passcode:
            console.print("\n[red]This document requires a passcode.[/red]")
            console.print("[yellow]Run again with: --passcode YOUR_PASSCODE[/yellow]")
            return None

        # Try privacy email first
        privacy_email = "platypus@gmail.com"
        console.print("\n[bold]Attempting privacy-preserving authentication...[/bold]")

        if authenticate_with_email(session, url, privacy_email, passcode, headers):
            authenticated_email = privacy_email
        else:
            # Privacy email failed, try user email if provided
            if email:
                console.print(
                    "\n[bold]Privacy email failed, trying user-provided email...[/bold]"
                )

                if authenticate_with_email(session, url, email, passcode, headers):
                    authenticated_email = email
                else:
                    console.print("\n[red]Authentication failed with both emails[/red]")
                    console.print("[red]Unable to access document[/red]")
                    return None
            else:
                console.print(
                    "\n[red]Privacy email failed and no user email provided[/red]"
                )
                console.print(
                    "[yellow]Try running again with --email YOUR_EMAIL[/yellow]"
                )
                return None
    else:
        console.print(
            "[green]‚úì No authentication required - document is public[/green]"
        )

    if authenticated_email:
        console.print(
            f"\n[bold green]üìß Successfully authenticated with: {authenticated_email}[/bold green]"
        )

    # Auto-detect slide count
    if not slide_count:
        detected = detect_slide_count_auto(session, document_code, full_doc_path)

        if not detected:
            console.print("[red]Failed to detect slide count[/red]")
            return None

        slide_count = detected

    # Create output directory structure
    # Final PDF goes to: output_dir/[folder_name]/ or output_dir/ for root
    # Sandbox files go to: output_dir/0__sandbox/{document_code}/
    base_output_dir = Path(output_dir) if output_dir else Path(".")
    base_output_dir.mkdir(parents=True, exist_ok=True)

    # Determine where final PDF will go
    if _folder_name and _folder_name != "Home":
        safe_folder = "".join(c if c.isalnum() or c in " -_&" else "_" for c in _folder_name)
        pdf_output_dir = base_output_dir / safe_folder
    else:
        pdf_output_dir = base_output_dir
    pdf_output_dir.mkdir(parents=True, exist_ok=True)

    # Sandbox for intermediate files (slides, raw PDF, etc.)
    sandbox_base = base_output_dir / "0__sandbox" / document_code
    sandbox_base.mkdir(parents=True, exist_ok=True)

    # Download all slides in parallel (to sandbox)
    successful = download_all_slides_parallel(
        session, document_code, slide_count, sandbox_base, max_workers, full_doc_path
    )

    # Extract document title and date (from sandbox)
    document_title = None
    document_date = None
    if basic_title:
        # Use simple OCR-based extraction from first slide
        first_slide = sandbox_base / "slide_001.png"
        if first_slide.exists():
            document_title = extract_title_from_image(first_slide)
    else:
        # Use Claude to suggest title and extract date from slide images (default)
        document_title, document_date = suggest_title_with_claude(
            sandbox_base, slide_count, llm_title_pages
        )
        if not document_title:
            # Fallback to basic extraction if Claude fails
            console.print("[yellow]Falling back to basic title extraction...[/yellow]")
            first_slide = sandbox_base / "slide_001.png"
            if first_slide.exists():
                document_title = extract_title_from_image(first_slide)

    # Create PDFs with verbose output (sandbox for intermediate, pdf_output_dir for final)
    if successful > 0:
        create_pdfs_verbose(sandbox_base, document_code, document_title, document_date, ocr_jobs, pdf_output_dir)

    # Final summary
    console.print("\n[bold green]‚úì Complete![/bold green]")

    # Show what was created - check pdf_output_dir for final PDFs
    table = Table(title="Files Created")
    table.add_column("Location", style="dim")
    table.add_column("File", style="cyan")
    table.add_column("Size", style="green")

    # Show final PDFs from pdf_output_dir
    for file in sorted(pdf_output_dir.iterdir()):
        if file.is_file() and file.suffix == ".pdf":
            size_kb = file.stat().st_size / 1024
            if size_kb > 1024:
                size_str = f"{size_kb / 1024:.1f} MB"
            else:
                size_str = f"{size_kb:.1f} KB"
            # Show relative path from base_output_dir
            rel_path = file.parent.relative_to(base_output_dir) if file.parent != base_output_dir else Path(".")
            table.add_row(str(rel_path), file.name, size_str)

    console.print(table)

    # Show sandbox location
    console.print(f"\n[dim]Intermediate files: {sandbox_base.relative_to(base_output_dir)}/[/dim]")

    if authenticated_email:
        console.print(f"[dim]Downloaded using: {authenticated_email}[/dim]")

    if document_title:
        console.print(f"[dim]Document title: {document_title}[/dim]")

    return base_output_dir


def main():
    parser = argparse.ArgumentParser(
        description="DocSend downloader with enhanced OCR progress and title extraction"
    )

    parser.add_argument("url", help="DocSend URL")
    parser.add_argument("--email", help="Fallback email if privacy email fails")
    parser.add_argument("--passcode", default="", help="Passcode if required")
    parser.add_argument("--slides", type=int, help="Override automatic detection")
    parser.add_argument(
        "--workers",
        type=int,
        default=8,
        help="Number of parallel download workers (default: 8)",
    )
    parser.add_argument(
        "--ocr-jobs",
        type=int,
        help="Number of CPU cores for OCR processing (default: all available)",
    )
    parser.add_argument(
        "--basic-title",
        action="store_true",
        help="Use simple OCR-based title extraction instead of Claude",
    )
    parser.add_argument(
        "--llm-title-pages",
        type=int,
        default=10,
        help="Number of pages to send to Claude for title suggestion (default: 10)",
    )
    parser.add_argument(
        "--cookies-from-browser",
        choices=["brave", "chrome", "firefox", "safari", "edge", "chromium"],
        help="Load cookies from browser (for authenticated sessions)",
    )
    parser.add_argument(
        "--log",
        choices=["debug", "info", "warning", "error"],
        default="warning",
        help="Set logging level (default: warning)",
    )
    parser.add_argument(
        "--output-dir", "-o",
        default=".",
        help="Output directory (default: current dir). Final PDFs go to folder structure, intermediate files to 0__sandbox/",
    )

    args = parser.parse_args()

    # Set up logging
    setup_logging(args.log)

    download_docsend(
        args.url,
        args.email,
        args.passcode,
        args.slides,
        args.workers,
        args.ocr_jobs,
        args.basic_title,
        args.llm_title_pages,
        args.cookies_from_browser,
        args.output_dir,
    )


if __name__ == "__main__":
    main()
