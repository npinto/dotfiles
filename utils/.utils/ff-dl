#!/usr/bin/env python3
"""
Fireflies Web Auth CLI v1.12.5 - Headless Browser Automation

ALL workflows run headless without closing your browser!

UPLOAD workflow:
- Uses headless Playwright browser with cookie+localStorage injection
- Uploads via file chooser UI, Fireflies JS handles API calls
- Polls for completion via PUBLIC API
- Downloads transcript (MD+PDF) + summary (MD+PDF) when ready
- Deletes transcript + upload entry from Fireflies after download (use --keep to retain)

VIDEO support:
- MP4, MKV, AVI, MOV, etc. auto-converted to MP3 via ffmpeg
- Temp files cleaned up automatically after processing

URL support (via yt-dlp):
- YouTube, Vimeo, Twitter/X, TikTok, Twitch, SoundCloud, and 1000+ sites
- Full list: https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md
- Default: keeps video file + transcripts (use --audio-only for no video)

DOWNLOAD/DELETE/LIST/CLEANUP workflows:
- Uses cookies extracted from browser + public API
- No browser automation needed

Usage:
    ./fireflies_web_auth_cli_chrome.py audio.m4a              # Upload + download + delete
    ./fireflies_web_auth_cli_chrome.py video.mp4              # Convert + upload
    ./fireflies_web_auth_cli_chrome.py audio.m4a --keep       # Upload + download (keep on Fireflies)
    ./fireflies_web_auth_cli_chrome.py "https://youtube.com/..."   # YouTube (keeps video + transcripts)
    ./fireflies_web_auth_cli_chrome.py "https://youtube.com/..." --audio-only  # No video file
    ./fireflies_web_auth_cli_chrome.py "https://vimeo.com/..."     # Vimeo video
    ./fireflies_web_auth_cli_chrome.py "https://twitter.com/..."   # Twitter/X video
    ./fireflies_web_auth_cli_chrome.py download ID            # Download transcript
    ./fireflies_web_auth_cli_chrome.py delete ID              # Delete transcript
    ./fireflies_web_auth_cli_chrome.py --list                 # List transcripts
    ./fireflies_web_auth_cli_chrome.py --cleanup              # Delete all CLI transcripts
    ./fireflies_web_auth_cli_chrome.py --cleanup --dry-run    # Preview cleanup

Requirements:
    pip install playwright requests browser_cookie3 yt-dlp
    playwright install chromium
    brew install ffmpeg  # For video support
    export FIREFLIES_API_KEY=your_key
"""

import argparse
import atexit
import functools
import hashlib
import json
import logging
import os
import random
import re
import shutil
import signal
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Callable, TypeVar
from urllib.parse import unquote

import requests

try:
    import yaml

    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

try:
    from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
except ImportError:
    print("pip install playwright && playwright install chromium")
    sys.exit(1)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# Global verbose flag (set via --verbose / -v)
VERBOSE = False


def log_verbose(msg: str) -> None:
    """Log message only in verbose mode."""
    if VERBOSE:
        logger.info(msg)


class JsonFormatter(logging.Formatter):
    """JSON lines formatter for structured logging."""

    def format(self, record: logging.LogRecord) -> str:
        log_entry = {
            "ts": datetime.now().isoformat(),
            "level": record.levelname,
            "msg": record.getMessage(),
        }
        # Add extra fields if present
        if hasattr(record, "file"):
            log_entry["file"] = record.file
        if hasattr(record, "phase"):
            log_entry["phase"] = record.phase
        return json.dumps(log_entry)


def setup_json_logging() -> None:
    """Switch to JSON logging format."""
    root_logger = logging.getLogger()
    for handler in root_logger.handlers:
        handler.setFormatter(JsonFormatter())


# =============================================================================
# Graceful Shutdown
# =============================================================================

# Global cleanup callbacks (called on SIGINT/SIGTERM)
_cleanup_callbacks: list[Callable[[], None]] = []
_shutdown_in_progress = False


def register_cleanup(callback: Callable[[], None]) -> None:
    """Register a cleanup function to be called on shutdown."""
    _cleanup_callbacks.append(callback)


def unregister_cleanup(callback: Callable[[], None]) -> None:
    """Unregister a cleanup function."""
    if callback in _cleanup_callbacks:
        _cleanup_callbacks.remove(callback)


def _shutdown_handler(signum: int, frame) -> None:
    """Handle SIGINT/SIGTERM for graceful shutdown."""
    global _shutdown_in_progress
    if _shutdown_in_progress:
        # Force exit on second signal
        logger.warning("\n‚ö†Ô∏è  Force quit...")
        sys.exit(1)

    _shutdown_in_progress = True
    sig_name = "SIGINT" if signum == signal.SIGINT else "SIGTERM"
    logger.info(f"\nüõë Received {sig_name}, cleaning up...")

    # Call all registered cleanup callbacks
    for callback in reversed(_cleanup_callbacks):
        try:
            callback()
        except Exception as e:
            logger.debug(f"Cleanup error: {e}")

    logger.info("Goodbye!")
    sys.exit(0)


def setup_signal_handlers() -> None:
    """Install signal handlers for graceful shutdown."""
    signal.signal(signal.SIGINT, _shutdown_handler)
    signal.signal(signal.SIGTERM, _shutdown_handler)


# =============================================================================
# Idempotent Operations Cache
# =============================================================================

CACHE_FILE = Path.home() / ".fireflies_cli_cache.json"


def compute_file_hash(file_path: Path) -> str:
    """Compute a hash of file for idempotent operations.

    Uses first 1MB + file size + mtime for fast hashing.
    """
    stat = file_path.stat()
    hasher = hashlib.sha256()

    # Include file metadata
    hasher.update(f"{stat.st_size}:{stat.st_mtime}".encode())

    # Hash first 1MB of content
    with open(file_path, "rb") as f:
        chunk = f.read(1024 * 1024)  # 1MB
        hasher.update(chunk)

    return hasher.hexdigest()[:16]  # Short hash is enough


def load_cache() -> dict:
    """Load the transcript cache from disk."""
    if not CACHE_FILE.exists():
        return {}
    try:
        return json.loads(CACHE_FILE.read_text())
    except Exception:
        return {}


def save_cache(cache: dict) -> None:
    """Save the transcript cache to disk with secure permissions."""
    try:
        CACHE_FILE.write_text(json.dumps(cache, indent=2))
        # Set secure permissions (owner read/write only)
        CACHE_FILE.chmod(0o600)
    except Exception as e:
        logger.debug(f"Failed to save cache: {e}")


def check_cache(file_path: Path) -> str | None:
    """Check if file was already transcribed.

    Returns transcript_id if found in cache, None otherwise.
    """
    cache = load_cache()
    file_hash = compute_file_hash(file_path)
    entry = cache.get(file_hash)
    if entry and isinstance(entry, dict):
        return entry.get("transcript_id")
    return None


def update_cache(file_path: Path, transcript_id: str) -> None:
    """Add a file ‚Üí transcript_id mapping to the cache."""
    cache = load_cache()
    file_hash = compute_file_hash(file_path)
    cache[file_hash] = {
        "transcript_id": transcript_id,
        "filename": file_path.name,
        "cached_at": datetime.now().isoformat(),
    }
    save_cache(cache)


# Prefix for CLI uploads - helps identify them in Fireflies UI for cleanup
# IMPORTANT: This prefix is ONLY used in Fireflies (for --cleanup to find/delete CLI uploads)
# Local output files should NOT have this prefix - they use the original filename
CLI_UPLOAD_PREFIX = "FIREFLIES_CLI__"


def strip_cli_prefix_from_md(content: bytes) -> bytes:
    """Strip FIREFLIES_CLI__ prefix from markdown content.

    The prefix is added to uploads for identification in Fireflies,
    but should not appear in downloaded transcript/summary files.
    """
    text = content.decode("utf-8", errors="replace")
    # Remove prefix from markdown headers (e.g., "# FIREFLIES_CLI__Title.mp3")
    text = text.replace(f"# {CLI_UPLOAD_PREFIX}", "# ")
    text = text.replace(f"## {CLI_UPLOAD_PREFIX}", "## ")
    # Also handle cases where it appears in the title without header
    text = text.replace(CLI_UPLOAD_PREFIX, "")
    return text.encode("utf-8")

# Default config file location
DEFAULT_CONFIG_PATH = Path.home() / ".fireflies_cli.yaml"

# Default config values
DEFAULT_CONFIG = {
    "browser": "brave",
    "output_dir": ".",
    "headless": True,
    "keep": False,
    "transcript_only": False,
    "md_only": False,
    "pdf_only": False,
    "quiet": False,
    # api_key: loaded from FIREFLIES_API_KEY env var by default
}


def atomic_write_bytes(path: Path, content: bytes) -> None:
    """Write bytes to file atomically using a temp file + rename.

    This prevents partial files on crash - either the full file exists
    or nothing is written.

    Args:
        path: Final destination path
        content: Bytes to write
    """
    tmp_path = path.with_suffix(path.suffix + ".tmp")
    try:
        tmp_path.write_bytes(content)
        tmp_path.rename(path)
    except Exception:
        # Clean up temp file on error
        if tmp_path.exists():
            tmp_path.unlink()
        raise


# =============================================================================
# Retry Helper with Exponential Backoff
# =============================================================================

T = TypeVar("T")


def retry_with_backoff(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0,
    jitter: bool = True,
    retryable_exceptions: tuple = (Exception,),
) -> Callable[[Callable[..., T]], Callable[..., T]]:
    """Decorator for retrying functions with exponential backoff.

    Args:
        max_retries: Maximum number of retry attempts
        base_delay: Initial delay in seconds
        max_delay: Maximum delay cap in seconds
        exponential_base: Base for exponential calculation (default 2 = 1s, 2s, 4s, 8s...)
        jitter: Add random jitter to prevent thundering herd
        retryable_exceptions: Tuple of exceptions to retry on

    Example:
        @retry_with_backoff(max_retries=3, base_delay=1)
        def flaky_api_call():
            ...
    """

    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> T:
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except retryable_exceptions as e:
                    last_exception = e
                    if attempt == max_retries:
                        logger.warning(f"  ‚ö†Ô∏è  All {max_retries + 1} attempts failed")
                        raise

                    # Calculate delay with exponential backoff
                    delay = min(base_delay * (exponential_base**attempt), max_delay)
                    if jitter:
                        delay = delay * (0.5 + random.random())  # 50-150% of delay

                    log_verbose(
                        f"  ‚ö†Ô∏è  Attempt {attempt + 1}/{max_retries + 1} failed: {e}"
                    )
                    log_verbose(f"  ‚è≥ Retrying in {delay:.1f}s...")
                    time.sleep(delay)

            # Should never reach here, but just in case
            raise last_exception  # type: ignore

        return wrapper

    return decorator


# =============================================================================
# Timing Metrics
# =============================================================================


class Timer:
    """Simple timer for tracking operation durations."""

    def __init__(self):
        self.start_time: float | None = None
        self.phases: dict[str, float] = {}
        self._phase_start: float | None = None
        self._current_phase: str | None = None

    def start(self) -> None:
        """Start the overall timer."""
        self.start_time = time.time()

    def start_phase(self, name: str) -> None:
        """Start timing a named phase."""
        self._current_phase = name
        self._phase_start = time.time()

    def end_phase(self) -> float:
        """End the current phase and return its duration."""
        if self._phase_start is None or self._current_phase is None:
            return 0.0
        duration = time.time() - self._phase_start
        self.phases[self._current_phase] = duration
        self._phase_start = None
        self._current_phase = None
        return duration

    def total(self) -> float:
        """Get total elapsed time."""
        if self.start_time is None:
            return 0.0
        return time.time() - self.start_time

    def format_duration(self, seconds: float) -> str:
        """Format duration as human-readable string."""
        if seconds < 60:
            return f"{seconds:.1f}s"
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}m {secs:.0f}s"

    def summary(self) -> str:
        """Get a summary of all phases and total time."""
        parts = []
        for name, duration in self.phases.items():
            parts.append(f"{name}: {self.format_duration(duration)}")
        total = self.total()
        return f"{', '.join(parts)} | Total: {self.format_duration(total)}"


def load_config(config_path: Path = None) -> dict:
    """Load configuration from YAML file.

    Priority: CLI args > config file > defaults
    """
    config = DEFAULT_CONFIG.copy()

    # Determine config path
    path = config_path or DEFAULT_CONFIG_PATH

    if not path.exists():
        return config

    if not YAML_AVAILABLE:
        logger.warning(f"Config file found at {path} but PyYAML not installed")
        logger.warning("Install with: pip install pyyaml")
        return config

    try:
        with open(path) as f:
            file_config = yaml.safe_load(f) or {}

        # Merge file config into defaults (file overrides defaults)
        for key, value in file_config.items():
            if key in config or key == "api_key":
                config[key] = value
            else:
                logger.warning(f"Unknown config key: {key}")

        logger.debug(f"Loaded config from {path}")

    except Exception as e:
        logger.warning(f"Could not load config from {path}: {e}")

    return config


def create_example_config(config_path: Path = None) -> bool:
    """Create example config file."""
    path = config_path or DEFAULT_CONFIG_PATH

    if path.exists():
        logger.error(f"Config file already exists: {path}")
        return False

    example_config = """# Fireflies CLI Configuration
# Location: ~/.fireflies_cli.yaml

# Browser to use for authentication (brave, chrome, chromium)
browser: brave

# Default output directory for downloaded files
output_dir: .

# Run browser in headless mode (true/false)
headless: true

# Keep transcripts on Fireflies after download (true/false)
keep: false

# Download options
transcript_only: false  # Skip summary download
md_only: false          # Skip PDF download
pdf_only: false         # Skip MD download

# Quiet mode - minimal output
quiet: false

# API key (can also use FIREFLIES_API_KEY env var)
# api_key: your_api_key_here
"""

    try:
        path.write_text(example_config)
        logger.info(f"Created example config at {path}")
        return True
    except Exception as e:
        logger.error(f"Could not create config: {e}")
        return False


# =============================================================================
# Browser Profile Paths
# =============================================================================

BROWSER_PROFILES = {
    "brave": Path.home() / "Library/Application Support/BraveSoftware/Brave-Browser",
    "chrome": Path.home() / "Library/Application Support/Google/Chrome",
    "chromium": Path.home() / "Library/Application Support/Chromium",
}

# Executable paths for macOS
BROWSER_EXECUTABLES = {
    "brave": "/Applications/Brave Browser.app/Contents/MacOS/Brave Browser",
    "chrome": "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
    "chromium": "/Applications/Chromium.app/Contents/MacOS/Chromium",
}


# =============================================================================
# Fireflies Browser Controller
# =============================================================================


class FirefliesBrowser:
    """Control Fireflies via browser - persistent context or cookie injection."""

    UPLOAD_URL = "https://app.fireflies.ai/upload"

    def __init__(self, browser: str = "brave", headless: bool = False, public_api=None):
        self.browser_name = browser
        self.headless = headless
        self.public_api = public_api  # FirefliesPublicAPI instance for API calls
        self.playwright = None
        self.context = None
        self.page = None
        self._profile_path = None
        self._use_cookie_injection = False

    def _get_profile_path(self) -> Path:
        """Get browser profile path."""
        base_path = BROWSER_PROFILES.get(self.browser_name)
        if not base_path or not base_path.exists():
            raise RuntimeError(f"Browser profile not found: {base_path}")

        default_profile = base_path / "Default"
        if not default_profile.exists():
            raise RuntimeError(f"Default profile not found: {default_profile}")

        return base_path

    def _extract_cookies(self) -> list[dict]:
        """Extract cookies from browser for Playwright injection."""
        try:
            import browser_cookie3
        except ImportError:
            raise RuntimeError("pip install browser_cookie3")

        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "chromium": browser_cookie3.chromium,
        }

        if self.browser_name not in browser_funcs:
            raise ValueError(f"Unsupported: {self.browser_name}")

        cookies = []
        cj = browser_funcs[self.browser_name](domain_name=".fireflies.ai")

        for cookie in cj:
            pw_cookie = {
                "name": cookie.name,
                "value": cookie.value,
                "domain": cookie.domain,
                "path": cookie.path or "/",
            }
            if cookie.expires:
                pw_cookie["expires"] = cookie.expires
            if cookie.secure:
                pw_cookie["secure"] = True

            cookies.append(pw_cookie)

        return cookies

    def _extract_local_storage(self) -> dict:
        """Extract localStorage from real browser via a quick page load."""

        # localStorage is in LevelDB (hard to read directly from disk)
        # Simpler approach: extract key auth values from cookies that we know
        # The authorization cookie contains the JWT token
        cookies = self._extract_cookies()
        local_storage = {}

        for cookie in cookies:
            if cookie["name"] == "authorization":
                token = cookie["value"]
                if token.startswith("Bearer%20"):
                    token = token[9:]
                elif token.startswith("Bearer "):
                    token = token[7:]
                local_storage["AUTHORIZATION"] = token
                local_storage["REFRESH_TOKEN"] = token

        return local_storage

    def start(self):
        """Launch browser - try persistent context first, fall back to cookie injection."""
        logger.info(f"üåê Launching {self.browser_name} browser...")

        self.playwright = sync_playwright().start()

        # Try persistent context first (best option when browser is closed)
        try:
            profile_path = self._get_profile_path()
            executable = BROWSER_EXECUTABLES.get(self.browser_name)

            if not Path(executable).exists():
                logger.debug(f"  Executable not found: {executable}")
                executable = None

            logger.debug("  Trying persistent context...")
            self.context = self.playwright.chromium.launch_persistent_context(
                user_data_dir=str(profile_path),
                headless=self.headless,
                executable_path=executable,
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--no-first-run",
                    "--no-default-browser-check",
                ],
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            )

            if self.context.pages:
                self.page = self.context.pages[0]
            else:
                self.page = self.context.new_page()

            logger.info("  Persistent context ready")
            return

        except Exception as e:
            err_msg = str(e).lower()
            if "lock" in err_msg or "closed" in err_msg or "cannot" in err_msg:
                logger.info("  Browser running, switching to cookie injection...")
                self._use_cookie_injection = True
            else:
                raise

        # Fallback: cookie injection with localStorage init script
        if self._use_cookie_injection:
            logger.info(
                "  Using headless browser with cookie+localStorage injection..."
            )

            browser = self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--disable-dev-shm-usage",
                    "--no-sandbox",
                ],
            )
            self.context = browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                locale="en-US",
            )

            # Get cookies and auth token
            cookies = self._extract_cookies()
            auth_token = None
            for cookie in cookies:
                if cookie["name"] == "authorization":
                    token = unquote(cookie["value"])
                    if token.startswith("Bearer "):
                        token = token[7:]
                    auth_token = token
                    break

            if not cookies or not auth_token:
                raise RuntimeError(
                    "No Fireflies cookies found. Login in browser first."
                )

            self.context.add_cookies(cookies)
            logger.info(f"  Injected {len(cookies)} cookies")

            self.page = self.context.new_page()

            # CRITICAL: Inject localStorage BEFORE page loads using init script
            self.page.add_init_script(f"""
                localStorage.setItem('AUTHORIZATION', '{auth_token}');
                localStorage.setItem('REFRESH_TOKEN', '{auth_token}');
                localStorage.setItem('TOKEN_EXPIRY', String(Date.now() + 86400000));
            """)

            logger.info("  Headless browser ready")

    def stop(self):
        """Close browser."""
        try:
            if self.context:
                self.context.close()
        except Exception:
            pass
        try:
            if self.playwright:
                self.playwright.stop()
        except Exception:
            pass

    def navigate_to_upload(self):
        """Navigate to upload page and verify login."""
        logger.info("  Navigating to upload page...")

        try:
            self.page.goto(
                self.UPLOAD_URL, wait_until="domcontentloaded", timeout=30000
            )
            # Wait for page to fully load
            self.page.wait_for_timeout(3000)
        except PlaywrightTimeout:
            logger.warning("  Page load timeout, continuing...")

        # Check if redirected to login
        current_url = self.page.url
        if "login" in current_url.lower() or "signin" in current_url.lower():
            raise RuntimeError(
                f"Not logged in. Please login to Fireflies in {self.browser_name} first."
            )

        # Wait for the upload UI to be ready
        try:
            # Try to wait for any of these common selectors
            self.page.wait_for_selector(
                'input[type="file"], div[class*="dropzone"], div[class*="upload"], button:has-text("Upload")',
                timeout=10000,
            )
            logger.debug("  Upload UI ready")
        except PlaywrightTimeout:
            logger.debug("  Upload UI not found via selector, proceeding anyway")

        logger.info("  ‚úÖ Logged in and ready")

    def _graphql(self, operation_name: str, query: str, variables: dict = None) -> dict:
        """Execute GraphQL via browser fetch."""
        variables = variables or {}

        # Escape backticks in query for JS template literal
        escaped_query = query.replace("`", "\\`")

        result = self.page.evaluate(f"""
            async () => {{
                try {{
                    const resp = await fetch("https://app.fireflies.ai/api/v4/graphql", {{
                        method: "POST",
                        headers: {{
                            "content-type": "application/json",
                            "apollographql-client-name": "app.fireflies.ai",
                            "apollographql-client-version": "20527828070",
                        }},
                        credentials: "include",
                        body: JSON.stringify({{
                            operationName: "{operation_name}",
                            variables: {json.dumps(variables)},
                            query: `{escaped_query}`
                        }})
                    }});
                    return await resp.json();
                }} catch (e) {{
                    return {{ error: e.toString() }};
                }}
            }}
        """)

        return result

    def _get_uploads_via_page(self) -> list[dict]:
        """Get uploads by reading from page's Apollo cache or re-triggering page load."""
        # Try to read from Apollo client cache (if available)
        result = self.page.evaluate("""
            () => {
                // Try to access Apollo cache
                if (window.__APOLLO_CLIENT__) {
                    try {
                        const cache = window.__APOLLO_CLIENT__.cache.extract();
                        const uploads = [];
                        for (const key in cache) {
                            if (key.startsWith('UploadedFile:')) {
                                uploads.push(cache[key]);
                            }
                        }
                        if (uploads.length > 0) return { source: 'apollo', uploads };
                    } catch (e) {}
                }

                // Try to find file list in DOM
                const rows = document.querySelectorAll('tr[data-row-key], [class*="file-row"], [class*="upload-item"]');
                if (rows.length > 0) {
                    const uploads = [];
                    rows.forEach(row => {
                        const name = row.querySelector('[class*="name"], td:first-child')?.textContent;
                        const id = row.getAttribute('data-row-key') || row.getAttribute('data-id');
                        if (name || id) {
                            uploads.push({ name, _id: id });
                        }
                    });
                    return { source: 'dom', uploads };
                }

                return { source: 'none', uploads: [] };
            }
        """)
        return result.get("uploads", [])

    def list_uploads(self, wait_for_data: bool = True) -> list[dict]:
        """List uploaded files via v4 GraphQL API.

        Args:
            wait_for_data: If True, wait for uploads page to load first
        """
        if wait_for_data:
            # Navigate to uploads page and wait for it to load
            try:
                logger.debug("  Navigating to uploads page for data...")
                self.page.goto(self.UPLOAD_URL, wait_until="domcontentloaded")

                # Wait for "Checking..." to disappear (page is loading file list)
                for _ in range(30):  # Max 30 seconds
                    body_text = self.page.evaluate("() => document.body.innerText")
                    if "Checking..." not in body_text:
                        break
                    self.page.wait_for_timeout(1000)
                else:
                    logger.warning("  Uploads page still loading after 30s")

                # Extra wait for data to populate
                self.page.wait_for_timeout(2000)
            except Exception as e:
                logger.debug(f"  Navigation error: {e}")

        # Try v4 API via browser fetch (uses browser session credentials)
        query = """
        query fetchUploadedFiles {
            fetchUploadedFiles {
                _id
                name
                meetingId
                transcriptId
                processing
                createdAt
            }
        }
        """

        result = self._graphql("fetchUploadedFiles", query)

        if "errors" in result:
            error_msg = result["errors"][0].get("message", "Unknown")
            logger.debug(f"  v4 API failed: {error_msg}")

            # Fallback to Apollo cache if API fails
            uploads = self._get_uploads_via_page()
            if uploads:
                logger.debug(f"  Got {len(uploads)} files from Apollo cache")
                return uploads
            return []

        uploads = result.get("data", {}).get("fetchUploadedFiles", [])
        logger.debug(f"  Got {len(uploads)} files from v4 API")
        return uploads

    def delete_upload_via_api(self, file_id: str) -> bool:
        """Delete an upload using v4 GraphQL API (no UI automation).

        Args:
            file_id: The _id from fetchUploadedFiles (MongoDB ObjectId format)

        Returns:
            True if deletion succeeded
        """
        mutation = """
        mutation deleteUserFile($fileId: String!) {
            deleteUserFile(fileId: $fileId)
        }
        """

        result = self._graphql("deleteUserFile", mutation, {"fileId": file_id})

        if "errors" in result:
            error_msg = result["errors"][0].get("message", "Unknown error")
            logger.debug(f"  deleteUserFile failed: {error_msg}")
            return False

        # Check if deletion returned true
        deleted = result.get("data", {}).get("deleteUserFile", False)
        return bool(deleted)

    def upload_file(
        self,
        file_path: Path,
        prefix: str = CLI_UPLOAD_PREFIX,
        title: str | None = None,
    ) -> str:
        """Upload file using UI file chooser. Returns file name for polling.

        Args:
            file_path: Path to audio file
            prefix: Prefix to add to filename (helps identify CLI uploads in Fireflies UI)
            title: Override title for Fireflies (use original video name when uploading converted audio)
        """
        file_size_mb = file_path.stat().st_size / 1024 / 1024
        # Use the actual audio extension to keep Fireflies format detection reliable
        audio_ext = file_path.suffix
        if title:
            # Use the provided title as the base name (strip any CLI prefix)
            title_stem = Path(title).stem
            while title_stem.startswith(CLI_UPLOAD_PREFIX):
                title_stem = title_stem[len(CLI_UPLOAD_PREFIX) :]
            safe_title = sanitize_filename(title_stem) or Path(file_path).stem
            audio_filename = f"{safe_title}{audio_ext}"
        else:
            # Use the actual audio file name (with extension)
            audio_filename = file_path.name  # e.g., giulioz_123456.m4a

        prefixed_name = f"{prefix}{audio_filename}"
        file_name = prefixed_name  # Use prefixed name for Fireflies title

        # Always use temp file with prefixed name (avoids Playwright buffer issues)
        # Use tempfile for secure temp file creation, then rename to desired name
        import tempfile as tmp_module

        temp_dir = Path(tmp_module.gettempdir())
        temp_file = temp_dir / f"{tmp_module.gettempprefix()}{prefixed_name}"
        if temp_file.exists():
            temp_file.unlink()
        # Hardlink is preferred (fast, no copy), falls back to copy if cross-device
        try:
            os.link(file_path.resolve(), temp_file)
            logger.debug(f"  Created hardlink: {temp_file}")
        except OSError as e:
            # Hardlink failed (cross-device link), fall back to copy
            logger.debug(f"  Hardlink failed ({e}), copying file...")
            shutil.copy2(file_path, temp_file)
        file_payload = str(temp_file)

        logger.info(f"üì§ Uploading {audio_filename} ({file_size_mb:.1f} MB)...")
        logger.info(f"   üè∑Ô∏è  Fireflies title: {prefixed_name}")

        try:
            # Upload logic - wrapped in try/finally for temp file cleanup
            return self._do_upload(file_payload, file_name, file_size_mb)
        finally:
            # Cleanup temp file if created
            if temp_file and temp_file.exists():
                try:
                    temp_file.unlink()
                    logger.debug(f"  Cleaned up temp file: {temp_file}")
                except Exception as e:
                    logger.debug(f"  Could not cleanup temp file: {e}")

    def _do_upload(self, file_payload, file_name: str, file_size_mb: float) -> str:
        """Internal upload logic. Returns file name for polling."""
        # Wait for page to be fully loaded
        self.page.wait_for_timeout(2000)

        # Take screenshot for debugging
        self._screenshot("before_upload")

        # Debug: log what we see on the page
        page_info = self.page.evaluate("""
            () => {
                const fileInputs = document.querySelectorAll('input[type="file"]');
                const buttons = document.querySelectorAll('button');
                const dropzones = document.querySelectorAll('[class*="drop"], [class*="upload"]');
                return {
                    url: window.location.href,
                    fileInputCount: fileInputs.length,
                    buttonCount: buttons.length,
                    dropzoneCount: dropzones.length,
                    bodyText: document.body.innerText.substring(0, 500),
                };
            }
        """)
        logger.debug(f"  Page URL: {page_info.get('url')}")
        logger.debug(
            f"  File inputs: {page_info.get('fileInputCount')}, Buttons: {page_info.get('buttonCount')}"
        )
        logger.debug(f"  Dropzones: {page_info.get('dropzoneCount')}")

        # Try to find file input (may be hidden) - retry if not found initially
        file_input = None
        for attempt in range(5):
            file_input = self.page.locator('input[type="file"]').first
            if file_input.count() > 0:
                break
            logger.debug(f"  File input not found, retrying... ({attempt + 1}/5)")
            self.page.wait_for_timeout(2000)
            # Try refreshing the page if stuck
            if attempt == 2:
                logger.debug("  Refreshing page...")
                self.page.reload(wait_until="domcontentloaded")
                self.page.wait_for_timeout(3000)

        if file_input and file_input.count() > 0:
            # Use file chooser dialog
            try:
                with self.page.expect_file_chooser(timeout=5000) as fc_info:
                    # Click on any visible upload trigger
                    upload_triggers = [
                        'div[class*="dropzone"]',
                        'div[class*="upload"]',
                        'button:has-text("Upload")',
                        'button:has-text("Browse")',
                        'label[for*="file"]',
                    ]

                    clicked = False
                    for selector in upload_triggers:
                        try:
                            el = self.page.locator(selector).first
                            if el.count() > 0 and el.is_visible():
                                el.click()
                                clicked = True
                                logger.debug(f"  Clicked: {selector}")
                                break
                        except Exception:
                            continue

                    if not clicked:
                        # Click the file input directly (may be hidden but still works)
                        file_input.dispatch_event("click")

                file_chooser = fc_info.value
                file_chooser.set_files(file_payload)  # Use buffer with custom name
                logger.info("  File selected via file chooser")

            except PlaywrightTimeout:
                # Fallback: set input files directly
                logger.debug("  File chooser timeout, using direct input...")
                file_input.set_input_files(file_payload)  # Use buffer with custom name
                logger.info("  File set via direct input")
        else:
            raise RuntimeError("No file input found on upload page")

        # Wait for upload modal to appear
        self.page.wait_for_timeout(2000)
        self._screenshot("after_file_select")

        # Verify language is set to "Auto-detect" (this is the default)
        # The upload modal has a custom dropdown that defaults to Auto-detect
        try:
            # Check if Auto-detect is visible in the modal (it's the default)
            modal_text = self.page.evaluate("() => document.body.innerText")
            if "Auto-detect" in modal_text:
                logger.debug("  ‚úÖ Language: Auto-detect (default)")
            else:
                logger.debug("  Language selector not visible, using default")
        except Exception as e:
            logger.debug(f"  Language check skipped: {e}")

        # Click the Upload button in modal
        logger.debug("  Looking for Upload button...")
        upload_btn_selectors = [
            'button:has-text("Upload"):visible',
            'button[type="submit"]:visible',
            '.modal button:has-text("Upload")',
            'div[role="dialog"] button:has-text("Upload")',
        ]

        upload_clicked = False
        for selector in upload_btn_selectors:
            try:
                btn = self.page.locator(selector).last
                if btn.count() > 0 and btn.is_visible():
                    btn.click()
                    upload_clicked = True
                    logger.info("  Clicked Upload button")
                    break
            except Exception as e:
                logger.debug(f"  Button {selector} failed: {e}")
                continue

        if not upload_clicked:
            logger.warning(
                "  Could not find Upload button, file may still be uploading..."
            )

        # Wait for upload to complete by watching page indicators
        max_upload_wait = min(600, max(120, int(file_size_mb * 5)))
        start_time = time.time()

        # Initial wait for upload to start
        self.page.wait_for_timeout(3000)

        while time.time() - start_time < max_upload_wait:
            # Check page for upload progress indicators
            status = self.page.evaluate("""
                () => {
                    const text = document.body.innerText;
                    const hasProgress = text.includes('Uploading') || text.includes('%') ||
                                        document.querySelectorAll('[class*="progress"]').length > 0;
                    const hasError = text.includes('Error') || text.includes('failed');
                    const hasSuccess = text.includes('uploaded') || text.includes('Processing') ||
                                       text.includes('Transcribing');
                    return { hasProgress, hasError, hasSuccess, sample: text.substring(0, 500) };
                }
            """)

            if status.get("hasError"):
                self._screenshot("upload_error")
                raise RuntimeError("Upload failed - check browser for details")

            if status.get("hasSuccess"):
                elapsed = int(time.time() - start_time)
                logger.info(f"  ‚úÖ Upload complete ({elapsed}s)")
                # Verify file appears in upload list
                if self._verify_upload_in_list(file_name):
                    logger.info("  ‚úÖ Verified file in upload list")
                else:
                    logger.warning("  ‚ö†Ô∏è Could not verify file in upload list")
                return file_name

            if status.get("hasProgress"):
                elapsed = int(time.time() - start_time)
                logger.info(f"  Uploading... ({elapsed}s)")
            else:
                # No progress indicator - verify file actually appeared in list
                elapsed = int(time.time() - start_time)
                if elapsed > 10:  # Give initial upload a few seconds to start
                    if self._verify_upload_in_list(file_name, max_attempts=1):
                        logger.info(f"  ‚úÖ Upload complete ({elapsed}s)")
                        logger.info("  ‚úÖ Verified file in upload list")
                        return file_name

            self.page.wait_for_timeout(3000)

        # Final verification attempt before giving up
        if self._verify_upload_in_list(file_name, max_attempts=3):
            logger.info(f"  ‚úÖ Upload complete (verified after {max_upload_wait}s)")
            return file_name

        # If we reach here, upload likely failed
        logger.warning(f"  ‚ö†Ô∏è Upload could not be verified after {max_upload_wait}s")
        self._screenshot("upload_timeout")
        raise RuntimeError(
            f"Upload verification failed - file not found in upload list after {max_upload_wait}s"
        )

    def get_existing_transcript_ids(self) -> set[str]:
        """Get set of existing transcript IDs before upload."""
        if self.public_api:
            return self.public_api.get_transcript_ids()
        return set()

    def poll_for_transcript_via_public_api(
        self,
        file_name: str,
        max_wait_minutes: int = 45,
        exclude_ids: set[str] = None,
    ) -> str | None:
        """Poll public API for NEW transcript by matching file name.

        Args:
            file_name: Name of uploaded file to match
            max_wait_minutes: Maximum wait time
            exclude_ids: Set of transcript IDs to exclude (existed before upload)
        """
        if not self.public_api:
            logger.error("No public API configured for polling")
            return None

        logger.info(f"‚è≥ Waiting for transcription (max {max_wait_minutes} min)...")

        exclude_ids = exclude_ids or set()
        # Remove extension for matching
        search_term = Path(file_name).stem

        start_time = time.time()
        max_wait = max_wait_minutes * 60

        while time.time() - start_time < max_wait:
            try:
                transcripts = self.public_api.list_transcripts(limit=20)

                for t in transcripts:
                    transcript_id = t.get("id")
                    # Skip if this ID existed before upload
                    if transcript_id in exclude_ids:
                        continue
                    title = t.get("title", "")
                    # Match by file name in title
                    if search_term in title:
                        if transcript_id:
                            elapsed = int(time.time() - start_time)
                            logger.info(
                                f"  ‚úÖ Transcript found ({elapsed}s): {transcript_id[:20]}..."
                            )
                            return transcript_id

            except Exception as e:
                logger.warning(f"  API poll error: {e}")

            elapsed = int(time.time() - start_time)
            logger.info(f"  Waiting for transcript... ({elapsed}s)")
            time.sleep(30)

        logger.error(f"‚ùå Timeout after {max_wait_minutes} minutes")
        return None

    def _verify_upload_in_list(self, file_name: str, max_attempts: int = 3) -> bool:
        """Verify uploaded file appears in the upload page list.

        Args:
            file_name: The file name to look for (with FIREFLIES_CLI__ prefix)
            max_attempts: Number of retry attempts

        Returns:
            True if file found in list, False otherwise
        """
        # Extract the core name without prefix and extension for matching
        search_name = file_name
        if search_name.startswith(CLI_UPLOAD_PREFIX):
            search_name = search_name[len(CLI_UPLOAD_PREFIX) :]
        # Remove extension
        search_name = Path(search_name).stem

        # Extract first significant words for matching (handles special chars)
        # e.g., "Steve Yegge's Vibe Coding" ‚Üí ["Steve", "Yegge", "Vibe", "Coding"]
        words = re.findall(r"\b[A-Za-z]{4,}\b", search_name)[:4]
        if not words:
            # Fallback: use first 20 alphanumeric chars
            words = [re.sub(r"[^A-Za-z0-9]", "", search_name)[:20]]

        for attempt in range(max_attempts):
            try:
                # Get page text content
                page_text = self.page.evaluate("""
                    () => document.body.innerText
                """)

                # Check if all key words are present (case-insensitive)
                page_lower = page_text.lower()
                matches = sum(1 for w in words if w.lower() in page_lower)

                # Consider verified if most words match (handles truncation)
                if matches >= len(words) * 0.75 or matches >= 2:
                    logger.debug(f"  Matched {matches}/{len(words)} words: {words}")
                    return True

                # Also check for FIREFLIES_CLI__ prefix
                if "FIREFLIES_CLI__" in page_text:
                    logger.debug("  Found FIREFLIES_CLI__ prefix in page")
                    return True

                if attempt < max_attempts - 1:
                    logger.debug(
                        f"  File not found in list ({matches}/{len(words)} words), retrying... ({attempt + 1}/{max_attempts})"
                    )
                    self.page.wait_for_timeout(2000)

            except Exception as e:
                logger.debug(f"  Verify upload error: {e}")

        return False

    def _screenshot(self, name: str):
        """Save debug screenshot. Only saves when logging level is DEBUG."""
        if not logger.isEnabledFor(logging.DEBUG):
            return
        try:
            path = f"/tmp/fireflies_{name}_{int(time.time())}.png"
            self.page.screenshot(path=path)
            logger.debug(f"  üì∏ Screenshot: {path}")
        except Exception as e:
            logger.debug(f"  Screenshot failed: {e}")


# =============================================================================
# Download API
# =============================================================================


class FirefliesDownloadAPI:
    """Download transcripts via download.fireflies.ai using browser cookies."""

    DOWNLOAD_API = "https://download.fireflies.ai"

    def __init__(self, browser: str = "brave"):
        self.session = requests.Session()
        self.auth_token = None
        self._load_cookies(browser)

    def _load_cookies(self, browser: str):
        """Load cookies from browser."""
        try:
            import browser_cookie3
        except ImportError:
            raise RuntimeError("pip install browser_cookie3")

        browser_funcs = {
            "brave": browser_cookie3.brave,
            "chrome": browser_cookie3.chrome,
            "firefox": browser_cookie3.firefox,
        }

        if browser not in browser_funcs:
            raise ValueError(f"Unsupported: {browser}")

        cj = browser_funcs[browser](domain_name=".fireflies.ai")
        for cookie in cj:
            self.session.cookies.set(cookie.name, cookie.value, domain=".fireflies.ai")

            if cookie.name == "authorization":
                token = unquote(cookie.value)
                if token.startswith("Bearer "):
                    token = token[7:]
                self.auth_token = token

        if not self.auth_token:
            raise RuntimeError(
                "No auth token found. Login to Fireflies in browser first."
            )

    def _validate_download(
        self, content: bytes, file_type: str, fmt: str
    ) -> tuple[bool, str]:
        """Validate downloaded content is valid.

        Returns:
            (is_valid, error_message) tuple
        """
        if not content:
            return False, "Empty content"

        if len(content) < 100:
            return False, f"Content too small ({len(content)} bytes)"

        if fmt == "pdf":
            # Check PDF magic bytes
            if not content.startswith(b"%PDF"):
                return False, "Invalid PDF (missing %PDF header)"

        elif fmt == "md":
            try:
                text = content.decode("utf-8")
                # Transcript MD should have speaker names (bold)
                if file_type == "transcript":
                    has_speaker = "**" in text
                    if not has_speaker:
                        return False, "Transcript MD missing speaker markers"
                # Summary MD should have section headers
                elif file_type == "summary":
                    has_header = "#" in text
                    if not has_header:
                        return False, "Summary MD missing headers"
            except UnicodeDecodeError:
                return False, "Invalid MD (not UTF-8 text)"

        return True, ""

    def download(self, transcript_id: str, file_type: str, fmt: str) -> bytes | None:
        """Download file (transcript/summary, md/pdf).

        Endpoints:
        - Transcript: /download/{fmt}/transcript/{id} with speaker/timestamp params
        - Summary: /{file_type}/{fmt}/{id} with withAIApps/withTimestamps params
        """
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json, text/plain, */*",
            "Origin": "https://app.fireflies.ai",
            "Referer": f"https://app.fireflies.ai/view/{transcript_id}",
            "authorization": self.auth_token,  # Header auth for summary endpoint
        }

        if file_type == "summary":
            # Summary uses different endpoint: /summary/pdf/{id} or /summary/md/{id}
            url = f"{self.DOWNLOAD_API}/{file_type}/{fmt}/{transcript_id}"
            body = {
                "token": self.auth_token,  # NO "Bearer " prefix
                "withAIApps": True,
                "withTimestamps": True,
                "timezone": "UTC",
            }
        else:
            # Transcript uses: /download/{fmt}/transcript/{id}
            url = f"{self.DOWNLOAD_API}/download/{fmt}/{file_type}/{transcript_id}"
            body = {
                "speaker": True,
                "timestamp": True,
                "timezone": "UTC",
                "token": f"Bearer {self.auth_token}",
            }

        logger.info(f"  üì• Downloading {file_type}.{fmt}...")

        try:
            resp = self.session.post(url, json=body, headers=headers, timeout=60)

            if resp.status_code != 200:
                logger.error(f"    HTTP {resp.status_code}")
                return None

            # Summary endpoint returns file directly, transcript returns JSON with downloadUrl
            content_type = resp.headers.get("Content-Type", "")
            content = None

            # Check for direct file response (PDF, markdown, or undefined for summary MD)
            if (
                "application/pdf" in content_type
                or "text/markdown" in content_type
                or (file_type == "summary" and "application/json" not in content_type)
            ):
                # Direct file response
                content = resp.content
            else:
                # JSON response with downloadUrl
                data = resp.json()
                if not data.get("ok"):
                    logger.error(f"    {data.get('message', 'Error')}")
                    return None

                download_url = data.get("downloadUrl")
                if not download_url:
                    return None

                file_resp = self.session.get(download_url, timeout=60)
                if file_resp.status_code == 200:
                    content = file_resp.content

            # Validate downloaded content
            if content:
                is_valid, error = self._validate_download(content, file_type, fmt)
                size_kb = len(content) / 1024
                if is_valid:
                    logger.info(f"    ‚úÖ {size_kb:.1f} KB")
                    return content
                else:
                    logger.warning(f"    ‚ö†Ô∏è {size_kb:.1f} KB - {error}")
                    return content  # Return anyway, let caller decide

        except Exception as e:
            logger.error(f"    {e}")

        return None


# --- Public API (for summary data) ---


class FirefliesPublicAPI:
    """Get transcript data from public API."""

    API_URL = "https://api.fireflies.ai/graphql"

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.session = requests.Session()
        self.session.headers.update(
            {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        )

    def get_transcript(self, transcript_id: str) -> dict | None:
        """Fetch transcript with summary."""
        query = """
        query($transcriptId: String!) {
            transcript(id: $transcriptId) {
                id
                title
                date
                summary {
                    notes
                    action_items
                }
            }
        }
        """

        try:
            response = self.session.post(
                self.API_URL,
                json={"query": query, "variables": {"transcriptId": transcript_id}},
                timeout=60,
            )

            if response.status_code == 200:
                data = response.json()
                return data.get("data", {}).get("transcript")

        except Exception as e:
            logger.error(f"  API error: {e}")

        return None

    def delete_transcript(self, transcript_id: str, verify: bool = True) -> bool:
        """Delete a transcript. Returns True if successful.

        Args:
            transcript_id: ID of transcript to delete
            verify: If True, verify deletion by checking transcript no longer exists
        """
        query = """
        mutation DeleteTranscript($id: String!) {
            deleteTranscript(id: $id) {
                id
                title
            }
        }
        """

        try:
            response = self.session.post(
                self.API_URL,
                json={"query": query, "variables": {"id": transcript_id}},
                timeout=30,
            )

            if response.status_code == 200:
                data = response.json()
                logger.debug(f"  Delete response: {data}")
                deleted = (
                    data.get("data", {}).get("deleteTranscript")
                    if data.get("data")
                    else None
                )
                if deleted:
                    logger.info(f"  ‚úÖ Deleted: {deleted.get('title', transcript_id)}")

                    # Verify deletion
                    if verify:
                        if self._verify_deleted(transcript_id):
                            logger.info("  ‚úÖ Verified: transcript removed from list")
                        else:
                            logger.warning(
                                "  ‚ö†Ô∏è Could not verify deletion (may be delayed)"
                            )

                    return True
                if "errors" in data and data["errors"]:
                    err = data["errors"][0]
                    msg = (
                        err.get("message", str(err))
                        if isinstance(err, dict)
                        else str(err)
                    )
                    logger.error(f"  Delete error: {msg}")

        except Exception as e:
            logger.error(f"  Delete error: {e}")

        return False

    def _verify_deleted(self, transcript_id: str, max_attempts: int = 3) -> bool:
        """Verify transcript was deleted by checking it no longer exists.

        Note: Fireflies has eventual consistency, so deletion may take a few seconds.
        """
        for attempt in range(max_attempts):
            try:
                # Try to fetch the transcript - should fail if deleted
                result = self.get_transcript(transcript_id)
                if result is None:
                    return True  # Not found = successfully deleted

                # Still exists, wait and retry
                if attempt < max_attempts - 1:
                    time.sleep(2)

            except Exception:
                return True  # Error fetching = likely deleted

        return False

    def get_transcript_ids(self, limit: int = 50) -> set[str]:
        """Get set of transcript IDs (for checking existing before upload)."""
        query = f"""
        query {{
            transcripts(limit: {limit}) {{
                id
            }}
        }}
        """
        try:
            resp = self.session.post(self.API_URL, json={"query": query}, timeout=30)
            if resp.status_code == 200:
                data = resp.json()
                transcripts = data.get("data", {}).get("transcripts", [])
                return {t.get("id") for t in transcripts if t.get("id")}
        except Exception as e:
            logger.debug(f"  Could not get transcript IDs: {e}")
        return set()

    def list_transcripts(self, limit: int = 20) -> list[dict]:
        """List recent transcripts with basic info."""
        query = f"""
        query {{
            transcripts(limit: {limit}) {{
                id
                title
                date
                duration
            }}
        }}
        """
        try:
            resp = self.session.post(self.API_URL, json={"query": query}, timeout=30)
            if resp.status_code == 200:
                data = resp.json()
                return data.get("data", {}).get("transcripts", [])
        except Exception as e:
            logger.debug(f"  Could not list transcripts: {e}")
        return []

    def validate_api_key(self) -> tuple[bool, str]:
        """Validate API key is working. Returns (is_valid, error_message)."""
        try:
            resp = self.session.post(
                self.API_URL,
                json={"query": "query { transcripts(limit: 1) { id } }"},
                timeout=10,
            )
            if resp.status_code == 401:
                return False, "API key invalid (unauthorized)"
            if resp.status_code != 200:
                return True, ""  # Non-401 errors might be temporary
            data = resp.json()
            if "errors" in data:
                error_msg = data["errors"][0].get("message", "Unknown")
                if (
                    "unauthorized" in error_msg.lower()
                    or "invalid" in error_msg.lower()
                ):
                    return False, f"API key invalid: {error_msg}"
            return True, ""
        except Exception as e:
            logger.debug(f"  API key validation error: {e}")
            return True, ""  # Assume valid on network errors


# =============================================================================
# Helpers
# =============================================================================


def sanitize_filename(name: str, max_length: int = 200) -> str:
    """Sanitize for filename use. Preserves apostrophes and full-width colon.

    Removes:
    - Filesystem-invalid characters (/ \\ : * ? " < > |)
    - Emojis - can cause Dropbox sync issues

    Preserves:
    - Full-width colon (U+FF1A) - already filesystem-safe, used by yt-dlp
    - Apostrophes - valid on macOS/Linux
    """
    # Remove emojis (Dropbox sync issues)
    name = re.sub(r"[\U0001F300-\U0001F9FF]", "", name)  # Misc symbols, emoticons
    name = re.sub(r"[\U00002600-\U000027BF]", "", name)  # Misc symbols

    # Replace filesystem-invalid characters (keep full-width colon Ôºö)
    for old, new in {
        "/": "-",
        "\\": "-",
        ":": "-",
        "*": "",
        "?": "",
        '"': "",
        "<": "",
        ">": "",
        "|": "-",
    }.items():
        name = name.replace(old, new)
    name = re.sub(r"\s+", " ", name).strip(" -")
    return name[:max_length] if len(name) > max_length else name


def detect_input_type(input_arg: str) -> str:
    """Detect input type.

    Returns:
        - "media_url": Any URL (YouTube, Vimeo, Twitter, etc.) - handled by yt-dlp
        - "file": Local audio file
        - "transcript_id": Fireflies transcript ID
        - "unknown": Unrecognized input
    """
    if input_arg.startswith(("http://", "https://")):
        # All URLs go through yt-dlp (supports 1000+ sites)
        return "media_url"
    if Path(input_arg).exists():
        return "file"
    if re.match(r"^[A-Z0-9]{20,}$", input_arg):
        return "transcript_id"
    return "unknown"


def check_existing_output_files(input_name: str, output_dir: Path) -> list[Path]:
    """Check if output files already exist.

    Args:
        input_name: Base name of input file (without extension)
        output_dir: Directory where output files would be saved

    Returns:
        List of existing file paths that would be overwritten
    """
    # Remove extension from input name
    base_name = Path(input_name).stem

    # Strip CLI prefix if present (loop handles double-prefix from v1.11.3 bug)
    while base_name.startswith(CLI_UPLOAD_PREFIX):
        base_name = base_name[len(CLI_UPLOAD_PREFIX) :]

    # Sanitize the base name (same as workflow does)
    base_name = sanitize_filename(base_name)

    # Output file patterns
    output_patterns = [
        f"{base_name} - Fireflies Transcript.md",
        f"{base_name} - Fireflies Transcript.pdf",
        f"{base_name} - Fireflies Summary.md",
        f"{base_name} - Fireflies Summary.pdf",
    ]

    existing = []
    for pattern in output_patterns:
        path = output_dir / pattern
        if path.exists():
            existing.append(path)

    return existing


# =============================================================================
# YouTube Helpers
# =============================================================================


def get_youtube_info(url: str) -> dict | None:
    """Get video title and ID without downloading."""
    import yt_dlp

    try:
        with yt_dlp.YoutubeDL({"quiet": True, "no_warnings": True}) as ydl:
            info = ydl.extract_info(url, download=False)
            return {
                "title": info.get("title", "Untitled"),
                "id": info.get("id", "unknown"),
            }
    except Exception as e:
        logger.error(f"Failed to get video info: {e}")
        return None


def create_dated_folder_path(title: str, video_id: str, base_dir: Path) -> Path:
    """Create folder path: YYYY-MM-DD Title [video_id]/"""
    from datetime import date

    date_str = date.today().strftime("%Y-%m-%d")
    safe_title = sanitize_filename(title)
    folder_name = f"{date_str} {safe_title} [{video_id}]"
    return base_dir / folder_name


class MediaDownloader:
    """Download audio from URLs via yt-dlp (supports 1000+ sites).

    Supported sites include: YouTube, Vimeo, Twitter/X, TikTok, Twitch,
    SoundCloud, Dailymotion, Facebook, Instagram, and many more.
    Full list: https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md
    """

    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        try:
            import yt_dlp  # noqa: F401
        except ImportError:
            raise RuntimeError("yt-dlp not found. Install: pip install yt-dlp")

    def download(
        self, url: str, output_basename: str | None = None, keep_video: bool = False
    ) -> tuple[Path | None, Path | None]:
        """Download audio (and optionally video), returns (audio_path, video_path) or (None, None).

        Args:
            url: Media URL to download (YouTube, Vimeo, Twitter, etc.)
            output_basename: Optional custom basename for output file (without extension)
            keep_video: If True, also download/keep the original video file

        Returns:
            Tuple of (audio_path, video_path). video_path is None if keep_video=False.
        """
        import yt_dlp

        logger.info("üé• Downloading audio from URL...")

        # First get video info (without downloading)
        try:
            with yt_dlp.YoutubeDL({"quiet": True, "no_warnings": True}) as ydl:
                info = ydl.extract_info(url, download=False)
        except Exception as e:
            logger.error(f"  Failed to get video info: {e}")
            return (None, None)

        title = info.get("title", "Untitled")
        video_id = info.get("id", "unknown")
        logger.info(f"  Title: {title}")

        # Use custom basename if provided, otherwise default format: "Title [id]"
        if output_basename:
            file_basename = output_basename
        else:
            safe_title = sanitize_filename(title)
            file_basename = f"{safe_title} [{video_id}]"

        output_template = str(self.output_dir / f"{file_basename}.%(ext)s")

        # Progress tracking state (use dict to allow mutation in closure)
        progress_state = {
            "last_percent": -1,
            "last_log_time": 0.0,
            "last_downloaded": 0,
            "fragment_index": 0,
            "fragment_count": 0,
            "total_bytes_all": 0,
            "current_filename": "",
            "start_time": time.time(),
            "is_fragment_download": False,
        }

        def format_bytes(b: int | float) -> str:
            """Format bytes to human readable string."""
            if b >= 1024 * 1024 * 1024:
                return f"{b / 1024 / 1024 / 1024:.2f} GB"
            elif b >= 1024 * 1024:
                return f"{b / 1024 / 1024:.1f} MB"
            elif b >= 1024:
                return f"{b / 1024:.0f} KB"
            return f"{b:.0f} B"

        def format_speed(s: float) -> str:
            """Format speed to human readable string."""
            if s <= 0:
                return "..."
            elif s >= 1024 * 1024:
                return f"{s / 1024 / 1024:.1f} MB/s"
            elif s >= 1024:
                return f"{s / 1024:.0f} KB/s"
            return f"{s:.0f} B/s"

        def format_eta(seconds: int | float | None) -> str:
            """Format ETA to human readable string."""
            if not seconds or seconds <= 0:
                return "..."
            seconds = int(seconds)
            if seconds >= 3600:
                return f"{seconds // 3600}h{(seconds % 3600) // 60:02d}m"
            elif seconds >= 60:
                return f"{seconds // 60}m{seconds % 60:02d}s"
            return f"{seconds}s"

        def progress_hook(d: dict) -> None:
            now = time.time()

            if d["status"] == "downloading":
                # Get available progress info
                total = d.get("total_bytes") or d.get("total_bytes_estimate") or 0
                downloaded = d.get("downloaded_bytes", 0)
                speed = d.get("speed") or 0
                eta = d.get("eta")
                filename = d.get("filename", "")

                # Fragment-based progress (DASH/HLS streams)
                frag_index = d.get("fragment_index", 0)
                frag_count = d.get("fragment_count", 0)

                # Detect fragment downloads (common for YouTube)
                if frag_count > 0:
                    progress_state["is_fragment_download"] = True
                    progress_state["fragment_index"] = frag_index
                    progress_state["fragment_count"] = frag_count

                # Track new file being downloaded
                if filename and filename != progress_state["current_filename"]:
                    progress_state["current_filename"] = filename
                    progress_state["last_percent"] = -1
                    progress_state["last_downloaded"] = 0
                    # Log new stream start (video vs audio)
                    basename = Path(filename).name if filename else ""
                    if ".f" in basename:  # Fragment temp file
                        pass  # Don't spam for temp fragments
                    else:
                        log_verbose(f"  üì¶ Stream: {basename[:60]}...")

                # Calculate overall progress percentage
                if progress_state["is_fragment_download"] and frag_count > 0:
                    # Use fragment progress for DASH/HLS
                    percent = int((frag_index / frag_count) * 100)
                    progress_info = f"fragment {frag_index}/{frag_count}"
                elif total > 0 and total > 10240:  # Ignore tiny manifest files < 10KB
                    percent = int(downloaded * 100 / total)
                    progress_info = f"{format_bytes(downloaded)}/{format_bytes(total)}"
                else:
                    # Unknown total - just show downloaded amount
                    percent = -1
                    progress_info = format_bytes(downloaded)

                # Throttle logging: 1% increments until 5%, then every 5% (max ~25 lines)
                # Pattern: 0, 1, 2, 3, 4, 5, 10, 15, 20, ... 100
                should_log = False
                if percent >= 0:
                    last_pct = progress_state["last_percent"]
                    if last_pct < 0:
                        # First log
                        should_log = True
                    elif percent <= 5:
                        # 1% increments for 0-5%
                        if percent > last_pct:
                            should_log = True
                    else:
                        # 5% increments after 5%
                        current_bucket = (percent // 5) * 5
                        last_bucket = (last_pct // 5) * 5
                        if current_bucket > last_bucket:
                            should_log = True
                elif downloaded > progress_state["last_downloaded"] + (10 * 1024 * 1024):
                    # Log every 10MB for unknown-size downloads
                    should_log = True

                if should_log:
                    progress_state["last_percent"] = percent
                    progress_state["last_downloaded"] = downloaded
                    progress_state["last_log_time"] = now

                    # Build progress line
                    if percent >= 0:
                        line = f"  üì• {percent:3d}% ({progress_info})"
                    else:
                        line = f"  üì• {progress_info}"

                    if speed > 0:
                        line += f" @ {format_speed(speed)}"
                    if eta and eta > 0:
                        line += f" ETA: {format_eta(eta)}"

                    logger.info(line)

            elif d["status"] == "finished":
                # File/fragment finished
                downloaded = d.get("downloaded_bytes") or d.get("total_bytes") or 0
                elapsed = d.get("elapsed", 0)
                filename = d.get("filename", "")

                # Accumulate total bytes
                progress_state["total_bytes_all"] += downloaded

                # Only log for significant downloads (not tiny manifest files)
                if downloaded > 102400:  # > 100KB
                    elapsed_str = f" in {elapsed:.1f}s" if elapsed else ""
                    basename = Path(filename).name if filename else ""
                    # Truncate long filenames
                    if len(basename) > 50:
                        basename = basename[:47] + "..."
                    logger.info(f"  ‚úÖ {format_bytes(downloaded)}{elapsed_str}")

                # Reset for next file/fragment
                progress_state["last_percent"] = -1
                progress_state["last_downloaded"] = 0

            elif d["status"] == "error":
                error_msg = d.get("error", "unknown error")
                logger.error(f"  ‚ùå Download error: {error_msg}")

        def postprocessor_hook(d: dict) -> None:
            if d["status"] == "started":
                pp_name = d.get("postprocessor", "unknown")
                # Friendly names for common post-processors
                pp_display = {
                    "FFmpegExtractAudio": "Extracting audio",
                    "FFmpegVideoConvertor": "Converting video",
                    "FFmpegMerger": "Merging video+audio",
                    "MoveFiles": "Finalizing",
                    "FFmpegMetadata": "Adding metadata",
                    "EmbedThumbnail": "Embedding thumbnail",
                }.get(pp_name, pp_name)
                logger.info(f"  üîÑ {pp_display}...")
            elif d["status"] == "finished":
                pass  # Don't spam "finished" for each post-processor

        # When keep_video=True, download video and extract audio separately
        if keep_video:
            return self._download_with_video(
                url,
                file_basename,
                output_template,
                progress_hook,
                postprocessor_hook,
            )

        # Try multiple download strategies (YouTube keeps changing restrictions)
        strategies = [
            # Strategy 1: Extract audio as m4a (fastest when it works)
            {
                "name": "audio extraction",
                "opts": {
                    "format": "bestaudio/best",
                    "postprocessors": [
                        {
                            "key": "FFmpegExtractAudio",
                            "preferredcodec": "m4a",
                        }
                    ],
                },
            },
            # Strategy 2: Best audio format directly (no conversion)
            {
                "name": "best audio",
                "opts": {
                    "format": "bestaudio/best",
                },
            },
            # Strategy 3: Download video and extract audio (most reliable)
            {
                "name": "video+extract",
                "opts": {
                    "format": "best[height<=720]/best",
                    "postprocessors": [
                        {
                            "key": "FFmpegExtractAudio",
                            "preferredcodec": "m4a",
                        }
                    ],
                },
            },
        ]

        # Network errors that should trigger retry
        network_error_patterns = [
            "timed out",
            "Read timed out",
            "Connection reset",
            "Connection refused",
            "HTTPSConnectionPool",
            "bytes read",
            "IncompleteRead",
            "ConnectionError",
        ]

        def is_network_error(error_msg: str) -> bool:
            return any(
                pattern.lower() in error_msg.lower()
                for pattern in network_error_patterns
            )

        def reset_progress_state() -> None:
            """Reset progress state for new download attempt."""
            progress_state["last_percent"] = -1
            progress_state["last_log_time"] = 0.0
            progress_state["last_downloaded"] = 0
            progress_state["fragment_index"] = 0
            progress_state["fragment_count"] = 0
            progress_state["current_filename"] = ""
            progress_state["is_fragment_download"] = False

        for strategy in strategies:
            logger.info(f"  Trying {strategy['name']}...")

            # Reset progress tracking for each strategy
            reset_progress_state()

            ydl_opts = {
                "outtmpl": output_template,
                "noplaylist": True,
                "quiet": True,  # Suppress yt-dlp's own output
                "no_warnings": True,
                "noprogress": True,  # We handle progress ourselves
                "progress_hooks": [progress_hook],
                "postprocessor_hooks": [postprocessor_hook],
                **strategy["opts"],
            }

            # Retry loop for network errors
            max_network_retries = 3
            for retry_attempt in range(max_network_retries + 1):
                try:
                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        ydl.download([url])

                    # Search for downloaded file
                    for ext in [".m4a", ".webm", ".opus", ".mp3", ".ogg", ".mp4"]:
                        found_path = self.output_dir / f"{file_basename}{ext}"
                        if found_path.exists():
                            logger.info(f"  ‚úÖ Downloaded: {found_path.name}")
                            return (found_path, None)  # (audio_path, video_path=None)

                    # File not found after download - try next strategy
                    break

                except yt_dlp.utils.DownloadError as e:
                    error_str = str(e)
                    if (
                        is_network_error(error_str)
                        and retry_attempt < max_network_retries
                    ):
                        delay = 2**retry_attempt  # 1s, 2s, 4s
                        log_verbose(
                            f"  ‚ö†Ô∏è  Network error (attempt {retry_attempt + 1}/{max_network_retries + 1}): {error_str[:80]}"
                        )
                        log_verbose(f"  ‚è≥ Retrying in {delay}s...")
                        time.sleep(delay)
                        reset_progress_state()  # Reset progress display
                        continue
                    else:
                        logger.debug(f"  {strategy['name']} failed: {error_str[:100]}")
                        break  # Try next strategy

                except Exception as e:
                    logger.debug(f"  {strategy['name']} error: {e}")
                    break  # Try next strategy

        # All strategies failed
        logger.error("  ‚ùå YouTube download failed - all strategies exhausted")
        logger.error("  This may be a temporary YouTube issue. Try:")
        logger.error("    1. Update yt-dlp: pip install -U yt-dlp")
        logger.error("    2. Download manually and use the local file")

        return (None, None)

    def _download_with_video(
        self,
        url: str,
        file_basename: str,
        output_template: str,
        progress_hook: Callable,
        postprocessor_hook: Callable,
    ) -> tuple[Path | None, Path | None]:
        """Download video file and extract audio, keeping both.

        Uses multiple strategies with fallback (YouTube keeps changing restrictions).

        Returns:
            Tuple of (audio_path, video_path) or (None, None) on failure.
        """
        import yt_dlp

        logger.info("  üìπ Downloading video (will also extract audio)...")

        # Video extensions to search for
        video_exts = [".mp4", ".mkv", ".webm", ".mov", ".avi"]
        audio_exts = [".m4a", ".mp3", ".opus", ".ogg", ".wav"]

        # Multiple strategies - YouTube keeps changing what works
        strategies = [
            # Strategy 1: Best combined format (most compatible)
            {
                "name": "best combined",
                "format": "best[height<=720]/best",
                "merge": None,
            },
            # Strategy 2: Merge best video+audio (higher quality)
            {
                "name": "merge best",
                "format": "bestvideo[height<=720]+bestaudio/best",
                "merge": "mp4",
            },
            # Strategy 3: Any available format
            {
                "name": "any format",
                "format": "best",
                "merge": None,
            },
        ]

        video_path: Path | None = None
        audio_path: Path | None = None

        for strategy in strategies:
            logger.info(f"  Trying {strategy['name']}...")

            ydl_opts = {
                "outtmpl": output_template,
                "noplaylist": True,
                "quiet": True,
                "no_warnings": True,
                "noprogress": True,
                "progress_hooks": [progress_hook],
                "postprocessor_hooks": [postprocessor_hook],
                "format": strategy["format"],
            }
            if strategy["merge"]:
                ydl_opts["merge_output_format"] = strategy["merge"]

            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    ydl.download([url])

                # Find the downloaded video file
                for ext in video_exts:
                    candidate = self.output_dir / f"{file_basename}{ext}"
                    if candidate.exists():
                        video_path = candidate
                        logger.info(f"  ‚úÖ Video saved: {video_path.name}")
                        break

                if video_path:
                    break  # Success, exit strategy loop

            except yt_dlp.utils.DownloadError as e:
                logger.debug(f"  {strategy['name']} failed: {e}")
                continue  # Try next strategy

            except Exception as e:
                logger.debug(f"  {strategy['name']} error: {e}")
                continue

        if not video_path:
            logger.error("  ‚ùå Video download failed - all strategies exhausted")
            logger.error("  Try: pip install -U yt-dlp")
            return (None, None)

        # Extract audio from the video using optimized functions
        # (stream copy is instant for AAC, fallback to encoding with progress)
        logger.info("  üéµ Extracting audio from video...")

        try:
            import tempfile

            # Get video duration for progress reporting
            duration = _get_video_duration(video_path)
            timestamp = int(time.time())

            # Try stream copy first (instant for AAC/MP3/Opus)
            temp_dir = Path(tempfile.gettempdir())
            audio_path = _try_stream_copy(video_path, self.output_dir, timestamp)

            if audio_path:
                # Rename to clean filename
                final_audio = self.output_dir / f"{file_basename}{audio_path.suffix}"
                if audio_path != final_audio:
                    audio_path.rename(final_audio)
                    audio_path = final_audio
                logger.info(f"  ‚úÖ Audio saved: {audio_path.name}")
                return (audio_path, video_path)

            # Fallback: encode to MP3 with progress (handles long files properly)
            audio_path = _encode_to_mp3(video_path, self.output_dir, timestamp, duration)

            if audio_path:
                # Rename to clean filename
                final_audio = self.output_dir / f"{file_basename}.mp3"
                if audio_path != final_audio:
                    audio_path.rename(final_audio)
                    audio_path = final_audio
                logger.info(f"  ‚úÖ Audio saved: {audio_path.name}")
                return (audio_path, video_path)

            # Last resort: check if yt-dlp created an audio file
            for ext in audio_exts:
                candidate = self.output_dir / f"{file_basename}{ext}"
                if candidate.exists() and candidate != video_path:
                    logger.info(f"  ‚úÖ Found audio: {candidate.name}")
                    return (candidate, video_path)

            logger.error("  ‚ùå Audio extraction failed")
            return (None, video_path)

        except Exception as e:
            logger.error(f"  ‚ùå Unexpected error during audio extraction: {e}")
            return (None, video_path)


# =============================================================================
# Batch Mode Helpers
# =============================================================================

# Supported audio extensions for batch mode (must match FirefliesWorkflow.AUDIO_EXTENSIONS)
# Audio extensions that Fireflies reliably transcribes (tested 2026-01-22)
# These can be uploaded directly without conversion
AUDIO_EXTENSIONS = {
    ".mp3",     # MP3 - works
    ".m4a",     # AAC or ALAC in M4A container - works
    ".ogg",     # Opus or Vorbis in Ogg container - works
    ".opus",    # Opus - works
    ".webm",    # Opus in WebM container - works
}

# Audio extensions that Fireflies accepts but transcription often fails/hangs
# These will be re-encoded to MP3 before upload
AUDIO_EXTENSIONS_REENCODE = {
    ".wav",     # PCM - transcription hangs
    ".flac",    # FLAC - transcription hangs
    ".aac",     # Raw AAC (no container) - transcription hangs
    ".wma",     # WMA - transcription hangs
    ".aiff",    # AIFF - not tested, likely fails
    ".ac3",     # AC3/Dolby - not tested, likely fails
}

# Video extensions that can be converted to audio (requires ffmpeg)
VIDEO_EXTENSIONS = {
    ".mp4",
    ".mkv",
    ".avi",
    ".mov",
    ".wmv",
    ".flv",
    ".webm",  # Can be video or audio
    ".m4v",
    ".mpeg",
    ".mpg",
    ".3gp",
}

# Global list of temp files to clean up on exit
_temp_audio_files: list[Path] = []


def _cleanup_temp_audio_files() -> None:
    """Clean up any temporary audio files created from video conversion."""
    for temp_file in _temp_audio_files:
        try:
            if temp_file.exists():
                temp_file.unlink()
                log_verbose(f"  üßπ Cleaned up temp file: {temp_file.name}")
        except Exception as e:
            logger.debug(f"Failed to clean up temp file {temp_file}: {e}")


def _get_video_duration(video_path: Path) -> float | None:
    """Get video duration in seconds using ffprobe."""
    try:
        result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-show_entries",
                "format=duration",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                str(video_path),
            ],
            capture_output=True,
            text=True,
            timeout=30,
        )
        if result.returncode == 0 and result.stdout.strip():
            return float(result.stdout.strip())
    except Exception:
        pass
    return None


def _get_audio_codec(video_path: Path) -> str | None:
    """Get audio codec from video/audio file using ffprobe."""
    try:
        result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-select_streams",
                "a:0",
                "-show_entries",
                "stream=codec_name",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                str(video_path),
            ],
            capture_output=True,
            text=True,
            timeout=30,
        )
        if result.returncode == 0 and result.stdout.strip():
            return result.stdout.strip().lower()
    except Exception:
        pass
    return None


# Mapping of audio codecs to file extensions for stream copy
# Only codecs that Fireflies reliably transcribes (tested 2026-01-21)
# NOT supported: flac, wav, raw aac, wma (upload OK but transcription hangs)
STREAM_COPY_CODEC_MAP = {
    "aac": ".m4a",      # MPEG-4 Audio (most common in MP4)
    "alac": ".m4a",     # Apple Lossless in M4A container
    "mp3": ".mp3",      # MP3
    "opus": ".ogg",     # Opus in Ogg container (common in YouTube WebM)
    "vorbis": ".ogg",   # Vorbis in Ogg container
}


def _try_stream_copy(video_path: Path, temp_dir: Path, timestamp: int) -> Path | None:
    """Try to extract audio via stream copy (no re-encoding, instant).

    Works for codecs that Fireflies accepts: AAC, ALAC, MP3, Opus, Vorbis.
    Returns None if stream copy fails or codec is incompatible.
    """
    audio_codec = _get_audio_codec(video_path)
    log_verbose(f"  Source audio codec: {audio_codec or 'unknown'}")

    # Only attempt stream copy for known Fireflies-compatible codecs
    if audio_codec not in STREAM_COPY_CODEC_MAP:
        return None

    # Choose extension based on codec
    # Use clean name: video_stem + timestamp + audio_ext (upload_file adds FIREFLIES_CLI__ prefix)
    ext = STREAM_COPY_CODEC_MAP[audio_codec]
    temp_audio = temp_dir / f"{video_path.stem}_{timestamp}{ext}"

    logger.info("‚ö° Extracting audio (stream copy - no re-encoding)...")

    cmd = [
        "ffmpeg",
        "-i",
        str(video_path),
        "-vn",  # No video
        "-acodec",
        "copy",  # Stream copy - no re-encoding
        "-y",
        str(temp_audio),
    ]

    start_time = time.time()
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
    elapsed = time.time() - start_time

    if result.returncode == 0 and temp_audio.exists() and temp_audio.stat().st_size > 0:
        size_mb = temp_audio.stat().st_size / 1024 / 1024
        logger.info(
            f"  ‚úÖ Extracted: {temp_audio.name} ({size_mb:.1f} MB) in {elapsed:.1f}s"
        )
        _temp_audio_files.append(temp_audio)
        return temp_audio

    # Stream copy failed, clean up and return None
    log_verbose(
        f"  Stream copy failed: {result.stderr[:100] if result.stderr else 'unknown error'}"
    )
    if temp_audio.exists():
        temp_audio.unlink()
    return None


def _encode_to_mp3(
    video_path: Path, temp_dir: Path, timestamp: int, duration: float | None
) -> Path | None:
    """Encode audio to MP3 with VBR and threading (fast fallback)."""
    # Use clean name: video_stem + timestamp + .mp3 (upload_file adds FIREFLIES_CLI__ prefix)
    temp_audio = temp_dir / f"{video_path.stem}_{timestamp}.mp3"

    duration_str = f" ({duration / 60:.1f} min)" if duration else ""
    video_size_mb = video_path.stat().st_size / 1024 / 1024

    logger.info(
        f"üîÑ Encoding to MP3: {video_path.name} ({video_size_mb:.1f} MB{duration_str})"
    )
    log_verbose(f"  Temp file: {temp_audio}")

    # Fast encode: VBR quality 5 (~128kbps), use all CPU threads
    cmd = [
        "ffmpeg",
        "-i",
        str(video_path),
        "-vn",  # No video
        "-acodec",
        "libmp3lame",
        "-q:a",
        "5",  # VBR quality (~128kbps avg, good for speech)
        "-threads",
        "0",  # Use all CPU cores
        "-y",
        "-progress",
        "pipe:1",
        "-nostats",
        str(temp_audio),
    ]

    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    start_time = time.time()
    last_progress_time = start_time
    last_percent = -1

    # Read progress from stdout
    while True:
        line = process.stdout.readline()
        if not line and process.poll() is not None:
            break

        # Parse progress output (key=value format)
        if line.startswith("out_time_ms="):
            try:
                out_time_ms = int(line.split("=")[1].strip())
                out_time_sec = out_time_ms / 1_000_000

                if duration and duration > 0:
                    percent = min(100, (out_time_sec / duration) * 100)
                    elapsed = time.time() - start_time

                    # Update progress every 5% or every 10 seconds
                    now = time.time()
                    if (
                        int(percent) >= last_percent + 5
                        or now - last_progress_time >= 10
                    ):
                        # Estimate remaining time
                        if percent > 0:
                            eta = (elapsed / percent) * (100 - percent)
                            eta_str = f", ~{eta:.0f}s remaining"
                        else:
                            eta_str = ""

                        logger.info(
                            f"  Encoding: {percent:.0f}% ({elapsed:.0f}s elapsed{eta_str})"
                        )
                        last_percent = int(percent)
                        last_progress_time = now
                else:
                    # No duration available, show time processed
                    now = time.time()
                    if now - last_progress_time >= 10:
                        elapsed = now - start_time
                        logger.info(
                            f"  Encoding: {out_time_sec:.0f}s processed ({elapsed:.0f}s elapsed)"
                        )
                        last_progress_time = now
            except (ValueError, IndexError):
                pass

    # Wait for process to complete
    process.wait(timeout=600)
    total_time = time.time() - start_time

    if process.returncode != 0:
        stderr = process.stderr.read()
        logger.error(f"‚ùå ffmpeg encoding failed: {stderr[:200]}")
        return None

    if not temp_audio.exists():
        logger.error("‚ùå Encoding completed but output file not found")
        return None

    size_mb = temp_audio.stat().st_size / 1024 / 1024
    logger.info(
        f"  ‚úÖ Encoded: {temp_audio.name} ({size_mb:.1f} MB) in {total_time:.0f}s"
    )

    # Register for cleanup
    _temp_audio_files.append(temp_audio)
    return temp_audio


def convert_video_to_audio(video_path: Path) -> Path | None:
    """Convert video file to audio for transcription.

    Optimized for speed:
    1. Try stream copy for AAC/MP3 sources (instant, no re-encoding)
    2. Fallback to fast VBR MP3 encode with threading

    Args:
        video_path: Path to video file

    Returns:
        Path to audio file (.m4a or .mp3), or None if conversion failed
    """
    import tempfile

    # Check ffmpeg is available
    try:
        result = subprocess.run(
            ["ffmpeg", "-version"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            logger.error("‚ùå ffmpeg not working properly")
            return None
    except FileNotFoundError:
        logger.error("‚ùå ffmpeg not found. Install: brew install ffmpeg")
        return None

    # Use secure temp directory with unique name to prevent symlink attacks
    temp_dir = Path(tempfile.mkdtemp(prefix="ff_convert_"))
    timestamp = int(time.time())

    # Get video duration for progress calculation (used in fallback)
    duration = _get_video_duration(video_path)

    try:
        # Strategy 1: Try stream copy (instant for AAC/MP3 sources)
        result = _try_stream_copy(video_path, temp_dir, timestamp)
        if result:
            return result

        # Strategy 2: Fallback to fast MP3 encode
        log_verbose("  Falling back to MP3 encoding...")
        return _encode_to_mp3(video_path, temp_dir, timestamp, duration)

    except subprocess.TimeoutExpired:
        logger.error("‚ùå ffmpeg timed out (>10 min)")
        return None
    except Exception as e:
        logger.error(f"‚ùå Conversion error: {e}")
        return None


def expand_inputs(inputs: list[str]) -> list[tuple[Path, str | None]]:
    """Expand input patterns to list of audio files with original names.

    Handles:
    - Direct file paths (audio or video - video converted to audio)
    - Glob patterns (*.m4a, **/*.mp3)
    - Directories (all audio files in directory)

    Video files (.mp4, .mkv, etc.) are automatically converted to MP3 using ffmpeg.
    Problematic audio formats (.wav, .flac, .wma, etc.) are re-encoded to MP3.

    Args:
        inputs: List of file paths, glob patterns, or directories

    Returns:
        List of tuples (audio_path, original_name) where:
        - audio_path: Path to audio file
        - original_name: Original filename (for videos/re-encoded audio, the original name; for audio, None)
    """
    # Combine all supported audio extensions
    all_audio_extensions = AUDIO_EXTENSIONS | AUDIO_EXTENSIONS_REENCODE

    files: list[tuple[Path, str | None]] = []

    for input_str in inputs:
        input_path = Path(input_str)

        # Direct file
        if input_path.is_file():
            ext = input_path.suffix.lower()
            if ext in AUDIO_EXTENSIONS:
                # Good audio file - no conversion needed
                files.append((input_path.resolve(), None))
            elif ext in AUDIO_EXTENSIONS_REENCODE:
                # Problematic audio format - re-encode to MP3
                logger.info(f"üîÑ Re-encoding {ext} to MP3 (format not reliable with Fireflies)...")
                audio_path = convert_video_to_audio(input_path)  # Works for audio too
                if audio_path:
                    files.append((audio_path, input_path.name))
                else:
                    logger.warning(f"Skipping audio (re-encoding failed): {input_path}")
            elif ext in VIDEO_EXTENSIONS:
                # Convert video to audio, track original video name
                audio_path = convert_video_to_audio(input_path)
                if audio_path:
                    # Store original VIDEO filename so output uses it, not temp name
                    files.append((audio_path, input_path.name))
                else:
                    logger.warning(f"Skipping video (conversion failed): {input_path}")
            else:
                logger.warning(f"Skipping unsupported file: {input_path}")
            continue

        # Directory - find all audio files (both good and re-encode types)
        if input_path.is_dir():
            for ext in all_audio_extensions:
                for f in input_path.glob(f"*{ext}"):
                    if ext in AUDIO_EXTENSIONS:
                        files.append((f.resolve(), None))
                    else:
                        # Re-encode problematic formats
                        audio_path = convert_video_to_audio(f)
                        if audio_path:
                            files.append((audio_path, f.name))
                for f in input_path.glob(f"*{ext.upper()}"):
                    if ext in AUDIO_EXTENSIONS:
                        files.append((f.resolve(), None))
                    else:
                        audio_path = convert_video_to_audio(f)
                        if audio_path:
                            files.append((audio_path, f.name))
            continue

        # Glob pattern
        if "*" in input_str or "?" in input_str:
            # Use glob from current directory or parent
            matched = list(Path(".").glob(input_str))
            for f in matched:
                ext = f.suffix.lower()
                if ext in AUDIO_EXTENSIONS:
                    files.append((f.resolve(), None))
                elif ext in AUDIO_EXTENSIONS_REENCODE:
                    audio_path = convert_video_to_audio(f)
                    if audio_path:
                        files.append((audio_path, f.name))
            if not files:
                logger.warning(f"No audio files matched pattern: {input_str}")
            continue

        # Unknown - might be a file that doesn't exist yet or other input type
        logger.warning(f"Skipping unknown input: {input_str}")

    # Deduplicate by audio path, keep first occurrence (preserves original_name)
    seen = set()
    unique_files = []
    for audio_path, original_name in files:
        resolved = audio_path.resolve()
        if resolved not in seen:
            seen.add(resolved)
            unique_files.append((resolved, original_name))

    return sorted(unique_files, key=lambda x: x[0])


# =============================================================================
# Main Workflow
# =============================================================================


class FirefliesWorkflow:
    """Complete workflow: upload ‚Üí transcribe ‚Üí download."""

    # Supported audio formats (Fireflies accepts these)
    # Audio formats that Fireflies reliably transcribes
    AUDIO_EXTENSIONS = {
        ".mp3",     # MP3
        ".m4a",     # AAC or ALAC in M4A
        ".ogg",     # Opus or Vorbis
        ".opus",    # Opus
        ".webm",    # Opus in WebM
    }
    # Audio formats that need re-encoding (transcription hangs otherwise)
    AUDIO_EXTENSIONS_REENCODE = {
        ".wav", ".flac", ".aac", ".wma", ".aiff", ".ac3",
    }
    # Max file size (Fireflies limit is 2GB, but we warn at 500MB)
    MAX_FILE_SIZE_MB = 2048
    WARN_FILE_SIZE_MB = 500

    def __init__(
        self, browser: str = "brave", headless: bool = False, api_key: str = None
    ):
        self.browser = browser
        self.headless = headless
        self.api_key = api_key
        self.browser_ctrl = None
        self.download_api = None
        self.public_api = FirefliesPublicAPI(api_key) if api_key else None

    def _validate_pre_upload(self, audio_path: Path) -> tuple[bool, str]:
        """Validate file before upload.

        Returns:
            (is_valid, error_message) tuple
        """
        # Check file exists
        if not audio_path.exists():
            return False, f"File not found: {audio_path}"

        # Check file extension
        ext = audio_path.suffix.lower()
        if ext not in self.AUDIO_EXTENSIONS:
            return (
                False,
                f"Unsupported format: {ext} (expected: {', '.join(sorted(self.AUDIO_EXTENSIONS))})",
            )

        # Check file size
        size_mb = audio_path.stat().st_size / 1024 / 1024
        if size_mb > self.MAX_FILE_SIZE_MB:
            return (
                False,
                f"File too large: {size_mb:.1f} MB (max: {self.MAX_FILE_SIZE_MB} MB)",
            )
        if size_mb > self.WARN_FILE_SIZE_MB:
            logger.warning(
                f"  ‚ö†Ô∏è Large file: {size_mb:.1f} MB (may take longer to process)"
            )

        # Check file is not empty
        if audio_path.stat().st_size == 0:
            return False, "File is empty (0 bytes)"

        # Check API key is set and valid
        if not self.api_key:
            return False, "FIREFLIES_API_KEY not set"

        if self.public_api:
            is_valid, error = self.public_api.validate_api_key()
            if not is_valid:
                return False, error

        return True, ""

    def process_file(
        self,
        audio_path: Path,
        output_dir: Path,
        download_options: dict = None,
        timer: Timer = None,
        original_name: str | None = None,
    ) -> bool:
        """Process audio file through complete workflow.

        Args:
            audio_path: Path to audio file (may be temp file from video conversion)
            output_dir: Directory to save output files
            download_options: Options for download (md_only, pdf_only, etc.)
            timer: Timer for phase tracking
            original_name: Original filename (for video conversions, use video name not temp audio name)
        """
        download_options = download_options or {}
        # Use original name for display/output, fall back to audio_path name
        display_name = original_name if original_name else audio_path.name
        logger.info("=" * 60)
        logger.info(f"PROCESSING: {display_name}")
        logger.info("=" * 60)

        # Pre-upload validation
        is_valid, error = self._validate_pre_upload(audio_path)
        if not is_valid:
            logger.error(f"‚ùå Pre-upload validation failed: {error}")
            return False, None
        logger.info("  ‚úÖ Pre-upload validation passed")

        transcript_id = None
        try:
            # Step 1: Start browser (pass public_api for connection pooling)
            if timer:
                timer.start_phase("Upload")
            self.browser_ctrl = FirefliesBrowser(
                self.browser, self.headless, public_api=self.public_api
            )
            self.browser_ctrl.start()
            self.browser_ctrl.navigate_to_upload()

            # Step 1b: Get existing transcript IDs BEFORE upload
            existing_ids = self.browser_ctrl.get_existing_transcript_ids()
            logger.debug(f"  Found {len(existing_ids)} existing transcripts to exclude")

            # Step 2: Upload via UI (returns file name with FIREFLIES_CLI__ prefix)
            # Pass original_name so Fireflies title uses clean name, not temp file name
            file_name = self.browser_ctrl.upload_file(audio_path, title=display_name)
            if timer:
                timer.end_phase()

            # Calculate dynamic timeout based on file size
            # Large files = longer audio = longer transcription time
            file_size_mb = audio_path.stat().st_size / 1024 / 1024
            if file_size_mb < 20:
                max_wait = 30  # 30 min for small files
            elif file_size_mb < 50:
                max_wait = 45  # 45 min for medium files
            elif file_size_mb < 100:
                max_wait = 60  # 60 min for large files
            else:
                max_wait = 90  # 90 min for very large files

            # Step 3: Poll for NEW transcript via PUBLIC API (excludes existing IDs)
            if timer:
                timer.start_phase("Transcription")
            transcript_id = self.browser_ctrl.poll_for_transcript_via_public_api(
                file_name,
                max_wait_minutes=max_wait,
                exclude_ids=existing_ids,
            )
            if timer:
                timer.end_phase()
            if not transcript_id:
                return False, None

            # Step 4: Download files (use display_name for clean output filenames)
            if timer:
                timer.start_phase("Download")
            self._download_files(
                transcript_id, display_name, output_dir, download_options
            )
            if timer:
                timer.end_phase()

            # Update cache for idempotent operations
            update_cache(audio_path, transcript_id)

            logger.info("Complete!")
            return True, transcript_id

        except Exception as e:
            logger.error(f"Error: {e}")
            import traceback

            traceback.print_exc()
            return False, transcript_id

        finally:
            if self.browser_ctrl:
                self.browser_ctrl.stop()

    def _download_files(
        self,
        transcript_id: str,
        base_name: str,
        output_dir: Path,
        download_options: dict = None,
    ):
        """Download transcript and summary files.

        Naming pattern: {title} - Fireflies {Transcript|Summary}.{md|pdf}

        Args:
            download_options: Dict with keys:
                - transcript_only: Skip summary download
                - md_only: Skip PDF download
                - pdf_only: Skip MD download
        """
        download_options = download_options or {}
        transcript_only = download_options.get("transcript_only", False)
        md_only = download_options.get("md_only", False)
        pdf_only = download_options.get("pdf_only", False)

        if not self.download_api:
            self.download_api = FirefliesDownloadAPI(self.browser)

        # Use base_name directly (original filename passed from caller)
        # Strip extension if present
        title = Path(base_name).stem
        basename = sanitize_filename(title)

        logger.info("üì• Downloading files...")

        # Transcript MD
        if not pdf_only:
            content = self.download_api.download(transcript_id, "transcript", "md")
            if content:
                path = output_dir / f"{basename} - Fireflies Transcript.md"
                atomic_write_bytes(path, strip_cli_prefix_from_md(content))
                logger.info(f"    üíæ Saved: {path.name}")

        # Transcript PDF
        if not md_only:
            content = self.download_api.download(transcript_id, "transcript", "pdf")
            if content:
                path = output_dir / f"{basename} - Fireflies Transcript.pdf"
                atomic_write_bytes(path, content)
                logger.info(f"    üíæ Saved: {path.name}")

        # Skip summary if transcript_only
        if transcript_only:
            return

        # Summary files - may need retry (generated after transcript)
        summary_md = None
        summary_pdf = None
        max_summary_retries = 6  # Up to 3 minutes wait

        # Determine which summary files to download
        need_md = not pdf_only
        need_pdf = not md_only

        for attempt in range(max_summary_retries):
            # Summary MD
            if need_md and not summary_md:
                summary_md = self.download_api.download(transcript_id, "summary", "md")
                if summary_md:
                    path = output_dir / f"{basename} - Fireflies Summary.md"
                    atomic_write_bytes(path, strip_cli_prefix_from_md(summary_md))
                    logger.info(f"    üíæ Saved: {path.name}")

            # Summary PDF
            if need_pdf and not summary_pdf:
                summary_pdf = self.download_api.download(
                    transcript_id, "summary", "pdf"
                )
                if summary_pdf:
                    path = output_dir / f"{basename} - Fireflies Summary.pdf"
                    atomic_write_bytes(path, summary_pdf)
                    logger.info(f"    üíæ Saved: {path.name}")

            # Check if all needed files are done
            md_done = (not need_md) or summary_md
            pdf_done = (not need_pdf) or summary_pdf
            if md_done and pdf_done:
                break

            # Wait and retry (summary generation can take 30-120s after transcript)
            if attempt < max_summary_retries - 1:
                logger.info(
                    f"    ‚è≥ Waiting for summary generation... ({(attempt + 1) * 30}s)"
                )
                time.sleep(30)

        # Warn about missing files
        if need_md and not summary_md:
            logger.warning("    ‚ö†Ô∏è Summary MD not available yet")
        if need_pdf and not summary_pdf:
            logger.warning("    ‚ö†Ô∏è Summary PDF not available yet")

    def download_existing(
        self, transcript_id: str, output_dir: Path, download_options: dict = None
    ) -> bool:
        """Download existing transcript by ID.

        Naming pattern: {title} - Fireflies {Transcript|Summary}.{md|pdf}
        """
        download_options = download_options or {}
        transcript_only = download_options.get("transcript_only", False)
        md_only = download_options.get("md_only", False)
        pdf_only = download_options.get("pdf_only", False)

        logger.info(f"üì• Downloading transcript: {transcript_id}")

        if not self.download_api:
            self.download_api = FirefliesDownloadAPI(self.browser)

        # Get title from public API
        title = transcript_id
        if self.public_api:
            transcript_data = self.public_api.get_transcript(transcript_id)
            if transcript_data:
                title = transcript_data.get("title", transcript_id)

        # Strip CLI prefix from title (added for Fireflies UI identification)
        # Loop handles edge case of double-prefix from v1.11.3 bug
        while title.startswith(CLI_UPLOAD_PREFIX):
            title = title[len(CLI_UPLOAD_PREFIX) :]

        # Strip common audio/video extensions from title (Fireflies includes them)
        media_extensions = [
            # Audio
            ".m4a",
            ".mp3",
            ".wav",
            ".opus",
            ".ogg",
            ".flac",
            ".aac",
            ".wma",
            # Video
            ".mp4",
            ".mkv",
            ".avi",
            ".mov",
            ".wmv",
            ".webm",
            ".flv",
            ".m4v",
        ]
        for ext in media_extensions:
            if title.lower().endswith(ext):
                title = title[: -len(ext)]
                break

        # Strip timestamp suffix (e.g., _1767529156) added during video conversion
        import re

        title = re.sub(r"_\d{10}$", "", title)

        basename = sanitize_filename(title)

        # Download transcript files
        if not pdf_only:
            content = self.download_api.download(transcript_id, "transcript", "md")
            if content:
                path = output_dir / f"{basename} - Fireflies Transcript.md"
                atomic_write_bytes(path, strip_cli_prefix_from_md(content))
                logger.info(f"  üíæ Saved: {path.name}")

        if not md_only:
            content = self.download_api.download(transcript_id, "transcript", "pdf")
            if content:
                path = output_dir / f"{basename} - Fireflies Transcript.pdf"
                atomic_write_bytes(path, content)
                logger.info(f"  üíæ Saved: {path.name}")

        # Skip summary if transcript_only
        if transcript_only:
            logger.info("Complete!")
            return True

        # Summary MD
        if not pdf_only:
            content = self.download_api.download(transcript_id, "summary", "md")
            if content:
                path = output_dir / f"{basename} - Fireflies Summary.md"
                atomic_write_bytes(path, strip_cli_prefix_from_md(content))
                logger.info(f"  üíæ Saved: {path.name}")

        # Summary PDF
        if not md_only:
            content = self.download_api.download(transcript_id, "summary", "pdf")
            if content:
                path = output_dir / f"{basename} - Fireflies Summary.pdf"
                atomic_write_bytes(path, content)
                logger.info(f"  üíæ Saved: {path.name}")

        logger.info("Complete!")
        return True

    def list_uploads(self) -> bool:
        """List recent transcripts via public API."""
        if not self.public_api:
            logger.error("FIREFLIES_API_KEY required for listing")
            logger.error("Set it via: export FIREFLIES_API_KEY=your_key")
            return False

        try:
            files = self.public_api.list_transcripts(limit=20)

            if not files:
                logger.info("No transcripts found")
                return True

            logger.info(f"\n{'ID':<28} {'Date':<18} {'Title'}")
            logger.info("-" * 80)

            for f in files[:20]:
                fid = f.get("id", "")[:26]
                title = f.get("title", "Unknown")[:45]

                date_ms = f.get("date", 0)
                if date_ms and date_ms > 10000000000:
                    dt = datetime.fromtimestamp(date_ms / 1000)
                elif date_ms:
                    dt = datetime.fromtimestamp(date_ms)
                else:
                    dt = None
                date_str = dt.strftime("%Y-%m-%d %H:%M") if dt else "Unknown"

                logger.info(f"{fid:<28} {date_str:<18} {title}")

            return True

        except Exception as e:
            logger.error(f"Error: {e}")
            return False

    def delete_transcript(self, transcript_id: str, title: str | None = None) -> bool:
        """Delete a transcript by ID and its associated upload.

        Args:
            transcript_id: ID of transcript to delete
            title: Title of transcript (used to find and delete upload from UI).
                   If not provided, will be fetched from API.
        """
        if not self.api_key:
            logger.error("FIREFLIES_API_KEY required for delete")
            return False

        if not self.public_api:
            self.public_api = FirefliesPublicAPI(self.api_key)

        # Get title from API if not provided (needed to find upload entry)
        if not title:
            try:
                transcript_data = self.public_api.get_transcript(transcript_id)
                if transcript_data:
                    title = transcript_data.get("title")
            except Exception as e:
                logger.debug(f"  Could not fetch title: {e}")

        logger.info(f"üóëÔ∏è Deleting transcript: {transcript_id}")

        # First delete via public API (transcript)
        api_success = self.public_api.delete_transcript(transcript_id)

        # Also delete the upload entry via v4 API
        # This is needed because transcript deletion doesn't remove the upload entry
        if title and CLI_UPLOAD_PREFIX in title:
            browser = None
            started_new_browser = False
            try:
                # Check if existing browser is still usable
                browser = self.browser_ctrl
                browser_usable = False
                if browser and browser.page and browser.playwright:
                    try:
                        # Test if browser is still responsive
                        browser.page.evaluate("() => true")
                        browser_usable = True
                    except Exception:
                        browser_usable = False

                if not browser_usable:
                    logger.debug("  Starting browser for upload deletion...")
                    browser = FirefliesBrowser(
                        self.browser, headless=True, public_api=self.public_api
                    )
                    browser.start()
                    started_new_browser = True

                # Find upload by title and delete via v4 API
                logger.debug(f"  Finding upload: {title[:50]}...")
                uploads = browser.list_uploads(wait_for_data=True)

                # Find matching upload by name
                matching = [u for u in uploads if u.get("name") == title]
                if matching:
                    file_id = matching[0].get("_id")
                    if file_id:
                        upload_deleted = browser.delete_upload_via_api(file_id)
                        if upload_deleted:
                            logger.info("  ‚úÖ Upload entry deleted")
                        else:
                            logger.debug("  Upload deletion via API failed")
                else:
                    logger.debug(f"  Upload not found in list ({len(uploads)} total)")
            except Exception as e:
                logger.debug(f"  Upload deletion failed (non-critical): {e}")
            finally:
                # Stop browser if we started it ourselves
                if started_new_browser and browser:
                    browser.stop()

        return api_success

    def cleanup_cli_transcripts(
        self, dry_run: bool = False, skip_confirm: bool = False
    ) -> bool:
        """Delete all transcripts with FIREFLIES_CLI__ prefix.

        Args:
            dry_run: If True, show what would be deleted without deleting
            skip_confirm: If True, skip confirmation prompt

        Returns:
            True if cleanup succeeded (or nothing to clean)
        """
        if not self.public_api:
            logger.error("FIREFLIES_API_KEY required for cleanup")
            logger.error("Set it via: export FIREFLIES_API_KEY=your_key")
            return False

        logger.info("üîç Scanning for FIREFLIES_CLI__ transcripts...")

        try:
            # Get all transcripts (up to 50)
            all_transcripts = self.public_api.list_transcripts(limit=50)

            if not all_transcripts:
                logger.info("No transcripts found")
                return True

            # Filter for CLI uploads - catch current and legacy prefixes
            cli_prefixes = [
                "FIREFLIES_CLI__",  # Current prefix
                "FIREFLIESCLI__",  # Old prefix (no underscore)
                "fireflies_cli_",  # Legacy lowercase prefix
                "ff_convert_",  # Legacy temp file prefix
            ]
            cli_transcripts = [
                t
                for t in all_transcripts
                if any(prefix in (t.get("title") or "") for prefix in cli_prefixes)
            ]

            if not cli_transcripts:
                logger.info(
                    "‚úÖ No FIREFLIES_CLI__ transcripts found - nothing to clean up"
                )
                return True

            # Display what will be deleted
            logger.info(f"\nFound {len(cli_transcripts)} transcripts to clean up:\n")
            logger.info(f"  {'ID':<28} {'Date':<12} {'Title'}")
            logger.info(f"  {'-' * 28} {'-' * 12} {'-' * 40}")

            for t in cli_transcripts:
                tid = t.get("id", "")[:26]
                title = t.get("title", "Unknown")[:40]

                date_ms = t.get("date", 0)
                if date_ms and date_ms > 10000000000:
                    dt = datetime.fromtimestamp(date_ms / 1000)
                elif date_ms:
                    dt = datetime.fromtimestamp(date_ms)
                else:
                    dt = None
                date_str = dt.strftime("%Y-%m-%d") if dt else "Unknown"

                logger.info(f"  {tid:<28} {date_str:<12} {title}")

            if dry_run:
                logger.info(
                    f"\nüìã DRY RUN: Would delete {len(cli_transcripts)} transcripts"
                )
                return True

            # Confirmation prompt
            if not skip_confirm:
                logger.info("")
                try:
                    response = (
                        input(f"‚ö†Ô∏è  Delete {len(cli_transcripts)} transcripts? [y/N]: ")
                        .strip()
                        .lower()
                    )
                    if response not in ("y", "yes"):
                        logger.info("Cancelled")
                        return True
                except (EOFError, KeyboardInterrupt):
                    logger.info("\nCancelled")
                    return True

            # Delete each transcript
            logger.info("")
            deleted = 0
            failed = 0

            for t in cli_transcripts:
                tid = t.get("id", "")
                title = t.get("title", "Unknown")[:30]

                try:
                    success = self.public_api.delete_transcript(tid, verify=False)
                    if success:
                        logger.info(f"üóëÔ∏è  Deleted {tid[:20]}... ({title}) ‚úÖ")
                        deleted += 1
                    else:
                        logger.warning(f"üóëÔ∏è  Failed {tid[:20]}... ({title}) ‚ùå")
                        failed += 1
                except Exception as e:
                    logger.warning(f"üóëÔ∏è  Error {tid[:20]}... ({title}): {e} ‚ùå")
                    failed += 1

                # Rate limit: sleep 2 seconds between deletes
                time.sleep(2)

            # Summary
            logger.info("")
            if failed == 0:
                logger.info(
                    f"‚úÖ Cleanup complete: {deleted}/{len(cli_transcripts)} deleted successfully"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è  Cleanup complete: {deleted} deleted, {failed} failed"
                )

            return failed == 0

        except Exception as e:
            logger.error(f"Cleanup error: {e}")
            return False

    def cleanup_cli_uploads(
        self, dry_run: bool = False, skip_confirm: bool = False
    ) -> bool:
        """Delete all uploads with FIREFLIES_CLI__ prefix via v4 API.

        This is separate from cleanup_cli_transcripts because:
        - Transcripts are deleted via public API (api.fireflies.ai)
        - Uploads are deleted via v4 API (app.fireflies.ai) which requires browser session

        Args:
            dry_run: If True, show what would be deleted without deleting
            skip_confirm: If True, skip confirmation prompt

        Returns:
            True if cleanup succeeded (or nothing to clean)
        """
        logger.info("üîç Scanning for FIREFLIES_CLI__ uploads...")

        browser = None
        try:
            # Start browser to access v4 API
            browser = FirefliesBrowser(
                self.browser, headless=True, public_api=self.public_api
            )
            browser.start()

            # Get all uploads via v4 API
            uploads = browser.list_uploads()

            if not uploads:
                logger.info("No uploads found")
                return True

            # Filter for CLI uploads - match on name field
            cli_prefixes = [
                "FIREFLIES_CLI__",
                "FIREFLIESCLI__",
                "fireflies_cli_",
                "ff_convert_",
            ]
            cli_uploads = [
                u
                for u in uploads
                if any(prefix in (u.get("name") or "") for prefix in cli_prefixes)
            ]

            if not cli_uploads:
                logger.info("‚úÖ No FIREFLIES_CLI__ uploads found - nothing to clean up")
                return True

            # Display what will be deleted
            logger.info(f"\nFound {len(cli_uploads)} uploads to clean up:\n")
            logger.info(f"  {'ID':<26} {'Name'}")
            logger.info(f"  {'-' * 26} {'-' * 50}")

            for u in cli_uploads:
                uid = u.get("_id", "")[:24]
                name = (u.get("name") or "Unknown")[:50]
                logger.info(f"  {uid:<26} {name}")

            if dry_run:
                logger.info(f"\nüìã DRY RUN: Would delete {len(cli_uploads)} uploads")
                return True

            # Confirmation prompt
            if not skip_confirm:
                logger.info("")
                try:
                    response = (
                        input(f"‚ö†Ô∏è  Delete {len(cli_uploads)} uploads? [y/N]: ")
                        .strip()
                        .lower()
                    )
                    if response not in ("y", "yes"):
                        logger.info("Cancelled")
                        return True
                except (EOFError, KeyboardInterrupt):
                    logger.info("\nCancelled")
                    return True

            # Delete each upload via v4 API
            logger.info("")
            deleted = 0
            failed = 0

            for u in cli_uploads:
                uid = u.get("_id", "")
                name = (u.get("name") or "Unknown")[:30]

                try:
                    success = browser.delete_upload_via_api(uid)
                    if success:
                        logger.info(f"üóëÔ∏è  Deleted {uid[:20]}... ({name}) ‚úÖ")
                        deleted += 1
                    else:
                        logger.warning(f"üóëÔ∏è  Failed {uid[:20]}... ({name}) ‚ùå")
                        failed += 1
                except Exception as e:
                    logger.warning(f"üóëÔ∏è  Error {uid[:20]}... ({name}): {e} ‚ùå")
                    failed += 1

            # Summary
            logger.info("")
            if failed == 0:
                logger.info(
                    f"‚úÖ Upload cleanup complete: {deleted}/{len(cli_uploads)} deleted"
                )
            else:
                logger.warning(f"‚ö†Ô∏è  Upload cleanup: {deleted} deleted, {failed} failed")

            return failed == 0

        except Exception as e:
            logger.error(f"Upload cleanup error: {e}")
            return False
        finally:
            if browser:
                browser.stop()


# =============================================================================
# Watch Mode
# =============================================================================


def watch_directory(
    watch_dir: Path,
    workflow: "FirefliesWorkflow",
    output_dir: Path,
    download_options: dict,
    keep: bool = False,
    poll_interval: int = 5,
    move_done: bool = False,
    notify: bool = False,
) -> None:
    """Watch a directory for new audio files and process them automatically.

    Args:
        watch_dir: Directory to watch for new audio files
        workflow: FirefliesWorkflow instance
        output_dir: Directory to save output files
        download_options: Options for download (md_only, pdf_only, etc.)
        keep: Keep transcripts on Fireflies after download
        poll_interval: Seconds between directory scans
        move_done: Move processed files to done/ subfolder
        notify: Send macOS notifications on completion
    """
    if not watch_dir.is_dir():
        logger.error(f"‚ùå Watch directory does not exist: {watch_dir}")
        return

    logger.info(f"üëÄ Watching directory: {watch_dir}")
    logger.info(f"   Poll interval: {poll_interval}s")
    logger.info(f"   Output dir: {output_dir}")
    if move_done:
        done_dir = watch_dir / "done"
        logger.info(f"   Move processed to: {done_dir}")
    logger.info("   Press Ctrl+C to stop\n")

    # Track processed files to avoid reprocessing
    processed_files: set[Path] = set()

    # All supported extensions (audio + video)
    all_extensions = AUDIO_EXTENSIONS | VIDEO_EXTENSIONS

    try:
        while True:
            # Scan for new files
            new_files: list[Path] = []
            for ext in all_extensions:
                for f in watch_dir.glob(f"*{ext}"):
                    if f not in processed_files and f.is_file():
                        # Skip files still being written (check if size is stable)
                        try:
                            size1 = f.stat().st_size
                            time.sleep(0.5)
                            size2 = f.stat().st_size
                            if size1 == size2 and size1 > 0:
                                new_files.append(f)
                        except OSError:
                            continue
                # Also check uppercase extensions
                for f in watch_dir.glob(f"*{ext.upper()}"):
                    if f not in processed_files and f.is_file():
                        try:
                            size1 = f.stat().st_size
                            time.sleep(0.5)
                            size2 = f.stat().st_size
                            if size1 == size2 and size1 > 0:
                                new_files.append(f)
                        except OSError:
                            continue

            # Process new files
            for file_path in sorted(new_files):
                logger.info(f"\nüÜï New file detected: {file_path.name}")

                # Convert video to audio if needed
                original_name = None
                audio_path = file_path
                if file_path.suffix.lower() in VIDEO_EXTENSIONS:
                    audio_path = convert_video_to_audio(file_path)
                    if not audio_path:
                        logger.error(f"‚ùå Video conversion failed: {file_path.name}")
                        processed_files.add(file_path)
                        continue
                    original_name = file_path.name

                # Process through workflow
                try:
                    success, transcript_id = workflow.process_file(
                        audio_path,
                        output_dir,
                        download_options,
                        original_name=original_name,
                    )

                    if success:
                        # Delete from Fireflies unless --keep
                        if not keep and transcript_id:
                            workflow.delete_transcript(transcript_id)

                        # Move to done/ if requested
                        if move_done:
                            done_dir = watch_dir / "done"
                            done_dir.mkdir(exist_ok=True)
                            dest = done_dir / file_path.name
                            try:
                                file_path.rename(dest)
                                logger.info(f"   üì¶ Moved to: done/{file_path.name}")
                            except OSError as e:
                                logger.warning(f"   ‚ö†Ô∏è Could not move file: {e}")

                        if notify:
                            notify_completion(file_path.name, success=True)
                    else:
                        if notify:
                            notify_completion(file_path.name, success=False)

                except Exception as e:
                    logger.error(f"‚ùå Error processing {file_path.name}: {e}")
                    if notify:
                        notify_completion(file_path.name, success=False)

                # Mark as processed (even on failure, to avoid retry loop)
                processed_files.add(file_path)

            # Wait before next scan
            time.sleep(poll_interval)

    except KeyboardInterrupt:
        logger.info("\n\nüëã Watch mode stopped")


# =============================================================================
# Notifications (macOS)
# =============================================================================


def _escape_applescript(s: str) -> str:
    """Escape string for safe use in AppleScript."""
    return s.replace("\\", "\\\\").replace('"', '\\"')


def send_notification(
    title: str, message: str, sound: bool = True, subtitle: str = None
) -> bool:
    """Send macOS notification using osascript.

    Args:
        title: Notification title
        message: Notification body
        sound: Play sound alert
        subtitle: Optional subtitle

    Returns:
        True if notification sent successfully
    """
    if sys.platform != "darwin":
        logger.debug("Notifications only supported on macOS")
        return False

    # Escape inputs to prevent AppleScript injection
    title = _escape_applescript(title)
    message = _escape_applescript(message)

    # Build AppleScript
    script_parts = [f'display notification "{message}"']
    script_parts.append(f'with title "{title}"')

    if subtitle:
        script_parts.append(f'subtitle "{_escape_applescript(subtitle)}"')

    if sound:
        script_parts.append('sound name "default"')

    script = " ".join(script_parts)

    try:
        subprocess.run(
            ["osascript", "-e", script],
            capture_output=True,
            timeout=5,
        )
        return True
    except Exception as e:
        logger.debug(f"Notification failed: {e}")
        return False


def notify_completion(filename: str, success: bool = True) -> None:
    """Send completion notification."""
    if success:
        send_notification(
            title="Fireflies CLI",
            message=f"Transcription complete: {filename}",
            subtitle="‚úÖ Success",
        )
    else:
        send_notification(
            title="Fireflies CLI",
            message=f"Transcription failed: {filename}",
            subtitle="‚ùå Error",
        )


# --- Self-Test ---


def run_self_test(browser: str = "brave", api_key: str = None) -> bool:
    """Verify dependencies, browser cookies, and API key."""
    print("FIREFLIES CLI - SELF TEST\n")
    results = []

    # Python version
    v = sys.version_info
    results.append(("Python >= 3.9", v >= (3, 9), f"{v.major}.{v.minor}"))

    # Required + optional packages
    for name, mod, req in [
        ("requests", "requests", True), ("playwright", "playwright", True),
        ("browser_cookie3", "browser_cookie3", True),
        ("yt-dlp", "yt_dlp", False), ("PyYAML", "yaml", False),
    ]:
        try:
            __import__(mod)
            results.append((name, True, "installed"))
        except ImportError:
            results.append((name, False if req else None, "missing"))

    # Playwright chromium
    try:
        r = subprocess.run(["playwright", "install", "--dry-run", "chromium"],
                          capture_output=True, text=True, timeout=10)
        ok = "chromium" not in r.stdout.lower() or r.returncode == 0
        results.append(("chromium", ok, "ready" if ok else "needs install"))
    except Exception as e:
        results.append(("chromium", False, str(e)[:30]))

    # API key
    env_key = os.environ.get("FIREFLIES_API_KEY")
    test_key = api_key or env_key
    if test_key:
        try:
            api_ok, api_err = FirefliesPublicAPI(test_key).validate_api_key()
            detail = "valid" if api_ok else (api_err or "invalid")
            results.append(("API key", api_ok, detail))
        except Exception as e:
            results.append(("API key", False, str(e)[:30]))
    else:
        results.append(("API key", None, "not set"))

    # Browser cookies
    try:
        import browser_cookie3
        funcs = {"brave": browser_cookie3.brave, "chrome": browser_cookie3.chrome,
                 "chromium": browser_cookie3.chromium}
        cj = funcs.get(browser, browser_cookie3.brave)(domain_name=".fireflies.ai")
        cookies = list(cj)
        auth = next((c for c in cookies if c.name == "authorization"), None)
        results.append((f"cookies ({browser})", len(cookies) > 0, f"{len(cookies)} found"))
        results.append(("auth token", auth is not None, "found" if auth else "missing - login to Fireflies"))
    except Exception as e:
        results.append((f"cookies ({browser})", False, str(e)[:40]))

    # Print results
    all_pass = True
    for check, status, detail in results:
        icon = "‚úÖ" if status is True else ("‚ùå" if status is False else "‚ö†Ô∏è")
        if status is False:
            all_pass = False
        print(f"  {icon} {check}: {detail}")

    print(f"\n{'‚úÖ All checks passed!' if all_pass else '‚ùå Some checks failed'}")
    return all_pass


# =============================================================================
# CLI
# =============================================================================


def main():
    parser = argparse.ArgumentParser(
        description="Fireflies CLI v1.11.2 - Headless Browser Automation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s audio.m4a                      # Upload + transcribe + download + delete
  %(prog)s video.mp4                      # Convert MP4 ‚Üí MP3, then transcribe
  %(prog)s audio.m4a --keep               # Upload + transcribe + download (keep on Fireflies)
  %(prog)s file1.m4a file2.mp3            # Batch: multiple files
  %(prog)s *.m4a                          # Batch: glob pattern
  %(prog)s recordings/                    # Batch: all audio in directory
  %(prog)s --watch inbox/                 # Watch mode: auto-process new files
  %(prog)s --watch inbox/ --move-done     # Watch mode: move processed to done/
  %(prog)s "https://youtube.com/..."      # YouTube ‚Üí keeps video + transcripts
  %(prog)s "https://youtube.com/..." -A   # YouTube ‚Üí transcripts only (no video)
  %(prog)s "https://vimeo.com/..."        # Vimeo ‚Üí keeps video + transcripts
  %(prog)s "https://twitter.com/..."      # Twitter/X ‚Üí keeps video + transcripts
  %(prog)s download 01KDJAJ4QQE...        # Download existing transcript
  %(prog)s delete 01KDJAJ4QQE...          # Delete transcript
  %(prog)s --list                         # List recent transcripts
  %(prog)s --self-test                    # Verify dependencies and config
  %(prog)s --check-auth                   # Quick authentication check

Video support (requires ffmpeg):
  MP4, MKV, AVI, MOV, etc. ‚Üí auto-converted to MP3 before upload.
  URLs: video file is kept by default. Use --audio-only (-A) to skip.

URL support (via yt-dlp):
  YouTube, Vimeo, Twitter/X, TikTok, Twitch, SoundCloud, and 1000+ sites.
  Full list: https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md

Requirements:
  - FIREFLIES_API_KEY env var (required)
  - Logged into Fireflies in browser (for cookie-based auth)
  - ffmpeg (for video conversion): brew install ffmpeg

All commands run headless (use --no-headless to show browser).
Default: delete from Fireflies after download (use --keep to retain).
Batch mode: continues on error, shows summary at end.
Watch mode: polls directory for new audio files (Ctrl+C to stop).
""",
    )

    parser.add_argument(
        "inputs",
        nargs="*",
        help='Audio file(s), YouTube URL, glob pattern, directory, or "download/delete ID"',
    )
    parser.add_argument(
        "transcript_id", nargs="?", help="Transcript ID (for download/delete)"
    )
    parser.add_argument(
        "--list", "-l", action="store_true", help="List recent transcripts"
    )
    parser.add_argument(
        "--keep",
        "-k",
        action="store_true",
        help="Keep on Fireflies after download (default: delete)",
    )
    parser.add_argument("--output-dir", "-o", default=".", help="Output directory")
    parser.add_argument(
        "--browser", "-b", default="brave", choices=["brave", "chrome", "chromium"]
    )
    parser.add_argument(
        "--no-headless",
        action="store_true",
        help="Show browser window (default: headless)",
    )
    parser.add_argument(
        "--debug", action="store_true", help="Enable debug logging and save screenshots"
    )
    parser.add_argument(
        "--force",
        "-f",
        action="store_true",
        help="Overwrite existing output files (default: quit if files exist)",
    )
    parser.add_argument(
        "--config",
        "-c",
        type=Path,
        help=f"Config file path (default: {DEFAULT_CONFIG_PATH})",
    )
    parser.add_argument(
        "--init-config",
        action="store_true",
        help="Create example config file and exit",
    )
    parser.add_argument(
        "--transcript-only",
        action="store_true",
        help="Download transcript only (skip summary)",
    )
    parser.add_argument(
        "--md-only",
        action="store_true",
        help="Download markdown only (skip PDF)",
    )
    parser.add_argument(
        "--pdf-only",
        action="store_true",
        help="Download PDF only (skip markdown)",
    )
    parser.add_argument(
        "--quiet",
        "-q",
        action="store_true",
        help="Quiet mode - minimal output",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Verbose mode - show retries, timing, detailed progress",
    )
    parser.add_argument(
        "--log-json",
        action="store_true",
        help="Output logs as JSON lines (for automation)",
    )
    parser.add_argument(
        "--self-test",
        action="store_true",
        help="Run self-test to verify dependencies and configuration",
    )
    parser.add_argument(
        "--check-auth",
        action="store_true",
        help="Check authentication (cookies + API key) and exit",
    )
    parser.add_argument(
        "--watch",
        "-w",
        type=Path,
        metavar="DIR",
        help="Watch directory for new audio files and process automatically",
    )
    parser.add_argument(
        "--poll-interval",
        type=int,
        default=5,
        metavar="SECS",
        help="Poll interval for watch mode (default: 5s)",
    )
    parser.add_argument(
        "--move-done",
        action="store_true",
        help="Move processed files to done/ subfolder (watch mode only)",
    )
    parser.add_argument(
        "--notify",
        "-n",
        action="store_true",
        help="Send macOS notification on completion",
    )
    parser.add_argument(
        "--dated-folder",
        "-d",
        action="store_true",
        help="Create dated folder for YouTube: YYYY-MM-DD Title [id]/",
    )
    parser.add_argument(
        "--audio-only",
        "-A",
        action="store_true",
        help="Don't keep video file (default: video is kept when downloading from URLs)",
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="Delete ALL FIREFLIES_CLI__ prefixed transcripts AND uploads",
    )
    parser.add_argument(
        "--cleanup-uploads",
        action="store_true",
        help="Delete only FIREFLIES_CLI__ prefixed uploads (not transcripts)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be deleted without actually deleting (use with --cleanup)",
    )
    parser.add_argument(
        "--yes",
        "-y",
        action="store_true",
        help="Skip confirmation prompts (use with --cleanup)",
    )

    args = parser.parse_args()

    # Handle --init-config
    if args.init_config:
        config_path = args.config or DEFAULT_CONFIG_PATH
        success = create_example_config(config_path)
        sys.exit(0 if success else 1)

    # Load config file
    config = load_config(args.config)

    # Handle --self-test (early, before other config)
    if args.self_test:
        browser = (
            args.browser if args.browser != "brave" else config.get("browser", "brave")
        )
        api_key = os.environ.get("FIREFLIES_API_KEY") or config.get("api_key")
        success = run_self_test(browser=browser, api_key=api_key)
        sys.exit(0 if success else 1)

    # Handle --check-auth (quick auth verification)
    if args.check_auth:
        browser = (
            args.browser if args.browser != "brave" else config.get("browser", "brave")
        )
        api_key = os.environ.get("FIREFLIES_API_KEY") or config.get("api_key")

        print("Checking authentication...")
        all_ok = True

        # Check API key
        if api_key:
            try:
                test_api = FirefliesPublicAPI(api_key)
                api_ok, api_err = test_api.validate_api_key()
                if api_ok:
                    print("  ‚úÖ API key: valid")
                else:
                    detail = api_err or "invalid"
                    print(f"  ‚ùå API key: {detail}")
                    all_ok = False
            except Exception as e:
                print(f"  ‚ùå API key: error - {e}")
                all_ok = False
        else:
            print("  ‚ùå API key: not set")
            all_ok = False

        # Check cookies
        try:
            import browser_cookie3

            browser_funcs = {
                "brave": browser_cookie3.brave,
                "chrome": browser_cookie3.chrome,
                "chromium": browser_cookie3.chromium,
            }
            browser_func = browser_funcs.get(browser, browser_cookie3.brave)
            cj = browser_func(domain_name=".fireflies.ai")

            auth_token = None
            for cookie in cj:
                if cookie.name == "authorization":
                    auth_token = unquote(cookie.value)
                    break

            if auth_token:
                print(f"  ‚úÖ Browser cookies ({browser}): auth token found")
            else:
                print(
                    f"  ‚ùå Browser cookies ({browser}): no auth token - login to Fireflies"
                )
                all_ok = False
        except Exception as e:
            print(f"  ‚ùå Browser cookies: {e}")
            all_ok = False

        print()
        if all_ok:
            print("‚úÖ Authentication OK!")
        else:
            print("‚ùå Authentication issues - see above")
        sys.exit(0 if all_ok else 1)

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    # Merge config with CLI args (CLI args override config)
    # API key: env var > config file
    api_key = os.environ.get("FIREFLIES_API_KEY") or config.get("api_key")

    # Output dir: CLI arg > config file > default
    output_dir_str = (
        args.output_dir if args.output_dir != "." else config.get("output_dir", ".")
    )
    output_dir = Path(output_dir_str)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Browser: CLI arg > config file > default
    browser = (
        args.browser if args.browser != "brave" else config.get("browser", "brave")
    )

    # Headless: CLI --no-headless overrides config
    headless = config.get("headless", True) if not args.no_headless else False

    # Keep: CLI --keep overrides config
    keep = args.keep or config.get("keep", False)

    # Download options: CLI overrides config
    transcript_only = args.transcript_only or config.get("transcript_only", False)
    md_only = args.md_only or config.get("md_only", False)
    pdf_only = args.pdf_only or config.get("pdf_only", False)
    quiet = args.quiet or config.get("quiet", False)

    # Quiet mode: set logging to WARNING
    if quiet:
        logging.getLogger().setLevel(logging.WARNING)

    # Verbose mode: enable verbose logging
    global VERBOSE
    if args.verbose:
        VERBOSE = True

    # JSON logging mode
    if args.log_json:
        setup_json_logging()

    # Setup graceful shutdown handlers
    setup_signal_handlers()

    # Register temp audio file cleanup (for video conversions)
    register_cleanup(_cleanup_temp_audio_files)
    atexit.register(_cleanup_temp_audio_files)

    # Build download options dict
    download_options = {
        "transcript_only": transcript_only,
        "md_only": md_only,
        "pdf_only": pdf_only,
    }

    workflow = FirefliesWorkflow(browser=browser, headless=headless, api_key=api_key)

    # Handle --watch (watch mode)
    if args.watch:
        watch_directory(
            watch_dir=args.watch,
            workflow=workflow,
            output_dir=output_dir,
            download_options=download_options,
            keep=keep,
            poll_interval=args.poll_interval,
            move_done=args.move_done,
            notify=args.notify,
        )
        sys.exit(0)

    # Handle --list
    if args.list:
        success = workflow.list_uploads()
        sys.exit(0 if success else 1)

    # Handle --cleanup-uploads (uploads only via v4 API)
    if args.cleanup_uploads:
        success = workflow.cleanup_cli_uploads(
            dry_run=args.dry_run, skip_confirm=args.yes
        )
        sys.exit(0 if success else 1)

    # Handle --cleanup (both transcripts AND uploads)
    if args.cleanup:
        # First clean transcripts via public API
        t_success = workflow.cleanup_cli_transcripts(
            dry_run=args.dry_run, skip_confirm=args.yes
        )
        # Then clean uploads via v4 API (browser session)
        logger.info("")  # Visual separator
        u_success = workflow.cleanup_cli_uploads(
            dry_run=args.dry_run, skip_confirm=args.yes
        )
        sys.exit(0 if (t_success and u_success) else 1)

    # Require inputs
    if not args.inputs:
        parser.print_help()
        sys.exit(1)

    # Handle delete command
    if args.inputs[0] == "delete":
        transcript_id = args.inputs[1] if len(args.inputs) > 1 else args.transcript_id
        if not transcript_id:
            print("Usage: fireflies_web_auth_cli_chrome.py delete TRANSCRIPT_ID")
            sys.exit(1)
        success = workflow.delete_transcript(transcript_id)
        sys.exit(0 if success else 1)

    # Handle download command
    if args.inputs[0] == "download":
        transcript_id = args.inputs[1] if len(args.inputs) > 1 else args.transcript_id
        if not transcript_id:
            print("Usage: fireflies_web_auth_cli_chrome.py download TRANSCRIPT_ID")
            sys.exit(1)
        success = workflow.download_existing(
            transcript_id, output_dir, download_options
        )
        sys.exit(0 if success else 1)

    # Single input - check type
    if len(args.inputs) == 1:
        input_str = args.inputs[0]
        input_type = detect_input_type(input_str)

        # Media URL (YouTube, Vimeo, Twitter, etc. - all handled by yt-dlp)
        if input_type == "media_url":
            try:
                # Start timing
                timer = Timer()
                timer.start()

                # Handle --dated-folder: create YYYY-MM-DD Title [id]/ folder
                # Default: keep video. Use --audio-only to skip.
                keep_video = not args.audio_only
                video_path: Path | None = None

                if args.dated_folder:
                    # Get video info first
                    info = get_youtube_info(input_str)
                    if not info:
                        sys.exit(1)

                    # Create dated folder
                    dated_folder = create_dated_folder_path(
                        info["title"], info["id"], output_dir
                    )
                    dated_folder.mkdir(parents=True, exist_ok=True)
                    logger.info(f"üìÅ Created folder: {dated_folder.name}")

                    # Download with custom basename: Title [id]
                    output_basename = (
                        f"{sanitize_filename(info['title'])} [{info['id']}]"
                    )
                    timer.start_phase("Download")
                    downloader = MediaDownloader(dated_folder)
                    audio_path, video_path = downloader.download(
                        input_str,
                        output_basename=output_basename,
                        keep_video=keep_video,
                    )
                    timer.end_phase()
                    target_dir = dated_folder
                else:
                    # Default behavior
                    timer.start_phase("Download")
                    downloader = MediaDownloader(output_dir)
                    audio_path, video_path = downloader.download(
                        input_str, keep_video=keep_video
                    )
                    timer.end_phase()
                    target_dir = output_dir

                if not audio_path:
                    sys.exit(1)

                # Check for existing files after we know the title
                if not args.force:
                    existing = check_existing_output_files(audio_path.name, target_dir)
                    if existing:
                        logger.error(
                            "‚ùå Output files already exist (use --force to overwrite):"
                        )
                        for f in existing:
                            logger.error(f"   {f.name}")
                        audio_path.unlink()  # Cleanup downloaded audio
                        if video_path and video_path.exists():
                            video_path.unlink()  # Cleanup video too
                        sys.exit(1)

                success, transcript_id = workflow.process_file(
                    audio_path, target_dir, download_options, timer=timer
                )
                # Keep audio in dated folder, cleanup otherwise
                if not args.dated_folder:
                    audio_path.unlink()  # Cleanup downloaded audio
                # Video is kept by default (unless --audio-only)
                if video_path and video_path.exists():
                    logger.info(f"üìπ Video saved: {video_path.name}")
                # Delete from Fireflies unless --keep specified
                if success and not keep and transcript_id:
                    workflow.delete_transcript(transcript_id)

                # Print final timing summary
                if success:
                    logger.info(f"‚è±Ô∏è  {timer.summary()}")

                # Send notification
                if args.notify:
                    notify_completion(audio_path.name, success=success)
                sys.exit(0 if success else 1)
            except RuntimeError as e:
                logger.error(f"{e}")
                if args.notify:
                    notify_completion(input_str, success=False)
                sys.exit(1)

        # Transcript ID - download existing
        elif input_type == "transcript_id":
            success = workflow.download_existing(
                input_str, output_dir, download_options
            )
            sys.exit(0 if success else 1)

    # Batch mode: expand inputs and process multiple files
    # Returns tuples of (audio_path, original_name) where original_name is set for video conversions
    audio_files_with_names = expand_inputs(args.inputs)

    if not audio_files_with_names:
        logger.error("‚ùå No audio files found in inputs")
        sys.exit(1)

    # Show batch summary
    if len(audio_files_with_names) > 1:
        logger.info(
            f"üìÇ Batch mode: {len(audio_files_with_names)} audio files to process"
        )
        for i, (f, orig_name) in enumerate(audio_files_with_names, 1):
            display = orig_name if orig_name else f.name
            logger.info(f"   {i}. {display}")

    # Process each file, continue on error
    results = {"success": [], "failed": [], "skipped": []}

    for i, (audio_path, original_name) in enumerate(audio_files_with_names, 1):
        # Use original name for display/output, fall back to audio path name
        display_name = original_name if original_name else audio_path.name

        if len(audio_files_with_names) > 1:
            logger.info(f"\n{'=' * 60}")
            logger.info(f"[{i}/{len(audio_files_with_names)}] {display_name}")
            logger.info("=" * 60)

        # Check for existing output files (unless --force)
        if not args.force:
            existing = check_existing_output_files(display_name, output_dir)
            if existing:
                logger.warning(f"‚è≠Ô∏è Skipping (files exist): {display_name}")
                for f in existing:
                    logger.debug(f"   {f.name}")
                results["skipped"].append(display_name)
                continue

        # Check cache for idempotent operations (unless --force)
        if not args.force:
            cached_id = check_cache(audio_path)
            if cached_id:
                logger.warning(f"‚è≠Ô∏è Skipping (already transcribed): {display_name}")
                logger.info(f"   Cached transcript ID: {cached_id}")
                results["skipped"].append(display_name)
                continue

        try:
            success, transcript_id = workflow.process_file(
                audio_path, output_dir, download_options, original_name=original_name
            )
            if success:
                results["success"].append(display_name)
                # Delete from Fireflies unless --keep specified
                if not keep and transcript_id:
                    workflow.delete_transcript(transcript_id)
                # Send notification
                if args.notify:
                    notify_completion(display_name, success=True)
            else:
                results["failed"].append(display_name)
                if args.notify:
                    notify_completion(display_name, success=False)
        except Exception as e:
            logger.error(f"‚ùå Error processing {display_name}: {e}")
            results["failed"].append(display_name)
            if args.notify:
                notify_completion(display_name, success=False)

    # Show batch summary
    if len(audio_files_with_names) > 1:
        logger.info("\n" + "=" * 60)
        logger.info("BATCH SUMMARY")
        logger.info("=" * 60)
        total = len(audio_files_with_names)
        logger.info(f"‚úÖ Success: {len(results['success'])}/{total}")
        if results["skipped"]:
            logger.info(f"‚è≠Ô∏è Skipped: {len(results['skipped'])}/{total}")
        if results["failed"]:
            logger.info(f"‚ùå Failed:  {len(results['failed'])}/{total}")
            for f in results["failed"]:
                logger.info(f"   - {f}")

    # Exit with error if any failed
    sys.exit(0 if not results["failed"] else 1)


if __name__ == "__main__":
    main()
